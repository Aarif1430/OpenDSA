{"document": {"@ids": "amortized-analysis", "@names": "amortized\\ analysis", "@source": "<string>", "@title": "Amortized Analysis", "title": "Amortized Analysis", "comment": [{"@xml:space": "preserve", "#text": "This file is part of the OpenDSA eTextbook project. See"}, {"@xml:space": "preserve", "#text": "http://algoviz.org/OpenDSA for more details."}, {"@xml:space": "preserve", "#text": "Copyright (c) 2012-2013 by the OpenDSA Project Contributors, and"}, {"@xml:space": "preserve", "#text": "distributed under an MIT open source license."}], "raw": [{"@format": "xml", "@xml:space": "preserve", "avmetadata": "null"}, {"@format": "xml", "@xml:space": "preserve", "codeinclude": "null"}], "paragraph": [{"title_reference": "amortized analysis", "math": ["n", "n"], "#text": "This module presents the concept of ,\nwhich is the analysis for a series of operations taken as a whole.\nIn particular, amortized analysis allows us to deal with the\nsituation where the worst-case cost for  operations is less\nthan  times the worst-case cost of any one operation.\nRather than focusing on the individual cost of each operation\nindependently and summing them, amortized analysis looks at the\ncost of the entire series and \"charges\" each individual operation\nwith a share of the total cost."}, {"math": ["n", "n/2", "n^2/2", "n", "n^2", "n", "\\sum_{i=i}^n i \\approx n^2/2"], "emphasis": ["expected", "must", "must"], "#text": "We can apply the technique of amortized analysis in the case of\na series of sequential searches in an unsorted array.\nFor  random searches, the average-case cost for each search\nis , and so the  total cost for the series is\n.\nUnfortunately, in the worst case all of the searches would\nbe to the last item in the array.\nIn this case, each search costs  for a total worst-case cost\nof .\nCompare this to the cost for a series of  searches such that\neach item in the array is searched for precisely once.\nIn this situation, some of the searches  be expensive, but\nalso some searches  be cheap.\nThe total number of searches, in the best, average, and worst case,\nfor this problem must be\n.\nThis is a factor of two better than the more pessimistic analysis that\ncharges each operation in the series with its worst-case cost."}, {"math": ["n", "n"], "title_reference": "A", "#text": "As another example of amortized analysis, consider the process of\nincrementing a binary counter.\nThe algorithm is to move from the lower-order (rightmost) bit toward\nthe high-order (leftmost) bit, changing 1s to 0s until the first 0\nis encountered.\nThis 0 is changed to a 1, and the increment operation is done.\nBelow is an implementation for the increment operation,\nassuming that a binary number of length  is stored in array\n of length ."}, {"math": ["2^n - 1", "n", "n", "n", "2^n", "n 2^n"], "#text": "If we count from 0 through , (requiring a counter with\nat least  bits), what is the average cost for an increment\noperation in terms of the number of bits processed?\nNaive worst-case analysis says that if all  bits are 1\n(except for the high-order bit), then  bits need to be\nprocessed.\nThus, if there are  increments, then the cost is\n.\nHowever, this is much too high, because it is rare for so many bits to\nbe processed.\nIn fact, half of the time the low-order bit is 0, and so only that\nbit is processed.\nOne quarter of the time, the low-order two bits are 01, and so\nonly the low-order two bits are processed.\nAnother way to view this is that the low-order bit is always flipped,\nthe bit to its left is flipped half the time,\nthe next bit one quarter of the time, and so on.\nWe can capture this with the summation (charging costs to bits going\nfrom right to left)"}, {"math": ["2 \\cdot 2^n", "2^n"], "#text": "In other words, the average number of bits flipped on each\nincrement is 2, leading to a total cost of only \nfor a series of  increments."}, {"title_reference": "pop", "math": ["k", "k"], "#text": "A useful concept for amortized analysis is illustrated by a simple\nvariation on the stack data structure, where the  function\nis slightly modified to take a second parameter  indicating\nthat  pop operations are to be performed."}, {"title_reference": ["multipop", "push", "multipop", "multipop"], "math": ["\\Theta(n)", "n", "m_1", "m_2", "m_1 + m_2\\cdot n = m_1 + m_2 \\cdot m_1", "m_1"], "#text": "The \"local\" worst-case analysis for  is \nfor  elements in the stack.\nThus, if there are  calls to  and  calls\nto , then the naive worst-case cost for the series of\noperation is .\nThis analysis is unreasonably pessimistic.\nClearly it is not really possible to pop  elements each\ntime  is called.\nAnalysis that focuses on single operations cannot deal with this\nglobal limit, and so we turn to amortized analysis to model the\nentire series of operations."}, {"title_reference": ["potential", "multipop", "push", "multipop", "push", "multipop", "multipop"], "#text": "The key to an amortized analysis of this problem lies in the concept\nof .\nAt any given time, a certain number of items may be on the stack.\nThe cost for  can be no more than this number of items.\nEach call to  places another item on the stack, which can\nbe removed by only a single  operation.\nThus, each call to  raises the potential of the stack by\none item.\nThe sum of costs for all calls to  can never be more\nthan the total potential of the stack (aside from a constant time cost\nassociated with each call to  itself)."}, {"title_reference": ["push", "multipop", "push", "multipop", "multipop", "push"], "math": "m_1", "#text": "The amortized cost for any series of  and \noperations is the sum of three costs.\nFirst, each of the  operations takes constant time.\nSecond, each  operation takes a constant time in\noverhead, regardless of the number of items popped on that call.\nFinally, we count the sum of the potentials expended by all\n operations, which is at most , the number of\n operations.\nThis total cost can therefore be expressed as"}, {"title_reference": "Quicksort <Quicksort> <Quicksort>", "#text": "A similar argument was used in our analysis for the partition function\nin the  algorithm.\nWhile on any given pass through the while loop the left or right\npointers might move all the way through the remainder of the\npartition, doing so would reduce the number of times that the while\nloop can be further executed."}, {"title_reference": "move-to-front <move-to-front> <SelfOrg>", "#text": "Our final example uses amortized analysis to prove a relationship\nbetween the cost of the\n self-organizing list\nheuristic and the cost for the optimal static ordering of the list."}, "Recall that, for a series of search operations, the minimum cost for a\nstatic list results when the list is sorted by\nfrequency of access to its records.\nThis is the optimal ordering for the records if we never allow the\npositions of records to change, because the most-frequently accessed\nrecord is first (and thus has least cost), followed by the next most\nfrequently accessed record, and so on."], "math_block": [{"@xml:space": "preserve", "#text": "\\sum_{i=0}^{n-1} \\frac{1}{2^i} < 2."}, {"@xml:space": "preserve", "#text": "m_1 + (m_2 + m_1) = \\Theta(m_1 + m_2)."}], "target": {"@refid": "mtfthm"}, "topic": {"@ids": "mtfthm", "@names": "mtfthm", "title": "Theorem", "paragraph": [{"strong": "Theorem:", "math": ["S", "n", "n", "S"], "#text": "The total number of comparisons required by any series\n of  or more searches on a self-organizing list\nof length  using the  move-to-front heuristic is never\nmore than twice the total number of comparisons required when\nseries  is applied to the list stored in its optimal\nstatic order."}, {"strong": "Proof:", "math": ["m", "m"], "#text": "Each comparison of the search key with a record in the list is\neither successful or unsuccessful.\nFor  searches, there must be exactly  successful\ncomparisons for both the self-organizing list and the static list.\nThe total number of unsuccessful comparisons in the self-organizing\nlist is the sum, over all pairs of distinct keys, of the number of\nunsuccessful comparisons made between that pair."}, {"math": ["A", "B", "S", "A", "B", "A", "B", "S", "A", "B", "S_{AB}", "A", "B", "A", "B"], "#text": "Consider a particular pair of keys:  and .\nFor any sequence of searches , the total number of\n(unsuccessful) comparisons between  and  is\nidentical to the number of comparisons between  and\n required for the subsequence of  made up only of\nsearches for  or .\nCall this subsequence .\nIn other words, including searches for other keys does not\naffect the relative position of  and  and so does\nnot affect the relative contribution to the total cost of the\nunsuccessful comparisons between  and ."}, {"math": ["A", "B", "S_{AB}", "A", "B", "S_{AB}", "S_{AB}", "i", "A", "j", "B", "i \\leq j", "i", "B", "A", "A", "B", "B", "A", "2i", "A", "A"], "#text": "The number of unsuccessful comparisons between  and\n made by the move-to-front heuristic on subsequence\n is at most twice the number of unsuccessful\ncomparisons between  and  required\nwhen  is applied to the optimal static\nordering for the list.\nTo see this, assume that  contains\n  s and   s,\nwith .\nUnder the optimal static ordering,  unsuccessful\ncomparisons are required because  must appear before\n in the list (because its access frequency is higher).\nMove-to-front will yield an unsuccessful comparison whenever the\nrequest sequence changes from  to  or from\n to .\nThe total number of such changes possible is  because\neach change involves an  and each  can be part of\nat most two changes."}, "Because the total number of unsuccessful comparisons required by\nmove-to-front for any given pair of keys is at most twice that\nrequired by the optimal static ordering, the total number of\nunsuccessful comparisons required by move-to-front for all pairs of\nkeys is also at most twice as high.\nBecause the number of successful comparisons is the same for both\nmethods, the total number of comparisons required by move-to-front is\nless than twice the number of comparisons required by the optimal\nstatic ordering."]}}}