{"document": {"@dupnames": "huffman\\ coding\\ trees", "@ids": "huffman-coding-trees", "@source": "<string>", "@title": "Huffman Coding Trees", "title": "Huffman Coding Trees", "comment": [{"@xml:space": "preserve", "#text": "This file is part of the OpenDSA eTextbook project. See"}, {"@xml:space": "preserve", "#text": "http://algoviz.org/OpenDSA for more details."}, {"@xml:space": "preserve", "#text": "Copyright (c) 2012-2016 by the OpenDSA Project Contributors, and"}, {"@xml:space": "preserve", "#text": "distributed under an MIT open source license."}], "raw": [{"@format": "xml", "@xml:space": "preserve", "avmetadata": "null"}, {"@format": "xml", "@xml:space": "preserve", "odsalink": "DataStructures/huffman.css"}, {"@format": "xml", "@xml:space": "preserve", "odsalink": "AV/Binary/huffmanCON.css"}, {"@format": "xml", "@xml:space": "preserve", "index": "null"}], "section": [{"@dupnames": "huffman\\ coding\\ trees", "@ids": "id1", "title": "Huffman Coding Trees", "paragraph": ["One can often gain an improvement in space requirements in exchange\nfor a penalty in running time.\nThere are many situations where this is a desirable tradeoff.\nA typical example is storing files on disk.\nIf the files are not actively used, the owner might wish to compress\nthem to save space. Later, they can be uncompressed for use, which\ncosts some time, but only once.", {"title_reference": ["ASCII coding <ASCII character coding>", "leftlceil log128rightrceil"], "#text": "We often represent a set of items in a computer program by assigning a\nunique code to each item.\nFor example, the standard \nscheme assigns a unique eight-bit value to each character.\nIt takes a certain minimum number of bits to provide enough unique\ncodes so that we have a different one for each character.\nFor example, it takes \nor seven bits to provide the 128 unique codes needed\nto represent the 128 symbols of the ASCII character set."}, {"title_reference": ["left lceil logn rightrceil", "n", "fixed-length codes <fixed-length coding>"], "#text": "The requirement for  bits to\nrepresent  unique code values assumes that all codes will be\nthe same length, as are ASCII codes.\nThese are called .\nIf all characters were used equally often, then a fixed-length coding\nscheme is the most space efficient method.\nHowever, you are probably aware that not all characters are used\nequally often in many applications.\nFor example, the various letters in an English language document have\ngreatly different frequencies of use."}, {"title_reference": "#Freq", "#text": "Table  shows the relative frequencies of the\nletters of the alphabet. From this table we can see that the letter\n'E' appears about 60 times more often than the letter 'Z'. In normal\nASCII, the words \"DEED\" and \"MUCK\" require the same amount of space\n(four bytes). It would seem that words such as \"DEED\", which are\ncomposed of relatively common letters, should be storable in less\nspace than words such as \"MUCK\", which are composed of relatively\nuncommon letters."}, {"title_reference": ["variable-length codes <variable-length coding>", "Huffman coding <Huffman codes>", "search trie"], "#text": "If some characters are used more frequently than others, is it\npossible to take advantage of this fact and somehow assign them\nshorter codes?\nThe price could be that other characters require longer codes, but\nthis might be worthwhile if such characters appear rarely enough.\nThis concept is at the heart of file compression techniques in\ncommon use today.\nThe next section presents one such approach to assigning\n,\ncalled .\nWhile it is not commonly used in its simplest form for file\ncompression (there are better methods), Huffman coding gives the\nflavor of such coding schemes.\nOne motivation for studying Huffman coding is because it provides our\nfirst opportunity to see a type of tree structure referred to as a\n."}], "target": {"@refid": "freq"}, "topic": {"@ids": "freq", "@names": "freq", "title": "Table", "paragraph": "Relative frequencies for the 26 letters of the\nalphabet as they appear in a selected set of English\ndocuments. \"Frequency\" represents the expected frequency of occurrence\nper 1000 letters, ignoring case.", "math_block": {"@xml:space": "preserve", "#text": "\\begin{array}{c|c|c|c}\n\\textbf{Letter}&\\textbf{Frequency}&\\textbf{Letter}&\\textbf{Frequency}\\\\\n\\textrm A & 77 & N & 67\\\\\n\\textrm B & 17 & O & 67\\\\\n\\textrm C & 32 & P & 20\\\\\n\\textrm D & 42 & Q &  5\\\\\n\\textrm E &120 & R & 59\\\\\n\\textrm F & 24 & S & 67\\\\\n\\textrm G & 17 & T & 85\\\\\n\\textrm H & 50 & U & 37\\\\\n\\textrm I & 76 & V & 12\\\\\n\\textrm J &  4 & W & 22\\\\\n\\textrm K &  7 & X &  4\\\\\n\\textrm L & 42 & Y & 22\\\\\n\\textrm M & 24 & Z &  2\\\\\n\\end{array}"}}, "footnote": {"@auto": "1", "@ids": "id2", "@names": "1", "label": "1", "paragraph": {"title_reference": ["sorted list", "heap", "priority queue"], "#text": "To keep things simple, these examples for building Huffman\ntrees uses a  to keep the partial Huffman trees\nordered by frequency.\nBut a real implementation would use a  to implement a\n keyed by the frequencies."}}}, {"@ids": "building-huffman-coding-trees", "@names": "building\\ huffman\\ coding\\ trees", "title": "Building Huffman Coding Trees", "paragraph": [{"title_reference": ["weight", "Huffman coding tree", "Huffman tree", "minimum external path weight", "weighted path length"], "#text": "Huffman coding assigns codes to characters such that the length of the\ncode depends on the relative frequency or  of the\ncorresponding character.\nThus, it is a variable-length code.\nIf the estimated frequencies for letters match the actual frequency\nfound in an encoded message, then the length of that message will\ntypically be less than if a fixed-length code had been used.\nThe Huffman code for each letter is derived from a full binary tree\ncalled the , or simply the\n.\nEach leaf of the Huffman tree corresponds to a letter, and we\ndefine the weight of the leaf node to be the weight (frequency) of its\nassociated letter.\nThe goal is to build a tree with the\n.\nDefine the  of a leaf to be its weight\ntimes its depth.\nThe binary tree with minimum external path weight is the one with the\nminimum sum of weighted path lengths for the given set of leaves.\nA letter with high weight should have low depth, so that it will count\nthe least against the total path length.\nAs a result, another letter might be pushed deeper in the tree if it\nhas less weight."}, {"title_reference": ["n", "n", "n"], "#text": "The process of building the Huffman tree for  letters is\nquite simple.\nFirst, create a collection of  initial Huffman trees,\neach of which is a single leaf node containing one of the letters.\nPut the  partial trees onto a priority queue\norganized by weight (frequency).\nNext, remove the first two trees (the ones with lowest weight) from\nthe priority queue.\nJoin these two trees together to create a new tree whose root has the\ntwo trees as children, and whose weight is the sum of the weights of\nthe two trees.\nPut this new tree back into the priority queue.\nThis process is repeated until all of the partial Huffman trees have\nbeen combined into one."}, {"title_reference": "Table #FreqExamp", "#text": "The following slideshow illustrates the Huffman tree\nconstruction process for the eight letters of\nTable ."}, "Here is the implementation for Huffman tree nodes.", {"title_reference": ["class hierarchy <class hierarchy> <BinaryTreeImpl>", "base class", "subclasses <subclass>"], "literal": ["HuffNode", "LeafNode", "IntlNode"], "#text": "This implementation is similar to\na typical \nfor implementing full binary trees.\nThere is an abstract , named , and two\n, named  and .\nThis implementation reflects the fact that leaf and\ninternal nodes contain distinctly different information."}, "Here is the implementation for the Huffman Tree class.", "Here is the implementation for the tree-building process.", {"literal": ["buildHuff", "fl", "buildTree", "for", "for", "temp1", "temp2", "temp3", "temp1", "temp2", "temp3", "fl"], "#text": "takes as input , the min-heap of partial\nHuffman trees, which initially are single leaf nodes as shown in Step\n1 of the slideshow above.\nThe body of function  consists mainly of a \nloop. On each iteration of the  loop, the first two partial\ntrees are taken off the heap and placed in variables  and\n.\nA tree is created () such that the left and right subtrees\nare  and , respectively.\nFinally,  is returned to ."}], "target": {"@refid": "freqexamp"}, "topic": {"@ids": "freqexamp", "@names": "freqexamp", "title": "Table", "paragraph": "The relative frequencies for eight selected letters.", "math_block": {"@xml:space": "preserve", "#text": "\\begin{array}{|c|cccccccc|}\n\\hline\n\\textrm Letter & C & D & E & K & L & M & U & Z\\\\\n\\textrm Frequency & 32 & 42 & 120 & 7 & 42 & 24 & 37 & 2\\\\\n\\hline\n\\end{array}"}}, "raw": [{"@format": "xml", "@xml:space": "preserve", "inlineav": {"@type": "ss", "@exer_name": "huffmanBuildCON", "@long_name": "huffmanBuildCON", "@points": "0", "@required": "True", "@threshold": "1.0"}}, {"@format": "xml", "@xml:space": "preserve", "codeinclude": "null"}, {"@format": "xml", "@xml:space": "preserve", "codeinclude": "null"}, {"@format": "xml", "@xml:space": "preserve", "codeinclude": "null"}], "footnote": {"@auto": "1", "@ids": "id3", "@names": "2", "label": "2", "paragraph": {"title_reference": "parity", "#text": "ASCII coding actually uses 8 bits per character.\nSeven bits are used to represent the 128 codes of the ASCII\ncharacter set.\nThe eigth bit as a  bit, that can be used to\ncheck if there is a transmission error for the character."}}}, {"@ids": "assigning-and-using-huffman-codes-1", "@names": "assigning\\ and\\ using\\ huffman\\ codes\\ (1)", "title": "Assigning and Using Huffman Codes (1)", "paragraph": "Once the Huffman tree has been constructed, it is an easy matter to\nassign codes to individual letters.\nBeginning at the root, we assign either a '0' or a '1' to each edge in\nthe tree. '0' is assigned to edges connecting a node with its left\nchild, and '1' to edges connecting a node with its right child.\nThis process is illustrated by the following slideshow.", "raw": {"@format": "xml", "@xml:space": "preserve", "inlineav": {"@type": "ss", "@exer_name": "huffmanLabelCON", "@long_name": "huffmanLabelCON", "@points": "0", "@required": "True", "@threshold": "1.0"}}}, {"@ids": "assigning-and-using-huffman-codes-2", "@names": "assigning\\ and\\ using\\ huffman\\ codes\\ (2)", "title": "Assigning and Using Huffman Codes (2)", "paragraph": ["Now that we see how the edges associate with bits in the code, it is a\nsimple matter to generate the codes for each letter (since each letter\ncorresponds to a leaf node in the tree).", "Now that we have a code for each letter,\nencoding a text message is done by replacing each letter of the\nmessage with its binary code.\nA lookup table can be used for this purpose."], "raw": {"@format": "xml", "@xml:space": "preserve", "inlineav": {"@type": "ss", "@exer_name": "huffmanCodesCON", "@long_name": "huffmanCodesCON", "@points": "0", "@required": "True", "@threshold": "1.0"}}}, {"@ids": "decoding", "@names": "decoding", "title": "Decoding", "paragraph": [{"title_reference": "prefix property", "#text": "A set of codes is said to meet the  if no\ncode in the set is the prefix of another.\nThe prefix property guarantees that there will be no ambiguity in how\na bit string is decoded.\nIn other words, once we reach the last bit of a code during\nthe decoding process, we know which letter it is the code for.\nHuffman codes certainly have the prefix property because any prefix\nfor a code would correspond to an internal node, while all codes\ncorrespond to leaf nodes."}, "When we decode a character using the Huffman coding tree, we follow a\npath through the tree dictated by the bits in the code string.\nEach '0' bit indicates a left branch while each '1' bit indicates a\nright branch.\nThe following slideshow shows an example for how to decode a message\nby traversing the tree appropriately."], "raw": {"@format": "xml", "@xml:space": "preserve", "inlineav": {"@type": "ss", "@exer_name": "huffmanDecodeCON", "@long_name": "huffmanDecodeCON", "@points": "0", "@required": "True", "@threshold": "1.0"}}}, {"@ids": "decoding-practice", "@names": "decoding\\ practice", "title": "Decoding Practice", "raw": {"@format": "xml", "@xml:space": "preserve", "avembed": {"@type": "ka", "@exer_name": "HuffmanDecodePRO", "@long_name": "HuffmanDecodePRO", "@points": "1.0", "@required": "True", "@threshold": "5"}}}, {"@ids": "how-efficient-is-huffman-coding", "@names": "how\\ efficient\\ is\\ huffman\\ coding?", "title": "How efficient is Huffman coding?", "paragraph": ["In theory, Huffman coding is an optimal coding method whenever the\ntrue frequencies are known, and the frequency of a letter is\nindependent of the context of that letter in the message.\nIn practice, the frequencies of letters in an English text document do\nchange depending on context.\nFor example, while E is the most commonly used letter of the alphabet\nin English documents, T is more common as the first letter of a\nword.\nThis is why most commercial compression utilities do not use Huffman\ncoding as their primary coding method, but instead use techniques that\ntake advantage of the context for the letters.", "Another factor that affects the compression efficiency of Huffman\ncoding is the relative frequencies of the letters.\nSome frequency patterns will save no space as compared to fixed-length\ncodes; others can result in great compression.\nIn general, Huffman coding does better when there is large variation\nin the frequencies of letters.", {"title_reference": "Table #Freq", "#text": "Huffman coding for all ASCII symbols should do better than this\nexample.\nThe letters of Table  are atypical in that there\nare too many common letters compared to the number of rare letters.\nHuffman coding for all 26 letters would yield an expected\ncost of 4.29 bits per letter.\nThe equivalent fixed-length code would require about five bits.\nThis is somewhat unfair to fixed-length coding because there is\nactually room for 32 codes in five bits, but only 26 letters.\nMore generally, Huffman coding of a typical text file\nwill save around 40% over ASCII coding if we charge ASCII coding at\neight bits per character.\nHuffman coding for a binary file (such as a compiled executable) would\nhave a very different set of distribution frequencies and so would\nhave a different space savings.\nMost commercial compression programs use two or three coding schemes\nto adjust to different types of files."}, "In decoding example, \"DEED\" was coded in 8 bits, a saving of 33%\nover the twelve bits required from a fixed-length coding.\nHowever, \"MUCK\" would require 18 bits, more space than required by the\ncorresponding fixed-length coding.\nThe problem is that \"MUCK\" is composed of letters that are not\nexpected to occur often.\nIf the message does not match the expected frequencies of the letters,\nthan the length of the encoding will not be as expected either."], "topic": {"title": "Example", "paragraph": [{"title_reference": ["Table #Freq", "c_i", "p_i", "c_1 p_1 + c_2 p_2 + cdots + c_n p_n.", "frac{c_1 f_1 + c_2 f_2 + cdots + c_n f_n}{f_T}", "f_i", "i", "f_T", "[(1 times 120) + (3 times 121) + (4 times 32) + (5 times 24) + (6 times 9)]/306 = 785/306 approx 2.57."], "#text": "In the particular case of the frequencies shown in\nTable , we can determine the expected savings from\nHuffman coding if the actual frequencies of a coded message match the\nexpected frequencies.\nBecause the sum of the frequencies is 306 and E has frequency 120,\nwe expect it to appear 120 times in a message containing 306\nletters.\nAn actual message might or might not meet this expectation.\nLetters D, L, and U have code lengths of three,\nand together are expected to appear 121 times in 306 letters.\nLetter C has a code length of four, and is expected to appear 32\ntimes in 306 letters.\nLetter M has a code length of five, and is expected to appear\n24 times in 306 letters.\nFinally, letters K and Z have code lengths of six,\nand together are expected to appear only 9 times in 306 letters.\nThe average expected cost per character is simply the sum of\nthe cost for each character () times the probability of\nits occurring (), or\n\nThis can be reorganized as\n,\nwhere  is the (relative) frequency of letter\n and  is the total for all letter frequencies.\nFor this set of frequencies, the expected cost per letter is"}, {"title_reference": "log 8 = 3", "#text": "A fixed-length code for these eight characters would require\n bits per letter as opposed to about 2.57 bits\nper letter for Huffman coding.\nThus, Huffman coding is expected to save about 14% for this set of\nletters."}]}, "raw": [{"@format": "xml", "@xml:space": "preserve", "odsascript": "DataStructures/huffman.js"}, {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/Binary/huffmanBuildCON.js"}, {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/Binary/huffmanLabelCON.js"}, {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/Binary/huffmanCodesCON.js"}, {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/Binary/huffmanDecodeCON.js"}]}]}}