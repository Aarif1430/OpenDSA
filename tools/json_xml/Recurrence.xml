<?xml version="1.0" encoding="utf8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.1 -->
<document dupnames="solving\ recurrence\ relations" ids="solving-recurrence-relations" source="&lt;string&gt;" title="Solving Recurrence Relations"><title>Solving Recurrence Relations</title><subtitle dupnames="solving\ recurrence\ relations" ids="id1">Solving Recurrence Relations</subtitle><comment xml:space="preserve">This file is part of the OpenDSA eTextbook project. See</comment><comment xml:space="preserve">http://algoviz.org/OpenDSA for more details.</comment><comment xml:space="preserve">Copyright (c) 2012-2013 by the OpenDSA Project Contributors, and</comment><comment xml:space="preserve">distributed under an MIT open source license.</comment><raw format="xml" xml:space="preserve"><avmetadata>null</avmetadata></raw><raw format="xml" xml:space="preserve"><odsalink>AV/Development/AlgAnal/DivideAndConquerRecurrencesCON.css</odsalink></raw><raw format="xml" xml:space="preserve"><todo>null</todo></raw><paragraph>Recurrence relations are often used to model the cost of recursive
functions.
For example, the standard <title_reference>Mergesort &lt;Mergesort&gt; &lt;Mergesort&gt;</title_reference>
takes a list of size <math>n</math>, splits it in half, performs Mergesort
on each half, and finally merges the two sublists in <math>n</math> steps.
The cost for this can be modeled as</paragraph><math_block xml:space="preserve">{\bf T}(n) = 2{\bf T}(n/2) + n.</math_block><paragraph>In other words, the cost of the algorithm on input of
size <math>n</math> is two times the cost for input of size <math>n/2</math>
(due to the two recursive calls to Mergesort) plus <math>n</math>
(the time to merge the sublists together again).</paragraph><paragraph>There are many approaches to solving recurrence relations, and we
briefly consider three here.
The first is an estimation technique:
Guess the upper and lower bounds for the recurrence, use
induction to prove the bounds, and tighten as required.
The second approach is to expand the recurrence to convert it to a
summation and then use summation techniques.
The third approach is to take advantage of already proven theorems
when the recurrence is of a suitable form.
In particular, typical divide-and-conquer algorithms such as
Mergesort yield recurrences of a form that fits a pattern for which
we have a ready solution.</paragraph><section ids="estimating-upper-and-lower-bounds" names="estimating\ upper\ and\ lower\ bounds"><title>Estimating Upper and Lower Bounds</title><paragraph>The first approach to solving recurrences is to guess the
answer and then attempt to prove it correct.
If a correct upper or lower bound estimate is given,
an easy induction proof will verify this fact.
If the proof is successful, then try to tighten the bound.
If the induction proof fails, then loosen the bound and try again.
Once the upper and lower bounds match, you are finished.
This is a useful technique when you are only looking for asymptotic
complexities.
When seeking a precise closed-form solution (i.e., you seek the
constants for the expression), this method will probably be too much
work.</paragraph><topic><title>Example</title><comment xml:space="preserve">Very close to Manber's example.</comment><paragraph>Use the guessing technique to find the asymptotic bounds for
Mergesort, whose running time is described by the equation</paragraph><math_block xml:space="preserve">{\bf T}(n) = 2{\bf T}(n/2) + n; \quad {\bf T}(2) = 1.</math_block><paragraph>We begin by guessing that this recurrence has an upper
bound in <math>O(n^2)</math>.
To be more precise, assume that</paragraph><math_block xml:space="preserve">{\bf T}(n) \leq n^2.</math_block><paragraph>We prove this guess is correct by induction.
In this proof, we assume that <math>n</math> is a
power of two, to make the calculations easy.
For the base case, <math>{\bf T}(2) = 1 \leq 2^2</math>.
For the induction step, we need to show that
<math>{\bf T}(n) \leq n^2</math> implies that
<math>{\bf T}(2n) \leq (2n)^2</math> for <math>n = 2^N, N \geq 1</math>.
The induction hypothesis is</paragraph><math_block xml:space="preserve">{\bf T}(i) \leq i^2,\ {\rm for\ all}\ i \leq n.</math_block><paragraph>It follows that</paragraph><math_block xml:space="preserve">{\bf T}(2n) = 2{\bf T}(n) + 2n \leq 2n^2 + 2n \leq 4n^2 \leq (2n)^2</math_block><paragraph>which is what we wanted to prove.
Thus, <math>{\bf T}(n)</math> is in <math>O(n^2)</math>.</paragraph><paragraph>Is <math>O(n^2)</math> a good estimate?
In the next-to-last step we went from <math>n^2 + 2n</math> to the much
larger <math>4n^2</math>.
This suggests that  <math>O(n^2)</math> is a high estimate.
If we guess something smaller, such as <math>{\bf T}(n) \leq cn</math>
for some constant <math>c</math>, it should be clear that this cannot
work because <math>c 2 n = 2 c n</math> and there is no room for the
extra <math>n</math> cost to join the two pieces together.
Thus, the true cost must be somewhere between <math>cn</math> and
<math>n^2</math>.</paragraph><paragraph>Let us now try <math>{\bf T}(n) \leq n \log n</math>.
For the base case, the definition of the recurrence sets
<math>{\bf T}(2) = 1 \leq (2 \cdot \log 2) = 2</math>.
Assume (induction hypothesis) that <math>{\bf T}(n) \leq n \log n</math>.
Then,</paragraph><math_block xml:space="preserve">{\bf T}(2n) = 2{\bf T}(n) + 2n \leq 2n \log n + 2n
\leq 2n(\log n + 1) \leq 2 n \log 2n</math_block><paragraph>which is what we seek to prove.
In similar fashion, we can prove that <math>{\bf T}(n)</math> is in
<math>\Omega(n \log n)</math>.
Thus, <math>{\bf T}(n)</math> is also <math>\Theta(n \log n)</math>.</paragraph></topic><topic><title>Example</title><paragraph>We know that the factorial function grows exponentially.
How does it compare to <math>2^n</math>? To <math>n^n</math>?
Do they all grow "equally fast" (in an asymptotic sense)?
We can begin by looking at a few initial terms.</paragraph><math_block xml:space="preserve">\begin{array}{r|rrrrrrrrr}
n&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9\\
\hline
n! &amp;1&amp;2&amp;6&amp;24&amp;120&amp;720&amp;5040&amp;40320&amp;362880\\
2^n&amp;2&amp;4&amp;8&amp;16&amp;32&amp;64&amp;128&amp;256&amp;512\\
n^n&amp;1&amp;4&amp;9&amp;256&amp;3125&amp;46656&amp;823543&amp;16777216&amp;387420489
\end{array}</math_block><paragraph>We can also look at these functions in terms of their recurrences.</paragraph><math_block xml:space="preserve">n! = \left\{
\begin{array}{ll}
1&amp;n=1\\
n(n-1)!&amp;n&gt;1\\
\end{array}
\right.</math_block><math_block xml:space="preserve">2^n = \left\{
\begin{array}{ll}
2&amp;n=1\\
2(2^{n-1})&amp;n&gt;1\\
\end{array}
\right.</math_block><math_block xml:space="preserve">n^n = \left\{
\begin{array}{ll}
n&amp;n=1\\
n(n^{n-1})&amp;n&gt;1\\
\end{array}
\right.</math_block><paragraph>At this point, our intuition should be telling us pretty clearly
the relative growth rates of these three functions.
But how do we prove formally which grows the fastest?
And how do we decide if the differences are significant in an
asymptotic sense, or just constant factor differences?</paragraph><paragraph>We can use logarithms to help us get an idea about the relative
growth rates of these functions.
Clearly, <math>\log 2^n = n</math>.
Equally clearly, <math>\log n^n = n \log n</math>.
We can easily see from this that <math>2^n</math> is <math>o(n^n)</math>,
that is, <math>n^n</math> grows asymptotically faster than <math>2^n</math>.</paragraph><paragraph>How does <math>n!</math> fit into this?
We can again take advantage of logarithms.
Obviously <math>n! \leq n^n</math>, so we know that <math>\log n!</math> is
<math>O(n \log n)</math>.
But what about a lower bound for the factorial function?
Consider the following.</paragraph><math_block xml:space="preserve">\begin{eqnarray*}
n! &amp;=&amp; n \times (n - 1) \times \cdots \times \frac{n}{2} \times
(\frac{n}{2} - 1) \times \cdots \times 2 \times 1\\
&amp;\geq&amp; \frac{n}{2} \times \frac{n}{2} \times \cdots \times \frac{n}{2}
\times 1 \times \cdots \times 1 \times 1\\
&amp;=&amp; (\frac{n}{2})^{n/2}
\end{eqnarray*}</math_block><paragraph>Therefore</paragraph><math_block xml:space="preserve">\log n! \geq \log(\frac{n}{2})^{n/2} =
(\frac{n}{2})\log(\frac{n}{2}).</math_block><paragraph>In other words, <math>\log n!</math> is in <math>\Omega(n \log n)</math>.
Thus, <math>\log n! = \Theta(n \log n)</math>.</paragraph><paragraph>Note that this does <strong>not</strong> mean that <math>n! = \Theta(n^n)</math>.
Because <math>\log n^2 = 2 \log n</math>, it follows that
<math>\log n = \Theta(\log n^2)</math> but <math>n \neq \Theta(n^2)</math>.
The log function often works as a "flattener" when dealing with
asymptotics.
That is, whenever <math>\log f(n)</math> is in <math>O(\log g(n))</math> we
know that <math>f(n)</math> is in <math>O(g(n))</math>.
But knowing that <math>\log f(n) = \Theta(\log g(n))</math> does not
necessarily mean that <math>f(n) = \Theta(g(n))</math>.</paragraph></topic><topic><title>Example</title><paragraph>What is the growth rate of the Fibonacci sequence?
We define the Fibonacci sequence as
<math>f(n) = f(n-1) + f(n-2)</math> for <math>n \geq 2</math>;
<math>f(0) = f(1) = 1</math>.</paragraph><paragraph>In this case it is useful to compare the ratio of <math>f(n)</math> to
<math>f(n-1)</math>.
The following table shows the first few values.</paragraph><math_block xml:space="preserve">\begin{array}{c|lllllll}
n&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7\\
\hline
f(n)&amp;1&amp;2&amp;3&amp;5&amp;8&amp;13&amp;21\\
f(n)/f(n-1)&amp;1&amp;2&amp;1.5&amp;1.666&amp;1.625&amp;1.615&amp;1.619
\end{array}</math_block><paragraph>If we continue for more terms, the ratio appears to converge on a
value slightly greater then 1.618.
Assuming <math>f(n)/f(n-1)</math> really does converge to a fixed value
as <math>n</math> grows, we can determine what that value must be.</paragraph><math_block xml:space="preserve">\frac{f(n)}{f(n-2)} = \frac{f(n-1)}{f(n-2)} + \frac{f(n-2)}{f(n-2)}
\rightarrow x+1</math_block><paragraph>for some value <math>x</math>.
This follows from the fact that <math>f(n) = f(n-1) + f(n-2)</math>.
We divide by <math>f(n-2)</math> to make the second term go away, and we
also get something useful in the first term.
Remember that the goal of such manipulations is to give us an
equation that relates <math>f(n)</math> to something without recursive
calls.</paragraph><paragraph>For large <math>n</math>, we also observe that:</paragraph><math_block xml:space="preserve">\frac{f(n)}{f(n-2)} = \frac{f(n)}{f(n-1)}\frac{f(n-1)}{f(n-2)}
\rightarrow x^2</math_block><paragraph>as <math>n</math> gets big.
This comes from multiplying <math>f(n)/f(n-2)</math> by
<math>f(n-1)/f(n-1)</math> and rearranging.</paragraph><paragraph>If <math>x</math> exists, then <math>x^2 - x - 1 \rightarrow 0</math>.
Using the quadratic equation, the only solution greater than one is</paragraph><math_block xml:space="preserve">x = \frac{1 + \sqrt{5}}{2} \approx 1.618.</math_block><paragraph>This expression also has the name <math>\phi</math>.
What does this say about the growth rate of the Fibonacci sequence?
It is exponential, with <math>f(n) = \Theta(\phi^n)</math>.
More precisely, <math>f(n)</math> converges to</paragraph><math_block xml:space="preserve">\frac{\phi^n - (1 - \phi)^n}{\sqrt{5}}.</math_block></topic></section><section ids="expanding-recurrences" names="expanding\ recurrences"><title>Expanding Recurrences</title><paragraph>Estimating bounds is effective if you only need an approximation to
the answer.
More precise techniques are required to find an exact solution.
One approach is called <title_reference>expanding the recurrence</title_reference>.
In this method, the smaller terms on the right side of the equation
are in turn replaced by their definition.
This is the expanding step.
These terms are again expanded, and so on, until a full series
with no recurrence results.
This yields a :ref`summation &lt;summation&gt; &lt;Summation&gt;`,
and techniques for solving summations can then be used.
A more complex example is given next.</paragraph><topic><title>Example</title><paragraph>Find the solution for</paragraph><math_block xml:space="preserve">{\bf T}(n) = 2{\bf T}(n/2) + 5 n^2; \quad {\bf T}(1) = 7.</math_block><paragraph>For simplicity we assume that <math>n</math> is a power of two,
so we will rewrite it as <math>n = 2^k</math>.
This recurrence can be expanded as follows:</paragraph><math_block xml:space="preserve">\begin{eqnarray*}
{\bf T}(n) &amp; = &amp; 2{\bf T}(n/2) + 5n^2\\
           &amp; = &amp; 2(2{\bf T}(n/4) + 5(n/2)^2) + 5n^2\\
           &amp; = &amp; 2(2(2{\bf T}(n/8) + 5(n/4)^2) + 5(n/2)^2) + 5n^2\\
           &amp; = &amp; 2^k{\bf T}(1) + 2^{k-1}\cdot5\left (\frac{n}{2^{k-1}}\right )^2
                   + \cdots + 2\cdot5\left (\frac{n}{2}\right )^2
                   + 5n^2.
\end{eqnarray*}</math_block><paragraph>This last expression can best be represented by a summation
as follows:</paragraph><math_block xml:space="preserve">\begin{eqnarray*}
&amp;   &amp; 7n + 5\sum_{i=0}^{k-1} n^2/2^i\\
&amp; = &amp; 7n + 5n^2\sum_{i=0}^{k-1} 1/2^i.\\
\end{eqnarray*}</math_block><paragraph>From Equation (6) of Module <title_reference>Summation &lt;Summation&gt; &lt;Summation&gt;</title_reference>,
we have:</paragraph><math_block xml:space="preserve">\begin{eqnarray*}
&amp; = &amp; 7n + 5n^2\left (2 - 1/2^{k-1}\right )\\
&amp; = &amp; 7n + 5n^2(2 - 2/n)\\
&amp; = &amp; 7n + 10 n^2 - 10n\\
&amp; = &amp; 10n^2 - 3n.
\end{eqnarray*}</math_block><paragraph>This is the <emphasis>exact</emphasis> solution to the recurrence for <math>n</math>
a power of two.
At this point, we should use a simple induction proof to verify
that our solution is indeed correct.</paragraph></topic><raw format="xml" xml:space="preserve"><inlineav
    type="ss"
    exer_name="DivideAndConquerRecurrencesCON"
    long_name="DivideAndConquerRecurrencesCON"
    points="0"
    required="True"
    threshold="1.0">
</inlineav>
</raw><topic><title>Example</title><paragraph>Our next example models the cost of the algorithm to build a heap.
You should recall that to build a <title_reference>heap &lt;heap&gt; &lt;Heaps&gt;</title_reference>,
we first heapify the two subheaps, then push down the root to its
proper position.
The cost is:</paragraph><math_block xml:space="preserve">f(n) \leq 2f(n/2) + 2 \log n.</math_block><paragraph>Let us find a closed form solution for this recurrence.
We can expand the recurrence a few times to see that</paragraph><math_block xml:space="preserve">\begin{eqnarray*}
f(n) &amp;\leq&amp; 2f(n/2) + 2 \log n\\
&amp;\leq&amp; 2[2f(n/4) + 2 \log n/2] + 2 \log n\\
&amp;\leq&amp; 2[2(2f(n/8) + 2 \log n/4) + 2 \log n/2] + 2 \log n
\end{eqnarray*}</math_block><paragraph>We can deduce from this expansion that this recurrence is
equivalent to following summation and its derivation:</paragraph><math_block xml:space="preserve">\begin{eqnarray*}
f(n) &amp;\leq&amp; \sum_{i=0}^{\log n -1} 2^{i+1} \log(n/2^i)\\
&amp;=&amp; 2 \sum_{i=0}^{\log n -1} 2^i (\log n - i)\\
&amp;=&amp; 2 \log n \sum_{i=0}^{\log n -1} 2^i - 4 \sum_{i=0}^{\log n -1} i 2^{i-1}\\
&amp;=&amp; 2 n \log n - 2 \log n - 2 n \log n + 4n -4\\
&amp;=&amp; 4n - 2 \log n - 4.
\end{eqnarray*}</math_block></topic></section><section ids="divide-and-conquer-recurrences" names="divide-and-conquer\ recurrences"><title>Divide-and-Conquer Recurrences</title><paragraph>The third approach to solving recurrences is to take advantage of
known theorems that provide the solution for classes of recurrences.
Of particular practical use is a theorem that gives the
answer for a class known as <title_reference>divide-and-conquer recurrences</title_reference>.
These have the form</paragraph><math_block xml:space="preserve">{\bf T}(n) = a{\bf T}(n/b) + cn^k; \quad {\bf T}(1) = c</math_block><paragraph>where <math>a</math>, <math>b</math>, <math>c</math>, and <math>k</math> are constants.
In general, this recurrence describes a problem of size <math>n</math>
divided into <math>a</math> subproblems of size <math>n/b</math>,
while <math>cn^k</math> is the amount of work necessary to combine the
partial solutions.
Mergesort is an example of a divide and conquer algorithm, and its
recurrence fits this form.
So does binary search.
We use the method of expanding recurrences to derive the general
solution for any divide and conquer recurrence, assuming that
<math>n = b^m</math>.</paragraph><math_block xml:space="preserve">\begin{eqnarray*}
{\bf T}(n) &amp; = &amp; a{\bf T}(n/b) + cn^k\\
  &amp; = &amp; a(a{\bf T}(n/b^2) + c(n/b)^k) + cn^k\\
  &amp; = &amp; a(a[a{\bf T}(n/b^3) + c(n/b^2)^k] + c(n/b)^k) + cn^k\\
  &amp; = &amp; a^m{\bf T}(1) + a^{m-1}c(n/b^{m-1})^k + \cdots + ac(n/b)^k + cn^k\\
  &amp; = &amp; a^mc + a^{m-1}c(n/b^{m-1})^k + \cdots + ac(n/b)^k + cn^k\\
  &amp; = &amp; c\sum_{i=0}^{m} a^{m-i} b^{ik}\\
  &amp; = &amp;ca^m\sum_{i=0}^{m} (b^k/a)^i.
\end{eqnarray*}</math_block><paragraph>Note that</paragraph><math_block xml:space="preserve">\begin{eqnarray}
\label{ThmEquiv}
a^m = a^{\log_bn} = n^{\log_ba}.
\end{eqnarray}</math_block><paragraph>The summation is a geometric series whose sum depends on the ratio
<math>r = b^k/a</math>.
There are three cases.</paragraph><enumerated_list enumtype="arabic" prefix="(" suffix=")"><list_item><paragraph><math>r&lt;1</math>.
From Equation (4) of Module :ref`summation &lt;summation&gt; &lt;Summation&gt;`,</paragraph><math_block xml:space="preserve">\sum_{i=0}^{m}r^i &lt; 1/(1-r),\ {\rm a~constant.}</math_block><paragraph>Thus,</paragraph><math_block xml:space="preserve">{\bf T}(n) = \Theta(a^m) = \Theta(n^{log_ba}).</math_block></list_item><list_item><paragraph>:math:r=1`.
Because <math>r = b^k/a</math>, we know that <math>a = b^k</math>.
From the definition of logarithms it follows immediately that
<math>k = \log_b a</math>.
We also note from Equation (1) above that <math>m = \log_b n</math>.
Thus,</paragraph><math_block xml:space="preserve">\sum_{i=0}^{m} r = m + 1 = \log_bn + 1.</math_block><paragraph>Because <math>a^m = n \log_b a = n^k</math>, we have</paragraph><math_block xml:space="preserve">{\bf T}(n) = \Theta(n^{\log_ba}\log n) = \Theta(n^k\log n).</math_block></list_item><list_item><paragraph><math>r&gt;1</math>.
From Equation (5) of Module :ref`summation &lt;summation&gt; &lt;Summation&gt;`,</paragraph><math_block xml:space="preserve">\sum_{i=0}^{m} r = \frac{r^{m+1} - 1}{r - 1} = \Theta(r^m).</math_block><paragraph>Thus,</paragraph><math_block xml:space="preserve">{\bf T}(n) = \Theta(a^mr^m)
           = \Theta(a^m(b^k/a)^m)
           = \Theta(b^{km})
           = \Theta(n^k).</math_block></list_item></enumerated_list><paragraph>We can summarize the above derivation as the following theorem,
sometimes referred to as the <title_reference>Master Theorem</title_reference>.</paragraph><target refid="recurthm"></target><topic ids="recurthm" names="recurthm"><title>Theorem</title><paragraph><strong>The Master Theorem:</strong> For any recurrence relation of the form
<math>{\bf T}(n) = a{\bf T}(n/b) + cn^k, {\bf T}(1) = c</math>,
the following relationships hold.</paragraph><math_block xml:space="preserve">{\bf T}(n) = \left\{ \begin{array}{ll}
             \Theta(n^{\log_ba}) &amp; \mbox{if \(a &gt; b^k\)} \\
             \Theta(n^k\log n)   &amp; \mbox{if \(a = b^k\)} \\
             \Theta(n^k)         &amp; \mbox{if \(a &lt; b^k\).}
            \end{array}
   \right.</math_block></topic><paragraph>This theorem may be applied whenever appropriate, rather than
re-deriving the solution for the recurrence.</paragraph><topic><title>Example</title><paragraph>Apply the Master Theorem to solve</paragraph><math_block xml:space="preserve">{\bf T}(n) = 3{\bf T}(n/5) + 8n^2.</math_block><paragraph>Because <math>a=3</math>, <math>b=5</math>, <math>c=8</math>, and <math>k=2</math>, we
find that <math>3&lt;5^2</math>.
Applying case (3) of the theorem, <math>{\bf T}(n) = \Theta(n^2)</math>.</paragraph></topic><topic><title>Example</title><paragraph>Use the Master Theorem to solve the recurrence relation
for Mergesort:</paragraph><math_block xml:space="preserve">{\bf T}(n) = 2{\bf T}(n/2) + n; \quad {\bf T}(1) = 1.</math_block><paragraph>Because <math>a=2</math>, <math>b=2</math>, <math>c=1</math>, and <math>k=1</math>,
we find that <math>2 = 2^1</math>.
Applying case (2) of the theorem,
<math>{\bf T}(n) = \Theta(n \log n)</math>.</paragraph></topic></section><section ids="average-case-analysis-of-quicksort" names="average-case\ analysis\ of\ quicksort"><title>Average-Case Analysis of Quicksort</title><paragraph>In Module <title_reference>Quicksort &lt;Quicksort&gt; &lt;Quicksort&gt;</title_reference>, we determined that
the average-case analysis of Quicksort had the following recurrence:</paragraph><math_block xml:space="preserve">{\bf T}(n) = cn + \frac{1}{n}\sum_{k=0}^{n-1} [{\bf T}(k) +
 {\bf T}(n -1 - k)], \qquad {\bf T}(0) = {\bf T}(1) = c.</math_block><paragraph>The <math>cn</math> term is an upper bound on the <title_reference>findpivot</title_reference> and
<title_reference>partition</title_reference> steps.
This equation comes from assuming that the partitioning element is
equally likely to occur in any position <math>k</math>.
It can be simplified by observing that the two
recurrence terms <math>{\bf T}(k)</math> and <math>{\bf T}(n - 1 - k)</math> are
equivalent, because one simply counts up from <math>T(0)</math> to
<math>T(n-1)</math> while the other counts down from <math>T(n-1)</math> to
<math>T(0)</math>.
This yields</paragraph><math_block xml:space="preserve">{\bf T}(n) = cn + \frac{2}{n}\sum_{k=0}^{n-1} {\bf T}(k).</math_block><paragraph>This form is known as a <title_reference>recurrence with full history</title_reference>.
The key to solving such a recurrence is to cancel out the summation
terms.
The shifting method for summations provides a way to do
this.
Multiply both sides by <math>n</math> and subtract the result from the
formula for <math>n{\bf T}(n+1)</math>:</paragraph><math_block xml:space="preserve">\begin{eqnarray*}
n{\bf T}(n) &amp; = &amp; cn^2 + 2 \sum_{k=1}^{n-1} {\bf T}(k)\\
(n+1){\bf T}(n+1) &amp; = &amp; c(n+1)^2 + 2 \sum_{k=1}^{n} {\bf T}(k).
\end{eqnarray*}</math_block><paragraph>Subtracting <math>n{\bf T}(n)</math> from both sides yields:</paragraph><math_block xml:space="preserve">\begin{eqnarray*}
(n+1){\bf T}(n+1) - n{\bf T}(n) &amp; = &amp; c(n+1)^2 - cn^2 + 2{\bf T}(n)\\
(n+1){\bf T}(n+1) - n{\bf T}(n) &amp; = &amp; c(2n+1) + 2{\bf T}(n)\\
(n+1){\bf T}(n+1) &amp; = &amp; c(2n+1) + (n+2){\bf T}(n)\\
{\bf T}(n+1) &amp; = &amp; \frac{c(2n+1)}{n+1} + \frac{n+2}{n+1}{\bf T}(n).
\end{eqnarray*}</math_block><paragraph>At this point, we have eliminated the summation and can now
use our normal methods for solving recurrences to get a closed-form
solution.
Note that <math>\frac{c(2n+1)}{n+1} &lt; 2c</math>, so we can simplify the
result.
Expanding the recurrence, we get</paragraph><math_block xml:space="preserve">\begin{eqnarray*}
{\bf T}(n+1) &amp; \leq &amp; 2c + \frac{n+2}{n+1} {\bf T}(n)\\
          &amp; = &amp; 2c + \frac{n+2}{n+1}\left (2c +
                     \frac{n+1}{n}{\bf T}(n-1)\right )\\
          &amp; = &amp; 2c + \frac{n+2}{n+1}\left (2c + \frac{n+1}{n}\left
                    (2c + \frac{n}{n-1}{\bf T}(n-2)\right )\right )\\
          &amp; = &amp; 2c + \frac{n+2}{n+1}\left (2c + \cdots +
                         \frac{4}{3}(2c + \frac{3}{2}{\bf T}(1))\right )\\
          &amp; = &amp; 2c\left (1 + \frac{n+2}{n+1}
                  + \frac{n+2}{n+1}\frac{n+1}{n} + \cdots
                  + \frac{n+2}{n+1}\frac{n+1}{n}\cdots\frac{3}{2}\right )\\
          &amp; = &amp; 2c\left (1 + (n+2)\left (\frac{1}{n+1}
                  + \frac{1}{n} + \cdots + \frac{1}{2}\right )\right )\\
          &amp; = &amp; 2c + 2c(n+2)\left ({\cal H}_{n+1} - 1\right )\\
\end{eqnarray*}</math_block><paragraph>for <math>{\cal H}_{n+1}</math>, the Harmonic Series.
From Equation (10) of Module :ref`summation &lt;summation&gt; &lt;Summation&gt;`,
<math>{\cal H}_{n+1} = \Theta(\log n)</math>,
so the final solution is <math>\Theta(n \log n)</math>.</paragraph><raw format="xml" xml:space="preserve"><odsascript>AV/Development/AlgAnal/DivideAndConquerRecurrencesCON.js</odsascript></raw></section></document>