{"document": {"@dupnames": "comparing\\ algorithms", "@ids": "comparing-algorithms", "@source": "<string>", "@title": "Comparing Algorithms", "title": "Comparing Algorithms", "comment": [{"@xml:space": "preserve", "#text": "This file is part of the OpenDSA eTextbook project. See"}, {"@xml:space": "preserve", "#text": "http://algoviz.org/OpenDSA for more details."}, {"@xml:space": "preserve", "#text": "Copyright (c) 2012-2016 by the OpenDSA Project Contributors, and"}, {"@xml:space": "preserve", "#text": "distributed under an MIT open source license."}], "raw": [{"@format": "xml", "@xml:space": "preserve", "avmetadata": "null"}, {"@format": "xml", "@xml:space": "preserve", "odsalink": "AV/AlgAnal/GrowthRatesCON.css"}, {"@format": "xml", "@xml:space": "preserve", "odsalink": "AV/AlgAnal/GrowthRatesZoomCON.css"}], "section": [{"@dupnames": "comparing\\ algorithms", "@ids": "id1", "title": "Comparing Algorithms", "section": [{"@ids": "introduction", "@names": "introduction", "title": "Introduction", "paragraph": ["How do you compare two algorithms for solving some problem in terms\nof efficiency?\nWe could implement both algorithms as computer programs and then\nrun them on a suitable range of inputs, measuring how much of the\nresources in question each program uses.\nThis approach is often unsatisfactory for four reasons.\nFirst, there is the effort involved in programming and testing two\nalgorithms when at best you want to keep only one.\nSecond, when empirically comparing two algorithms there\nis always the chance that one of the programs was \"better written\"\nthan the other, and therefore the relative qualities of the underlying\nalgorithms are not truly represented by their implementations.\nThis can easily occur when the programmer has a bias\nregarding the algorithms.\nThird, the choice of empirical test cases might unfairly favor one\nalgorithm.\nFourth, you could find that even the better of the two algorithms does\nnot fall within your resource budget.\nIn that case you must begin the entire process again with yet another\nprogram implementing a new algorithm.\nBut, how would you know if any algorithm can meet the resource budget?\nPerhaps the problem is simply too difficult for any implementation to\nbe within budget.", "These problems can often be avoided by using\nasymptotic analysis.\nAsymptotic analysis measures the efficiency of an algorithm, or its\nimplementation as a program, as the input size becomes large.\nIt is actually an estimating technique\nand does not tell us anything about the relative merits of two\nprograms where one is always \"slightly faster\" than the other.\nHowever, asymptotic analysis has proved useful\nto computer scientists who must determine if a particular algorithm\nis worth considering for implementation.", {"emphasis": ["time", "algorithm", "space", "data structure"], "#text": "The critical resource for a program is most often its running\ntime.\nHowever, you cannot pay attention to running time alone.\nYou must also be concerned with other factors such as the space\nrequired to run the program (both main memory and disk space).\nTypically you will analyze the  required for an\n (or the instantiation of an algorithm in the form\nof a program), and the  required for a\n."}, "Many factors affect the running time of a program.\nSome relate to the environment in which the program\nis compiled and run.\nSuch factors include the speed of the computer's CPU, bus, and\nperipheral hardware.\nCompetition with other users for the computer's (or the network's)\nresources can make a program slow to a crawl.\nThe programming language and the quality of code generated by a\nparticular compiler can have a significant\neffect.\nThe \"coding efficiency\" of the programmer who converts the algorithm\nto a program can have a tremendous impact as well.", "If you need to get a program working within time and space\nconstraints on a particular computer, all of these factors can be\nrelevant.\nYet, none of these factors address the differences between\ntwo algorithms or data structures.\nTo be fair, if you want to compare two programs derived from two\nalgorithms for solving the same problem, they should both be compiled\nwith the same compiler and run on the same computer under the same\nconditions.\nAs much as possible, the same amount of care should be taken in\nthe programming effort devoted to each program to make the\nimplementations \"equally efficient\".\nIn this sense, all of the factors mentioned above should cancel\nout of the comparison because they apply to both algorithms equally.", "If you truly wish to understand the running time of an algorithm,\nthere are other factors that are more appropriate to consider than\nmachine speed, programming language, compiler, and so forth.\nIdeally we would measure the running time of the algorithm under\nstandard benchmark conditions.\nHowever, we have no way to calculate the running time reliably other\nthan to run an implementation of the algorithm on some computer.\nThe only alternative is to use some other measure as a surrogate for\nrunning time."]}, {"@ids": "basic-operations-and-input-size", "@names": "basic\\ operations\\ and\\ input\\ size", "title": "Basic Operations and Input Size", "paragraph": {"title_reference": ["basic operations <basic operation>", "n", "n"], "#text": "Of primary consideration when estimating an algorithm's performance\nis the number of  required by\nthe algorithm to process an input of a certain size.\nThe terms \"basic operations\" and \"size\" are both\nrather vague and depend on the algorithm being analyzed.\nSize is often the number of inputs processed.\nFor example, when comparing sorting algorithms\nthe size of the problem is typically measured by the number of\nrecords to be sorted.\nA basic operation must have the property that its time to\ncomplete does not depend on the particular values of its operands.\nAdding or comparing two integer variables are examples of basic\noperations in most programming languages.\nSumming the contents of an array containing  integers is not,\nbecause the cost depends on the value of \n(i.e., the size of the input)."}, "target": {"@refid": "seqmax"}, "topic": [{"@ids": "seqmax", "@names": "seqmax", "title": "Example", "paragraph": [{"title_reference": "n", "emphasis": "largest-value sequential search", "#text": "Consider a simple algorithm to solve the problem of finding the\nlargest value in an array of  integers.\nThe algorithm looks at each integer in turn, saving the position of\nthe largest value seen so far.\nThis algorithm is called the \nand is illustrated by the following function:"}, {"literal": ["A.length", "A"], "#text": "Here, the size of the problem is ,\nthe number of integers stored in array .\nThe basic operation is to compare an integer's value to that\nof the largest value seen so far.\nIt is reasonable to assume that it takes a fixed amount of time to\ndo one such comparison, regardless of the value of the two\nintegers or their positions in the array."}, {"title_reference": ["n", "mathbf{T}", "n", "mathbf{T}(n)", "mathbf{T}(n)"], "#text": "Because the most important factor affecting running time is\nnormally size of the input, for a given input size  we\noften express the time  to  run the algorithm as\na function of , written as .\nWe will always assume  is a non-negative\nvalue."}, {"title_reference": ["c", "c", "i", "cn", "n", "c"], "literal": ["largest", "currlarge", "largest", "largest"], "#text": "Let us call  the amount of time required to compare two\nintegers in function .\nWe do not care right now what the precise value of  might\nbe.\nNor are we concerned with the time required to increment\nvariable  because this must be done for each value in the\narray, or the time for the actual assignment when a larger value is\nfound, or the little bit of extra time taken to initialize\n.\nWe just want a reasonable approximation for the time taken to\nexecute the algorithm.\nThe total time to run  is therefore approximately\n, because we must make  comparisons,\nwith each comparison costing  time.\nWe say that function \n(and by extension, the largest-value sequential search algorithm for\nany typical implementation) has a running time expressed\nby the equation"}, "This equation describes the growth rate for the running time of the\nlargest-value sequential search algorithm."], "raw": {"@format": "xml", "@xml:space": "preserve", "codeinclude": "null"}, "math_block": {"@xml:space": "preserve", "#text": "\\mathbf{T}(n) = cn."}}, {"title": "Example", "paragraph": [{"title_reference": ["c_1", "c_1"], "#text": "The running time of a statement that assigns the first value of an\ninteger array to a variable is simply the time required to copy the\nvalue of the first array value.\nWe can assume this assignment takes a constant amount of time\nregardless of the value.\nLet us call  the amount of time necessary to copy an\ninteger.\nNo matter how large the array on a typical computer\n(given reasonable conditions for memory and array size), the time\nto copy the value from the first position of the array is always\n.\nThus, the equation for this algorithm is simply"}, {"title_reference": ["n", "constant running time"], "#text": "indicating that the size of the input  has no effect on\nthe running time.\nThis is called a ."}], "math_block": {"@xml:space": "preserve", "#text": "\\mathbf{T}(n) = c_1,"}}, {"title": "Example", "paragraph": ["Consider the following code:", {"title_reference": ["n", "c_2", "c_2", "n^2", "mathbf{T}(n) = c_2 n^2"], "literal": ["sum", "sum", "i", "j"], "#text": "What is the running time for this code fragment?\nClearly it takes longer to run when  is larger.\nThe basic operation in this example is the\nincrement operation for variable .\nWe can assume that incrementing takes constant time;\ncall this time .\n(We can ignore the time required to initialize ,\nand to increment the loop counters  and .\nIn practice, these costs can safely be bundled into time\n.)\nThe total number of increment operations is .\nThus, we say that the running time is\n."}], "raw": {"@format": "xml", "@xml:space": "preserve", "codeinclude": "null"}}]}, {"@ids": "growth-rates", "@names": "growth\\ rates", "title": "Growth Rates", "paragraph": [{"title_reference": "growth rate", "#text": "The  for an algorithm is the rate at which the cost\nof the algorithm grows as the size of its input grows.\nThe following figure shows a graph for six equations,\neach meant to describe the running time for a particular program or\nalgorithm.\nA variety of growth rates that are representative of typical\nalgorithms are shown."}, {"title_reference": ["10n", "20n", "cn", "c", "linear growth rate", "n", "n", "n^2", "quadratic growth rate", "2n^2", "2^n", "exponential growth rate", "n", "n!"], "#text": "The two equations labeled  and  are graphed by\nstraight lines.\nA growth rate of  (for  any positive constant) is\noften referred to as a  or running time.\nThis means that as the value of  grows, the running time of\nthe algorithm grows in the same proportion.\nDoubling the value of  roughly doubles the running time.\nAn algorithm whose running-time equation has a highest-order term\ncontaining a factor of  is said to have a\n.\nIn the figure, the line labeled \nrepresents a quadratic growth rate.\nThe line labeled  represents an\n.\nThis name comes from the fact that  appears in the exponent.\nThe line labeled  also grows exponentially."}, {"title_reference": ["mathbf{T}(n) = 10n", "mathbf{T}(n) = 2n^2", "n", "n > 5", "mathbf{T}(n) = 2n^2", "10n", "2n^2", "20n", "2n^2", "n>10", "mathbf{T}(n) = 2n^2", "mathbf{T}(n) = 20n", "mathbf{T}(n) = 5 n log n", "mathbf{T}(n) = 10 n", "mathbf{T}(n) = 20 n", "mathbf{T}(n) = 2n^2", "a, b > 1, n^a", "log^b n", "log n^b", "mathbf{T}(n) = 2^n", "mathbf{T}(n) = n!", "n", "a, b geq 1, a^n", "n^b"], "#text": "As you can see from the figure,\nthe difference between an algorithm whose running time has cost\n and another with cost\n becomes tremendous as  grows.\nFor , the algorithm with running time\n is already much slower.\nThis is despite the fact that  has a greater constant\nfactor than .\nComparing the two curves marked  and  shows\nthat changing the constant factor for one of the equations only shifts\nthe point at which the two curves cross.\nFor , the algorithm with cost \nis slower than the algorithm with cost .\nThis graph also shows that the equation\n\ngrows somewhat more quickly than both  and\n, but not nearly so quickly as the\nequation .\nFor constants  grows faster than either\n or .\nFinally, algorithms with cost  or\n are prohibitively expensive for even modest\nvalues of .\nNote that for constants  grows faster than\n."}, "We can get some further insight into relative growth rates for various\nalgorithms from the following table.\nMost of the growth rates that appear in typical algorithms are shown,\nalong with some representative input sizes.\nOnce again, we see that the growth rate has a tremendous effect on the\nresources consumed by an algorithm."], "target": [{"@refid": "runtimegraph"}, {"@refid": "growthtable"}], "raw": [{"@format": "xml", "@ids": "runtimegraph", "@names": "runtimegraph", "@xml:space": "preserve", "inlineav": {"@type": "dgm", "@exer_name": "GrowthRatesCON", "@long_name": "GrowthRatesCON", "@points": "0", "@required": "True", "@threshold": "1.0"}}, {"@format": "xml", "@xml:space": "preserve", "inlineav": {"@type": "dgm", "@exer_name": "GrowthRatesZoomCON", "@long_name": "GrowthRatesZoomCON", "@points": "0", "@required": "True", "@threshold": "1.0"}}, {"@format": "xml", "@xml:space": "preserve", "avembed": {"@type": "ka", "@exer_name": "CompareGrowth", "@long_name": "CompareGrowth", "@points": "1.0", "@required": "True", "@threshold": "5"}}], "line_block": {"line": null}, "topic": {"@ids": "growthtable", "@names": "growthtable", "title": "Table", "paragraph": "Costs for representative growth rates.", "math_block": {"@xml:space": "preserve", "#text": "\\begin{array}{c|c|c|c|c|c|c|c}\n\\mathsf{n} & \\mathsf{\\log \\log n} & \\mathsf{\\log n} & \\mathsf{n} &\n\\mathsf{n \\log n} & \\mathsf{n^2} & \\mathsf{n^3} & \\mathsf{2^n}\\\\\n\\hline\n\\mathsf{16} & \\mathsf{2} & \\mathsf{4} & \\mathsf{2^{4}} &\n\\mathsf{4 \\cdot 2^{4} = 2^{6}} &\n\\mathsf{2^{8}} & \\mathsf{2^{12}} & \\mathsf{2^{16}}\\\\\n\\mathsf{256} & \\mathsf{3} & \\mathsf{8} & \\mathsf{2^{8}} &\n\\mathsf{8 \\cdot 2^{8} = 2^{11}} &\n\\mathsf{2^{16}} & \\mathsf{2^{24}} & \\mathsf{2^{256}}\\\\\n\\mathsf{1024} & \\mathsf{\\approx 3.3} & \\mathsf{10} & \\mathsf{2^{10}} &\n\\mathsf{10 \\cdot 2^{10} \\approx 2^{13}} &\n\\mathsf{2^{20}} & \\mathsf{2^{30}} & \\mathsf{2^{1024}}\\\\\n\\mathsf{64 {\\rm K}} & \\mathsf{4} & \\mathsf{16} & \\mathsf{2^{16}} &\n\\mathsf{16 \\cdot 2^{16} = 2^{20}} &\n\\mathsf{2^{32}} & \\mathsf{2^{48}} & \\mathsf{2^{64 {\\rm K}}}\\\\\n\\mathsf{1 {\\rm M}} & \\mathsf{\\approx 4.3} & \\mathsf{20} & \\mathsf{2^{20}} &\n\\mathsf{20 \\cdot 2^{20} \\approx 2^{24}} &\n\\mathsf{2^{40}} & \\mathsf{2^{60}} & \\mathsf{2^{1 {\\rm M}}}\\\\\n\\mathsf{1 {\\rm G}} & \\mathsf{\\approx 4.9} & \\mathsf{30} & \\mathsf{2^{30}} &\n\\mathsf{30 \\cdot 2^{30} \\approx 2^{35}} &\n\\mathsf{2^{60}} & \\mathsf{2^{90}} & \\mathsf{2^{1 {\\rm G}}}\\\\\n\\end{array}"}}}]}, {"@ids": "growth-rates-ordering-exercise", "@names": "growth\\ rates\\ ordering\\ exercise", "title": "Growth Rates Ordering Exercise", "raw": [{"@format": "xml", "@xml:space": "preserve", "avembed": {"@type": "ka", "@exer_name": "GrowthRatesPRO", "@long_name": "GrowthRatesPRO", "@points": "1.0", "@required": "True", "@threshold": "5"}}, {"@format": "xml", "@xml:space": "preserve", "todo": "null"}, {"@format": "xml", "@xml:space": "preserve", "odsascript": "DataStructures/Plot.js"}, {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/AlgAnal/GrowthRatesCON.js"}, {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/AlgAnal/GrowthRatesZoomCON.js"}]}]}}