<?xml version="1.0" encoding="utf8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.1 -->
<document ids="number-problems" names="number\ problems" source="&lt;string&gt;" title="Number Problems"><title>Number Problems</title><comment xml:space="preserve">This file is part of the OpenDSA eTextbook project. See</comment><comment xml:space="preserve">http://algoviz.org/OpenDSA for more details.</comment><comment xml:space="preserve">Copyright (c) 2012-2013 by the OpenDSA Project Contributors, and</comment><comment xml:space="preserve">distributed under an MIT open source license.</comment><raw format="xml" xml:space="preserve"><avmetadata>null</avmetadata></raw><section ids="introduction" names="introduction"><title>Introduction</title><paragraph>This moudle presents a variety of algorithms related to mathematical
computations on numbers.
Examples are activities like multiplying two numbers or raising a
number to a given power.
In particular, we are concerned with situations where built-in integer
or floating-point operations cannot be used because the values being
operated on are too large.
Similar concerns arise for operations on polynomials or matrices.</paragraph><paragraph>Since we cannot rely on the hardware to process the inputs in a single
constant-time operation, we are concerned with how to most effectively
implement the operation to minimize the time cost.
This begs a question as to how we should apply our normal measures of
asymptotic cost in terms of growth rates on input size.
First, what is an instance of addition or multiplication?
Each value of the operands yields a different problem instance.
And what is the input size when multiplying two numbers?
If we view the input size as two (since two numbers are input),
then any non-constant-time algorithm has a growth rate that is
infinitely high compared to the growth of the input.
This makes no sense, especially in light of the fact that we know from
grade school arithmetic that adding or multiplying numbers does seem
to get more difficult as the value of the numbers involved increases.
In fact, we know from standard grade school algorithms that the cost
of
standard addition is linear on the number of digits being added, and
multiplication has cost <math>n \times m</math> when multiplying an
<math>m</math> -digit
number by an <math>n</math> -digit number.</paragraph><paragraph>The number of digits for the operands does appear to be a key
consideration when we are performing a numeric algorithm that is
sensitive to input size.
The number of digits is simply the log of the value, for a suitable
base of the log.
Thus, for the purpose of calculating asymptotic growth rates of
algorithms, we will consider the "size" of an input value to be the
log of that value.
Given this view, there are a number of features that seem to relate
such operations.</paragraph><bullet_list bullet="*"><list_item><paragraph>Arithmetic operations on large values are not cheap.</paragraph></list_item><list_item><paragraph>There is only one instance of value $n$.</paragraph></list_item><list_item><paragraph>There are <math>2^k</math> instances of length <math>k</math> or less.</paragraph></list_item><list_item><paragraph>The size (length) of value <math>n</math> is <math>\log n</math>.</paragraph></list_item><list_item><paragraph>The cost of a particular algorithm can decrease when <math>n</math>
increases in value (say when going from a value of <math>2^k-1</math>
to <math>2^k</math> to <math>2^k+1</math>),
but generally increases when <math>n</math> increases in length.</paragraph></list_item></bullet_list></section><section ids="exponentiation" names="exponentiation"><title>Exponentiation</title><paragraph>We will start our examination of standard numerical algorithms by
considering how to perform exponentiation.
That is, how do we compute <math>m^n</math>?
We could multiply by <math>m</math> a total of <math>n-1</math> times.
Can we do better?
Yes, there is a simple divide and conquer approach that we can use.
We can recognize that, when <math>n</math> is even,
<math>m^n = m^{n/2}m^{n/2}</math>.
If <math>n</math> is odd, then
<math>m^n = m^{\lfloor n/2\rfloor}m^{\lfloor n/2\rfloor}m</math>.
This leads to the following recursive algorithm:</paragraph><literal_block xml:space="preserve">int Power(int base, int exp) {
  int half, total;
  if exp = 0 return 1;
  half = Power(base, exp/2);
  total = half * half;
  if (odd(exp)) then total = total * base;
  return total;
}</literal_block><paragraph>Function <title_reference>Power</title_reference> has recurrence relation</paragraph><math_block xml:space="preserve">f(n) = \left\{
\begin{array}{ll}
0&amp;n=1\\
f(\lfloor n/2\rfloor) + 1 + n \bmod 2&amp;n&gt;1
\end{array}
\right.</math_block><paragraph>This has solution</paragraph><math_block xml:space="preserve">f(n) = \lfloor \log n\rfloor + \beta(n) - 1</math_block><paragraph>where <math>\beta</math> is the number of 1's in the binary
representation of <math>n</math>.</paragraph><paragraph>How does this cost compare with the problem size?
The original problem size is <math>\log m + \log n</math>,
and the number of multiplications required is <math>\log n</math>.
This is far better (in fact, exponentially better) than performing
<math>n-1</math> multiplications.</paragraph></section><section ids="largest-common-factor" names="largest\ common\ factor"><title>Largest Common Factor</title><paragraph>We will next present Euclid's algorithm for finding the largest common
factor (LCF) for two integers.
The LCF is the largest integer that divides both inputs evenly.</paragraph><paragraph>First we make this observation: If <math>k</math> divides <math>n</math> and
<math>m</math>, then <math>k</math> divides <math>n - m</math>.
We know this is true because if <math>k</math> divides <math>n</math> then
<math>n = ak</math> for some integer <math>a</math>, and if <math>k</math> divides
<math>m</math> then <math>m = bk</math> for some integer <math>b</math>.
So, <math>LCF(n, m) = LCF(n-m, n) = LCF(m, n-m) = LCF(m, n)</math>.</paragraph><paragraph>Now, for any value <math>n</math> there exists <math>k</math> and <math>l</math> such
that</paragraph><math_block xml:space="preserve">n = km + l\ \mbox{where}\ m &gt; l \geq 0.</math_block><paragraph>From the definition of the <math>\bmod</math> function, we can derive
the fact that</paragraph><math_block xml:space="preserve">n = \lfloor n/m \rfloor m + n \bmod m.</math_block><paragraph>Since the LCF is a factor of both <math>n</math> and <math>m</math>,
and since <math>n = km + l</math>, the LCF must therefore be a factor of both
<math>km</math> and  <math>l</math>, and also the largest common factor of each
of these terms.
As a consequence, <math>LCF(n, m) = LCF(m, l) = LCF(m, n \bmod m)</math>.</paragraph><paragraph>This observation leads to a simple algorithm.
We will assume that <math>n \geq m</math>.
At each iteration we replace <math>n</math> with <math>m</math> and
<math>m</math> with <math>n \bmod m</math> until we have driven <math>m</math> to
zero:</paragraph><literal_block xml:space="preserve">int LCF(int n, int m) {
  if (m == 0) return n;
  return LCF(m, n % m);
}</literal_block><paragraph>To determine how expensive this algorithm is, we need to know how much
progress we are making at each step.
Note that after two iterations, we have replaced
<math>n</math> with <math>n \bmod m</math>.
So the key question becomes:
How big is <math>n \bmod m</math> relative to <math>n</math>?</paragraph><math_block xml:space="preserve">\begin{eqnarray*}
n \geq m &amp;\Rightarrow&amp; n/m \geq 1\\
&amp;\Rightarrow&amp; 2\lfloor n/m\rfloor &gt; n/m\\
&amp;\Rightarrow&amp; m\lfloor n/m\rfloor &gt; n/2\\
&amp;\Rightarrow&amp; n - n/2 &gt; n - m\lfloor n/m\rfloor = n \bmod m\\
&amp;\Rightarrow&amp; n/2 &gt; n \bmod m
\end{eqnarray*}</math_block><paragraph>Thus, function LCF will halve its first parameter in no more than 2
iterations.
The total cost is then <math>O(\log n)</math>.</paragraph></section></document>