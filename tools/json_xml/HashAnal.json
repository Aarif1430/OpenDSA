{"document": {"@dupnames": "analysis\\ of\\ closed\\ hashing", "@ids": "analysis-of-closed-hashing", "@source": "<string>", "@title": "Analysis of Closed Hashing", "title": "Analysis of Closed Hashing", "subtitle": {"@dupnames": "analysis\\ of\\ closed\\ hashing", "@ids": "id1", "#text": "Analysis of Closed Hashing"}, "comment": [{"@xml:space": "preserve", "#text": "This file is part of the OpenDSA eTextbook project. See"}, {"@xml:space": "preserve", "#text": "http://algoviz.org/OpenDSA for more details."}, {"@xml:space": "preserve", "#text": "Copyright (c) 2012-2013 by the OpenDSA Project Contributors, and"}, {"@xml:space": "preserve", "#text": "distributed under an MIT open source license."}], "raw": [{"@format": "xml", "@xml:space": "preserve", "avmetadata": "null"}, {"@format": "xml", "@xml:space": "preserve", "todo": "null"}, {"@format": "xml", "@ids": "hashplot", "@names": "hashplot", "@xml:space": "preserve", "odsafig": "null"}, {"@format": "xml", "@xml:space": "preserve", "avembed": {"@type": "ka", "@exer_name": "HashAnalSumm", "@long_name": "HashAnalSumm", "@points": "1.0", "@required": "True", "@threshold": "5"}}], "paragraph": ["How efficient is hashing?\nWe can measure hashing performance in terms of the number of\nrecord accesses required when performing an operation.\nThe primary operations of concern are insertion, deletion, and search.\nIt is useful to distinguish between successful and unsuccessful searches.\nBefore a record can be deleted, it must be found.\nThus, the number of accesses required to delete a record is\nequivalent to the number required to successfully search for it.\nTo insert a record, an empty slot along the record's probe\nsequence must be found.\nThis is equivalent to an\nunsuccessful search for the record\n(recall that a successful search for the record during insertion\nshould generate an error because two records with the same key are\nnot allowed to be stored in the table).", "When the hash table is empty, the first record inserted will always\nfind its home position free.\nThus, it will require only one record access to find a free slot.\nIf all records are stored in their home positions, then successful\nsearches will also require only one record access.\nAs the table begins to fill up, the probability that a record can be\ninserted into its home position decreases.\nIf a record hashes to an occupied slot, then the collision resolution\npolicy must locate another slot in which to store it.\nFinding records not stored in their home position also requires\nadditional record accesses as the record is searched for along its probe\nsequence.\nAs the table fills up, more and more records are likely to be located\never further from their home positions.", {"title_reference": ["load factor", "alpha = N/M", "N"], "#text": "From this discussion, we see that the expected cost of hashing is a\nfunction of how full the table is.\nDefine the \nfor the table as ,\nwhere  is the number of records currently in the table."}, {"title_reference": ["alpha", "alpha", "(N(N-1))/(M(M-1))", "i", "(N(N-1) ... (N-i+1))/(M(M-1) ... (M-i+1))", "N", "M", "(N/M)^i", "i >= 1", "i"], "#text": "An estimate of the expected cost for an insertion (or an unsuccessful\nsearch) can be derived analytically as a function of  in the\ncase where we assume that the probe sequence follows a random\npermutation of the slots in the hash\ntable.\nAssuming that every slot in the table has equal probability of being\nthe home slot for the next record,\nthe probability of finding the home position occupied is\n.\nThe probability of finding both the home position occupied and the\nnext slot on the probe sequence occupied is .\nThe probability of  collisions is\n.\nIf  and  are large,\nthen this is approximately .\nThe expected number of probes is one plus the sum over\n of the probability of  collisions,\nwhich is approximately"}, {"title_reference": ["alpha", "alpha", "(1/alpha) log_e 1/(1-alpha)."], "#text": "The cost for a successful search (or a deletion) has the same cost as\noriginally inserting that record.\nHowever, the expected value for the insertion cost depends on the\nvalue of  not at the time of deletion, but rather at the time\nof the original insertion.\nWe can derive an estimate of this cost (essentially an average over all\nthe insertion costs) by integrating from 0 to the current value of\n, yielding a result of"}, {"title_reference": [".5(1 + 1/(1-alpha)^2)", ".5(1 + 1/(1-alpha))"], "#text": "It is important to realize that these equations represent the expected\ncost for operations when using the unrealistic assumption that the\nprobe sequence is based on a random permutation of the slots in the\nhash table.\nWe thereby avoid all the expense that results from a less-than-perfect\ncollision resolution policy.\nThus, these costs are lower-bound estimates in the average case.\nThe true average cost under linear\nprobing is  for\ninsertions or unsuccessful searches and\n for deletions or successful\nsearches."}, {"title_reference": ["Figure #HashPlot", "alpha", "alpha"], "#text": "Figure \nshows how the expected number of record accesses grows as\n grows.\nThe horizontal axis is the value for  , the vertical axis\nis the expected number of accesses to the hash table.\nSolid lines show the cost for \"random\" probing (a theoretical lower\nbound on the cost), while dashed lines\nshow the cost for linear probing (a relatively poor collision\nresolution strategy).\nThe two leftmost lines show the cost for insertion\n(equivalently, unsuccessful search);\nthe two rightmost lines show the cost for deletion\n(equivalently, successful search)."}, {"title_reference": ["log n", "alpha", "alpha"], "#text": "From the figure, you should see that the cost for\nhashing when the table is not too full is typically close to one\nrecord access.\nThis is extraordinarily efficient, much better than\nbinary search which requires  record accesses.\nAs  increases, so does the expected cost.\nFor small values of , the expected cost is low.\nIt remains below two until the hash table is about half full.\nWhen the table is nearly empty, adding a new record to the table\ndoes not increase the cost of future search operations by much.\nHowever, the additional search cost caused by each additional\ninsertion increases rapidly once the table becomes half full.\nBased on this analysis, the rule of thumb is to design a hashing\nsystem so that the hash table never gets above about\nhalf full, because beyond that point performance will degrade rapidly.\nThis requires that the implementor have some idea of how many records\nare likely to be in the table at maximum loading, and select the\ntable size accordingly.\nThe goal should be to make the table small enough so that it does not\nwaste a lot of space on the one hand, while making it big enough to\nkeep performance good on the other."}], "math_block": {"@xml:space": "preserve", "#text": "1 + \\sum_{i=1}^\\infty (N/M)^i = 1/(1-\\alpha)."}, "target": {"@refid": "hashplot"}}}