{"document": {"@ids": "self-organizing-lists", "@names": "self-organizing\\ lists", "@source": "<string>", "@title": "Self-Organizing Lists", "title": "Self-Organizing Lists", "comment": [{"@xml:space": "preserve", "#text": "This file is part of the OpenDSA eTextbook project. See"}, {"@xml:space": "preserve", "#text": "http://algoviz.org/OpenDSA for more details."}, {"@xml:space": "preserve", "#text": "Copyright (c) 2012-2013 by the OpenDSA Project Contributors, and"}, {"@xml:space": "preserve", "#text": "distributed under an MIT open source license."}], "raw": [{"@format": "xml", "@xml:space": "preserve", "avmetadata": "null"}, {"@format": "xml", "@xml:space": "preserve", "odsalink": "AV/Development/selforgCON.css"}], "section": [{"@ids": "introduction", "@names": "introduction", "title": "Introduction", "paragraph": [{"title_reference": ["key", "sequential search"], "#text": "While ordering of lists is most commonly done by  value,\nthis is not the only viable option.\nAnother approach to organizing lists to speed search is to order the\nrecords by expected frequency of access.\nWhile the benefits might not be as great as when sorted by key\nvalue, the cost to organize (at least approximately) by frequency of\naccess can be much cheaper,\nand thus can speed up  in some situations."}, {"math": ["k_i", "p_i", "k_i"], "#text": "Assume that we know, for each key , the probability\n that the record with key  will be requested.\nAssume also that the list is ordered so that the most frequently\nrequested record is first, then the next most frequently requested\nrecord, and so on.\nSearch in the list will be done sequentially, beginning with the\nfirst position.\nOver the course of many searches, the expected number of comparisons\nrequired for one search is"}, {"strong": ["L", "L"], "math": ["p_0", "p_1", "n", "p_0", "p_{n-1}"], "#text": "In other words, the cost to access the record in\n [0] is 1 (because one key value is looked at), and the\nprobability of this occurring is .\nThe cost to access the record in  [1] is 2 (because\nwe must look at the first and the second records' key values),\nwith probability , and so on.\nFor  records, assuming that all searches are\nfor records that actually exist, the probabilities  through\n must sum to one."}, "Certain probability distributions give easily computed results.", "A geometric probability distribution can yield quite different\nresults.", {"title_reference": ["80/20 rule", "caching", "buffer pool <buffer pool> <BuffPool>", "disk drive"], "#text": "In many search applications, real access patterns follow a rule of\nthumb called the .\nThe 80/20 rule says that 80% of the record accesses are to 20%\nof the records.\nThe values of 80 and 20 are only estimates; every data access pattern\nhas its own values.\nHowever, behavior of this nature occurs surprisingly often in practice\n(which explains the success of  techniques widely\nused by web browsers for speeding access to web pages,\nand the use of a\n to speed access\nto data stored in slower memory such as a ).\nWhen the 80/20 rule applies, we can expect considerable improvements\nto search performance from a list ordered by frequency of access over\nstandard sequential search in an unordered list."}, "This is potentially a useful observation that typical \"real-life\"\ndistributions of record accesses, if the records were ordered by\nfrequency, would require that we visit on average only 10-15% of the\nlist when doing sequential search.\nThis means that if we had an application that used sequential search,\nand we wanted to make it go a bit faster (by a constant amount), we\ncould do so without a major rewrite to the system to implement\nsomething like a search tree.\nBut that is only true if there is an easy way to (at least\napproximately) order the records by frequency.", {"title_reference": "Self-organizing lists <self-organizing list>", "#text": "In most applications, we have no means of knowing in advance the\nfrequencies of access for the data records.\nTo complicate matters further, certain records might be accessed\nfrequently for a brief period of time, and then rarely thereafter.\nThus, the probability of access for records might change over time (in\nmost database systems, this is to be expected).\n seek to solve\nboth of these problems."}, {"title_reference": "buffer pools <buffer pool> <BuffPool>", "#text": "Self-organizing lists modify the order of records within the\nlist based on the actual pattern of record access.\nSelf-organizing lists use a heuristic for\ndeciding how to reorder the list.\nThese heuristics are similar to the rules for managing\n.\nIn fact, a buffer pool is a form of self-organizing list.\nOrdering the buffer pool by expected frequency of access is a good\nstrategy, because typically we must search the contents of the buffers\nto determine if the desired information is already in main memory.\nWhen ordered by frequency of access, the buffer at the end of the\nlist will be the one most appropriate for reuse when a new page\nof information must be read."}], "math_block": {"@xml:space": "preserve", "#text": "\\overline{C}_n = 1 p_0 + 2 p_1 + ... + n p_{n-1}."}, "topic": [{"title": "Example", "paragraph": [{"math": "p_i = 1/n", "#text": "Calculate the expected cost to search a list\nwhen each record has equal chance of being accessed (the classic\nsequential search through an unsorted list).\nSetting  yields"}, {"title_reference": "more general case <SortedSearch>", "math": "p_n", "#text": "This result matches our expectation that half the records will be\naccessed on average by normal sequential search.\nIf the records truly have equal access probabilities, then ordering\nrecords by frequency yields no benefit.\nIn the ,\nwe must consider the probability (labeled ) that\nthe search key does not match that for any record in the array.\nIn that case, the general formula gives us"}, {"math": "\\frac{n+1}{2} \\leq \\overline{C}_n \\leq n", "#text": "Thus, ,\ndepending on the value of (p_0)."}], "math_block": [{"@xml:space": "preserve", "#text": "\\overline{C}_n = \\sum_{i=1}^n i/n = (n+1)/2."}, {"@xml:space": "preserve", "#text": "(1-p_n) \\frac{n+1}{2} + p_n n =\n\\frac{n + 1 - p_n n - p_n + 2 p_n n}{2} =\n\\frac{n + 1 + p_n (n - 1)}{2}."}]}, {"title": "Example", "paragraph": ["Calculate the expected cost for searching a list ordered by\nfrequency when the probabilities are defined as", "Then,", "For this example, the expected number of accesses is a constant.\nThis is because the probability for accessing the first record is\nhigh (one half), the second is much lower (one quarter) but still\nmuch higher than for the third record, and so on.\nThis shows that for some probability distributions, ordering the\nlist by frequency can yield an efficient search technique."], "math_block": [{"@xml:space": "preserve", "#text": "p_i = \\left\\{ \\begin{array}{ll}\n  1/2^i & \\mbox{if \\(0 \\leq i \\leq n-2\\)}\\\\\n  1/2^n & \\mbox{if \\(i = n-1\\).}\n\\end{array} \\right."}, {"@xml:space": "preserve", "#text": "\\overline{C}_n \\approx \\sum_{i=0}^{n-1} (i+1)/2^{i+1} =\n   \\sum_{i=1}^n (i/2^i) \\approx 2."}]}, {"title": "Example", "paragraph": [{"title_reference": ["Zipf distribution", "Harmonic Series <Harmonic series> <Summations>"], "math": ["i", "n", "1/(i {\\cal H}_n)"], "#text": "The 80/20 rule is an example of a\n.\nNaturally occurring distributions often follow a Zipf distribution.\nExamples include the observed frequency for the use of words in a\nnatural language such as English, and the size of the population for\ncities (i.e., view the relative proportions for the populations as\nequivalent to the \"frequency of use\").\nZipf distributions are related to the\n.\nDefine the Zipf frequency for item  in the distribution for\n records as .\nThe expected cost for the series whose members follow this Zipf\ndistribution will be"}, "When a frequency distribution follows the 80/20 rule, the\naverage search looks at about 10-15% of the records in a list\nordered by frequency."], "math_block": {"@xml:space": "preserve", "#text": "\\overline{C}_n = \\sum_{i=1}^n i/i {\\cal H}_n = n/{\\cal H}_n \\approx\nn/\\log_e n."}}], "comment": {"@xml:space": "preserve", "#text": "ZipfExamp_"}}, {"@ids": "frequency-count", "@names": "frequency\\ count", "title": "Frequency Count", "paragraph": ["There are three traditional heuristics for managing self-organizing\nlists.", {"title_reference": ["frequency count", "least frequently used"], "#text": "The most obvious way to keep a list ordered by frequency would be to\nstore a count of accesses to each record and always maintain records\nin this order.\nThis method will be referred to as  or just\n\"count\".\nCount is similar to the  buffer\nreplacement strategy.\nWhenever a record is accessed, it might move toward the front of\nthe list if its number of accesses becomes greater than a record\npreceding it.\nThus, count will store the records in the order of frequency\nthat has actually occurred so far.\nBesides requiring space for the access counts, count does not\nreact well to changing frequency of access over time.\nOnce a record has been accessed a large number of times under the\nfrequency count system, it will\nremain near the front of the list regardless of further access\nhistory."}], "raw": [{"@format": "xml", "@xml:space": "preserve", "inlineav": {"@type": "ss", "@exer_name": "SelforgCON1", "@long_name": "SelforgCON1", "@points": "0", "@required": "True", "@threshold": "1.0"}}, {"@format": "xml", "@xml:space": "preserve", "avembed": {"@type": "ka", "@exer_name": "SelfOrgCounterPro", "@long_name": "SelfOrgCounterPro", "@points": "1.0", "@required": "True", "@threshold": "5"}}]}, {"@ids": "move-to-front", "@names": "move\\ to\\ front", "title": "Move to Front", "paragraph": {"title_reference": ["least recently used", "move-to-front", "optimal static ordering", "amortized analysis <amortized analysis> <AmortAnal>"], "math": ["n", "n", "n"], "#text": "Bring a record to the front of the list when it is\nfound, pushing all the other records back one position.\nThis is analogous to the \nbuffer replacement strategy and is called\n.\nThis heuristic is easy to implement if the records are stored using\na linked list.\nWhen records are stored in an array, bringing a record forward from\nnear the end of the array will result in a\nlarge number of records (slightly) changing position.\nMove-to-front's cost is bounded in the sense that it requires at\nmost twice the number of accesses required by the\n for  records when at least\n searches are performed.\nIn other words, if we had known the series of (at least )\nsearches in advance and had stored the records in order of frequency\nso as to minimize the total cost for these accesses, this cost would\nbe at least half the cost required by the move-to-front heuristic.\n(This can be proved using\n.)\nFinally, move-to-front responds well to local changes in frequency\nof access, in that if a record is frequently accessed for a brief\nperiod of time it will be near the front of the list during that\nperiod of access.\nMove-to-front does poorly when the records are processed in\nsequential order, especially if that sequential order is then\nrepeated multiple times."}, "raw": [{"@format": "xml", "@xml:space": "preserve", "inlineav": {"@type": "ss", "@exer_name": "SelforgCON2", "@long_name": "SelforgCON2", "@points": "0", "@required": "True", "@threshold": "1.0"}}, {"@format": "xml", "@xml:space": "preserve", "avembed": {"@type": "ka", "@exer_name": "SelfOrgMove-to-FrontPro", "@long_name": "SelfOrgMove-to-FrontPro", "@points": "1.0", "@required": "True", "@threshold": "5"}}]}, {"@ids": "transpose", "@names": "transpose", "title": "Transpose", "paragraph": {"title_reference": "transpose", "math": ["X", "Y", "Y", "Y", "X", "X", "Y"], "#text": "Swap any record found with the record immediately\npreceding it in the list.\nThis heuristic is called .\nTranspose is good for list implementations based on either linked\nlists or arrays.\nFrequently used records will, over time, move to the front of the\nlist.\nRecords that were once frequently accessed but are no longer used\nwill slowly drift toward the back.\nThus, it appears to have good properties with respect to changing\nfrequency of access.\nUnfortunately, there are some pathological sequences of access that\ncan make transpose perform poorly.\nConsider the case where the last record of the list\n(call it ) is accessed.\nThis record is then swapped with the next-to-last record\n(call it ), making  the last record.\nIf  is now accessed, it swaps with .\nA repeated series of accesses alternating between \nand  will continually search to the end of the list,\nbecause neither record will ever make progress toward the front.\nHowever, such pathological cases are unusual in practice.\nA variation on transpose would be to move the accessed record\nforward in the list by some fixed number of steps."}, "raw": [{"@format": "xml", "@xml:space": "preserve", "inlineav": {"@type": "ss", "@exer_name": "SelforgCON3", "@long_name": "SelforgCON3", "@points": "0", "@required": "True", "@threshold": "1.0"}}, {"@format": "xml", "@xml:space": "preserve", "avembed": {"@type": "ka", "@exer_name": "SelfOrgTransposePro", "@long_name": "SelfOrgTransposePro", "@points": "1.0", "@required": "True", "@threshold": "5"}}]}, {"@ids": "an-example", "@names": "an\\ example", "title": "An Example", "paragraph": [{"math": "O(\\log n)", "#text": "While self-organizing lists do not generally perform as well\nas search trees or a sorted list, both of which require\n search time, there are many situations in which\nself-organizing lists prove a valuable tool.\nObviously they have an advantage over sorted lists in that they need\nnot be sorted.\nThis means that the cost to insert a new record is low, which could\nmore than make up for the higher search cost when insertions are\nfrequent.\nSelf-organizing lists are simpler to implement than search trees and\nare likely to be more efficient for small lists.\nNor do they require additional space.\nFinally, in the case of an application where sequential\nsearch is \"almost\" fast enough, changing an\nunsorted list to a self-organizing list might speed the\napplication enough at a minor cost in additional code."}, {"footnote_reference": {"@auto": "1", "@ids": "id1", "@refid": "id2", "#text": "1"}, "#text": "As an example of applying self-organizing lists, consider an\nalgorithm for compressing and transmitting messages. \nThe list is self-organized by the move-to-front rule.\nTransmission is in the form of words and numbers, by the following\nrules:"}, "Both the sender and the receiver keep track of the position of words\nin the list in the same way (using the move-to-front rule), so\nthey agree on the meaning of the numbers that encode repeated\noccurrences of words.\nConsider the following example message to be transmitted\n(for simplicity, ignore case in letters).", {"literal": "The car on the left hit the car I left"}, "The first three words have not been seen before, so they must be sent\nas full words.\nThe fourth word is the second appearance of \"the\" which at this\npoint is the third word in the list.\nThus, we only need to transmit the position value \"3\".\nThe next two words have not yet been seen, so must be sent as full\nwords.\nThe seventh word is the third appearance of \"the\", which\ncoincidentally is again in the third position.\nThe eighth word is the second appearance of \"car\", which is now in the\nfifth position of the list.\n\"I\" is a new word, and the last word \"left\" is now in the fifth\nposition.\nThus the entire transmission would be", {"literal": "The car on 3 left hit 3 5 I 5"}, "This approach to compression is similar in spirit to\nZiv-Lempel coding, which is a class of coding algorithms commonly used\nin file compression utilities.\nZiv-Lempel coding replaces repeated occurrences of strings with a\npointer to the location in the file of the first occurrence of the\nstring.\nThe codes are stored in a self-organizing list in order to speed\nup the time required to search for a string that has previously been\nseen."], "enumerated_list": {"@enumtype": "arabic", "@prefix": "", "@suffix": ".", "list_item": [{"paragraph": "If the word has been seen before, transmit the current position of\nthe word in the list.\nMove the word to the front of the list."}, {"paragraph": "If the word is seen for the first time, transmit the word.\nPlace the word at the front of the list."}]}, "footnote": {"@auto": "1", "@backrefs": "id1", "@ids": "id2", "@names": "1", "label": "1", "paragraph": {"emphasis": "Communications of the ACM 29", "#text": "The compression algorithm and the example used both come from\nthe following paper:\nJ.L. Bentley, D.D. Sleator, R.E. Tarjan, and V.K. Wei,\n\"A Locally Adaptive Data Compression Scheme\",\n, 4(April 1986), 320-330."}}, "raw": {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/Development/selforgCON.js"}}]}}