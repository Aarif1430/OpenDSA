{"document": {"@dupnames": "analyzing\\ problems", "@ids": "analyzing-problems", "@source": "<string>", "@title": "Analyzing Problems", "title": "Analyzing Problems", "subtitle": {"@dupnames": "analyzing\\ problems", "@ids": "id1", "#text": "Analyzing Problems"}, "comment": [{"@xml:space": "preserve", "#text": "This file is part of the OpenDSA eTextbook project. See"}, {"@xml:space": "preserve", "#text": "http://algoviz.org/OpenDSA for more details."}, {"@xml:space": "preserve", "#text": "Copyright (c) 2012-2016 by the OpenDSA Project Contributors, and"}, {"@xml:space": "preserve", "#text": "distributed under an MIT open source license."}], "raw": [{"@format": "xml", "@xml:space": "preserve", "avmetadata": "null"}, {"@format": "xml", "@xml:space": "preserve", "avembed": {"@type": "ka", "@exer_name": "AnalProblemSumm", "@long_name": "AnalProblemSumm", "@points": "1.0", "@required": "True", "@threshold": "5"}}], "paragraph": [{"title_reference": ["algorithm", "program", "problem"], "#text": "You most often use the techniques of \"algorithm\" analysis to analyze\nan , or the instantiation of an algorithm as a\n.\nYou can also use these same techniques to analyze the cost of a\n.\nThe key question that we want to ask is: How hard is a problem?\nCertainly we should expect that in some sense, the problem of sorting a\nlist of records is harder than the problem of searching a list of\nrecords for a given key value.\nCertainly the algorithms that we know for sorting some records seem to\nbe more expensive than the algorithms that we know for searching those\nsame records."}, {"title_reference": ["upper bound", "lower bound"], "#text": "What we need are useful definitions for the  and\n of a problem."}, {"strong": "best", "#text": "One might start by thinking that the upper bound for a problem is how\nhard any algorithm can be for the problem.\nBut we can make algorithms as bad as we want, so that is not useful.\nInstead, what is useful is to say that a problem is only as hard as\nwhat we CAN do.\nIn other words, we should define the upper bound for a problem to be\nthe  algorithm that we know for the problem.\nOf course, whenever we talk about bounds, we have to say when they\napply.\nWe we really should say something like the best algorithm that we know\nin the worst case, or the best algorithm that we know in the average\ncase."}, "But what does it mean to give a lower bound for a problem?\nLower bound refers to the minimum that any algorithm MUST cost.\nFor example, when searching an unsorted list, we MUST look at every\nrecord.\nWhen sorting a list, we MUST look at every record (to even know if it\nis sorted).", {"math": ["\\Omega(f(n))", "\\Omega(f(n))", "\\Omega(f(n))", "\\Omega(f(n))"], "emphasis": "every", "#text": "It is much easier to show that an algorithm (or program) is in\n than it is to show that a problem is in\n.\nFor a problem to be in  means that \nalgorithm that solves the problem is in ,\neven algorithms that we have not thought of!\nIn other words, EVERY algorithm MUST have at least this cost.\nSo, to prove a lower bound, we need an argument that is true, even for\nalgorithms that we don't know."}, {"math": ["\\Omega", "\\Omega", "\\Theta"], "#text": "So far all of our examples of algorithm analysis\ngive \"obvious\" results, with big-Oh always matching .\nTo understand how big-Oh, , and  notations\nare properly used to describe our understanding of a problem or an\nalgorithm, it is best to consider an example where you do not already\nknow a lot about the problem."}, {"math": ["cn", "n", "\\Omega(n)"], "#text": "Let us look ahead to analyzing the problem of sorting to see\nhow this process works.\nWhat is the least possible cost for any sorting algorithm\nin the worst case?\nThe algorithm must at least look at every element in the input, just\nto determine that the input is truly sorted.\nThus, any sorting algorithm must take at least  time.\nFor many problems, this observation that each of the  inputs\nmust be looked at leads to an easy  lower bound."}, {"math": ["O(n^2)", "O(n^2)", "O(n^2)", "\\Omega(n)", "O(n^2)", "O(n^2)", "\\Omega(n)"], "#text": "In your previous study of computer science, you have probably\nseen an example of a sorting algorithm whose running time is in\n in the worst case.\nThe simple Bubble Sort and Insertion Sort algorithms\ntypically given as examples in a first year programming course have\nworst case running times in .\nThus, the problem of sorting can be said to have an upper bound\nin .\nHow do we close the gap between  and ?\nCan there be a better sorting algorithm?\nIf you can think of no algorithm whose worst-case growth rate is\nbetter than , and if you have discovered no\nanalysis technique to show that the least cost for the problem of\nsorting in the worst case is greater than ,\nthen you cannot know for sure whether or not there is a better\nalgorithm."}, {"math": ["O(n \\log n)", "\\Omega(n)", "O(n \\log n)", "\\Omega(n \\log n)", "c n \\log n", "n", "\\Theta(n \\log n)"], "title_reference": "we can prove that <sorting lower bound> <SortingLowerBound>", "footnote_reference": {"@auto": "1", "@ids": "id2", "@refid": "id3", "#text": "1"}, "#text": "Many good sorting algorithms have running time that is\nin  in the worst case.\nThis greatly narrows the gap.\nWith this new knowledge, we now have a lower bound in\n and an upper bound in .\nShould we search for a faster algorithm?\nMany have tried, without success.\nFortunately (or perhaps unfortunately?),\n\nany sorting algorithm must have running\ntime in  in the worst case. \nThis proof is one of the most important results in\nthe field of algorithm analysis, and it means that no sorting\nalgorithm can possibly run faster than  for the\nworst-case input of size .\nThus, we can conclude that the problem of sorting is\n in the worst case, because the upper and\nlower bounds have met."}, "Knowing the lower bound for a problem does not give you a good\nalgorithm.\nBut it does help you to know when to stop looking.\nIf the lower bound for the problem matches the upper bound for the\nalgorithm (within a constant factor), then we know that we can find an\nalgorithm that is better only by a constant factor.", "So, to summarize:\nThe upper bound for a problem is the best that you CAN do,\nwhile the lower bound for a problem is the least work that you MUST\ndo.\nIf those two are the same, then we say that we really understand our\nproblem."], "footnote": {"@auto": "1", "@backrefs": "id2", "@ids": "id3", "@names": "1", "label": "1", "paragraph": {"math": ["\\Theta(n \\log n)", "\\Theta(n)"], "#text": "While it is fortunate to know the truth, it is unfortunate that\nsorting is  rather than ."}}}}