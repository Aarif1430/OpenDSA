{"document": {"@ids": "search-in-sorted-arrays", "@names": "search\\ in\\ sorted\\ arrays", "@source": "<string>", "@title": "Search in Sorted Arrays", "title": "Search in Sorted Arrays", "subtitle": {"@ids": "analysis", "@names": "analysis", "#text": "Analysis"}, "comment": [{"@xml:space": "preserve", "#text": "This file is part of the OpenDSA eTextbook project. See"}, {"@xml:space": "preserve", "#text": "http://algoviz.org/OpenDSA for more details."}, {"@xml:space": "preserve", "#text": "Copyright (c) 2012-2013 by the OpenDSA Project Contributors, and"}, {"@xml:space": "preserve", "#text": "distributed under an MIT open source license."}], "raw": [{"@format": "xml", "@xml:space": "preserve", "avmetadata": "null"}, {"@format": "xml", "@xml:space": "preserve", "odsalink": "AV/Searching/binarySearchCON.css"}], "paragraph": {"strong": "L", "math": ["K", "K"], "#text": "For large collections of records that are searched repeatedly,\nsequential search is unacceptably slow.\nOne way to reduce search time is to preprocess the records by\nsorting them.\nGiven a sorted array,\nan obvious improvement over simple linear search is to test if the\ncurrent element in  is greater than .\nIf it is, then we know that  cannot appear later in the\narray, and we can quit the search early.\nBut this still does not improve the worst-case cost of the algorithm."}, "section": [{"@ids": "jump-search", "@names": "jump\\ search", "title": "Jump Search", "paragraph": [{"strong": ["L", "L", "L", "L"], "title_reference": "K", "math": ["K", "K", "K", "K", "K"], "#text": "We can also observe that if we look first at position 1 in sorted\narray  and find that  is bigger, then we rule out\nposition 0 as well as position 1.\nBecause more is often better, what if we look at position 2 in\n and find that  is bigger yet?\nThis rules out positions 0, 1, and 2 with one comparison.\nWhat if we carry this to the extreme and look first at the last\nposition in  and find that  is bigger?\nThen we know in one comparison that  is not in .\nThis is useful to know, but what is wrong with the conclusion\nthat we should always start by looking at the last position?\nThe problem is that, while we learn a lot sometimes (in one comparison\nwe might learn that  is not in the list), usually we learn\nonly a little bit (that the last element is not )."}, {"title_reference": "Jump Search", "math": ["j", "j", "\\mathbf{L}[j]", "\\mathbf{L}[2j]", "K", "K", "j-1", "K"], "strong": ["L", "L"], "#text": "The question then becomes: What is the right amount to jump?\nThis leads us to an algorithm known as .\nFor some value , we check every  'th element in\n, that is, we check elements ,\n, and so on.\nSo long as  is greater than the values we are checking, we\ncontinue on.\nBut when we reach a value in  greater than , we do a\nlinear search on the piece of length  that we know brackets\n if it is in the list."}, {"math": ["m", "mj \\leq n < (m+1)j", "m + j - 1", "K", "\\mathbf{L}[i]", "K", "\\mathbf{L}[i]", "n", "j"], "#text": "If we define  such that ,\nthen the total cost of this algorithm is at most \n3-way comparisons.\n(They are 3-way because at each comparison of  with some\n we need to know if  is less than,\nequal to, or greater than .)\nTherefore, the cost to run the algorithm on  items with a\njump of size  is"}, {"math": "j", "#text": "What is the best value that we can pick for ?\nWe want to minimize the cost:"}, {"math": ["f'(j) = 0", "j = \\sqrt{n}", "2\\sqrt{n}"], "#text": "Take the derivative and solve for  to find the\nminimum, which is .\nIn this case, the worst case cost will be\nroughly ."}, {"title_reference": "divide and conquer", "#text": "This example invokes a basic principle of algorithm design.\nWe want to balance the work done while selecting a sublist with the\nwork done while searching a sublist.\nIn general, it is a good strategy to make subproblems of equal effort.\nThis is an example of a\n algorithm."}, {"math": ["j", "j-1", "K", "j_1", "j_1 - 1"], "#text": "What if we extend this idea to three levels?\nWe would first make jumps of some size  to find a sublist of\nsize  whose end values bracket value .\nWe would then work through this sublist by making jumps of some\nsmaller size, say .\nFinally, once we find a bracketed sublist of size , we\nwould do sequential search to complete the process."}, {"title_reference": "binary search <binary search> <AnalProgram>", "#text": "This probably sounds convoluted to do two levels of jumping to be\nfollowed by a sequential search.\nWhile it might make sense to do a two-level algorithm (that is, jump\nsearch jumps to find a sublist and then does sequential search on the\nsublist),\nit almost never seems to make sense to do a three-level algorithm.\nInstead, when we go beyond two levels, we nearly always generalize by\nusing recursion.\nThis leads us to the most commonly used search algorithm for sorted\narrays, the ."}], "math_block": [{"@xml:space": "preserve", "#text": "\\mathbf{T}(n, j) = m + j - 1 =\n\\left\\lfloor \\frac{n}{j} \\right\\rfloor + j - 1."}, {"@xml:space": "preserve", "#text": "\\min_{1 \\leq j \\leq n} \\left\\{\\left\\lfloor\\frac{n}{j}\\right\\rfloor +\nj - 1\\right\\}"}]}, {"@ids": "binary-search", "@names": "binary\\ search", "title": "Binary Search", "paragraph": ["You are probably pretty familiar with Binary Search already.\nSo that we have a concrete example to discuss, here is an\nimplementation.", {"math": ["O(\\log n)", "O(\\log n"], "strong": "exact", "#text": "Of couse you know that Binary Search is far better than Sequential\nWhy would that be?\nBecause we have additional information to work with that we do not\nhave when the list is unsorted.\nYou probably already \"know\" that the standard binary search algorithm\nhas a worst case cost of .\nLet's do the math to make sure that it really is in\n, and see how to handle the nasty details of modeling\nthe  behavior of a recursive algorithm.\nAfter that, we can deal with proving that Binary Search is indeed\noptimal (at least in the worst case) for solving the problem of search\nin a sorted list."}, {"math": ["f(n) = 1 + f(n/2)", "(9-0)/2 = 4", "(10-0)/2 = 5"], "#text": "If we are willing to be casual about our analysis, we can reason\nthat we look at one element (for a cost of one), and then repeat the\nprocess on half of the array.\nThis would give us a recurrence that looks like\n.\nBut if we want to be more precise, then we need to think carefully\nabout what is going on in the worst case.\nFirst, we should notice that we are doing a little more than cutting\nthe array in half.\nWe never look again at a particular position that we test.\nFor example, if the input size is nine, then we actually look at\nposition 4 (since  when rounded down), and we then\neither continue to consider four positions to the left\n(positions 0 to 3) or four positions to the right (positions 5 to 8).\nBut what if there are ten element?\nThen we actually look at position 5 (since ).\nWe will then either need to continue dealing with five positions to\nthe left (positions 0 to 4), or four positions to the right.\nWhich means that in the worst case, we are looking at a little less\nthan half when the array size is odd, or exactly half when the array\nsize is even.\nTo capture this, we can use the floor function, to get an exact worst\ncase model as follows:"}, {"math": ["n/2 \\geq \\lfloor n/2 \\rfloor", "f(n)", "f(n) = f(n/2) + 1"], "#text": "Since ,\nand since  is assumed to be\nnon-decreasing (since adding more elements won't decrease the work)\nwe can estimate the upper bound with the simplification\n."}, "This recurrence is fairly easy to solve via expansion:", "Then, collapse to", "Now, we can prove that this is correct with induction.", {"math": "f(n/2) = \\log(n/2) + 1", "#text": "By the IH, ."}, "How do we calculate the average cost for Binary Search?\nThis requires some modeling, because we need to know things about the\nprobabilities of the various inputs.\nWe will estimate given these assumptions:", "What is the cost?", "What is the resulting equation?", {"math": "2^{\\log n-1} = n/2", "#text": "Note that ."}, "To solve the summation:", {"math": "i \\rightarrow i+1", "#text": "Note that in the above series of equations, we change variables:\n."}, "Now what?  Subtract from the original!", "Note that", "So,", "Now we come back to solving the original equation.\nSince we have a closed-form solution for the summation in hand, we can\nrestate the equation with the appropriate variable substitutions.", "So the average cost is only about one or two comparisons less than the\nworst cost.", {"math": "n = 2^k - 1", "#text": "If we want to relax the assumption that , we get\nthis as the exact cost:"}, "Identify each of the components of this equation as follows:"], "raw": {"@format": "xml", "@xml:space": "preserve", "inlineav": {"@type": "ss", "@exer_name": "binarySearchCON", "@long_name": "binarySearchCON", "@points": "0", "@required": "True", "@threshold": "1.0"}}, "math_block": [{"@xml:space": "preserve", "#text": "f(n) = \\left\\{\n\\begin{array}{ll}\n1 & n=1\\\\\nf(\\lfloor n/2 \\rfloor) + 1 & n > 1\n\\end{array}\n\\right."}, {"@xml:space": "preserve", "#text": "\\begin{eqnarray*}\nf(n) &=& f(n/2) + 1\\\\\n&=& \\{f(n/4) + 1\\} + 1\\\\\n&=& \\{\\{f(n/8) + 1\\} + 1\\} + 1\n\\end{eqnarray*}"}, {"@xml:space": "preserve", "#text": "f(n) = f(n/2^i) + i = f(1) + \\log n = \\log n + 1"}, {"@xml:space": "preserve", "#text": "\\begin{eqnarray*}\nf(n/2) + 1 &=& (\\log(n/2) + 1) + 1\\\\\n&=& (\\log n - 1 + 1) + 1\\\\\n&=& \\log n + 1 = f(n).\n\\end{eqnarray*}"}, {"@xml:space": "preserve", "#text": "\\frac{1\\times 1 + 2\\times 2 + 3 \\times 4 + ... + \\log n 2^{\\log n-1}}{n}\n= \\frac{1}{n}\\sum_{i=1}^{\\log n}i 2^{i-1}"}, {"@xml:space": "preserve", "#text": "\\begin{eqnarray*}\n\\sum_{i=1}^k i2^{i-1} &=& \\sum_{i=0}^{k-1}(i+1)2^i\n= \\sum_{i=0}^{k-1} i 2^i + \\sum_{i=0}^{k-1} 2^i\\\\\n&=& 2 \\sum_{i=0}^{k-1} i 2^{i-1} + 2^k - 1\\\\\n&=& 2 \\sum_{i=1}^{k} i 2^{i-1} - k 2^k + 2^k - 1\n\\end{eqnarray*}"}, {"@xml:space": "preserve", "#text": "\\sum_{i=1}^{k} i 2^{i-1} = k 2^k - 2^k + 1 = (k - 1)2^k + 1."}, {"@xml:space": "preserve", "#text": "\\sum_{i=1}^k i 2^{i-1} = 2 \\sum_{i=1}^k i 2^{i-1} - k 2^k + 2^k -1"}, {"@xml:space": "preserve", "#text": "\\begin{eqnarray*}\n\\sum_{i=1}^k i 2^{i-1} &=& k2^k - 2^k +1\\\\\n&=& (k-1)2^k +1\n\\end{eqnarray*}"}, {"@xml:space": "preserve", "#text": "\\begin{eqnarray*}\n\\frac{1}{n}\\sum_{i=1}^{\\log n}i 2^{i-1} &=&\n\\frac{(\\log n - 1)2^{\\log n} + 1}{n}\\\\\n&=& \\frac{n (\\log n -1) + 1}{n}\\\\\n&\\approx& \\log n - 1\n\\end{eqnarray*}"}, {"@xml:space": "preserve", "#text": "f(n) = \\left\\{\n\\begin{array}{ll}\n0 & n=0\\\\\n1 & n=1\\\\\n\\frac{\\lceil \\frac{n}{2} \\rceil - 1}{n}f(\\lceil \\frac{n}{2}\n\\rceil - 1) +\n\\frac{1}{n} 0\\ + \\\\\n\\frac{\\lfloor \\frac{n}{2} \\rfloor}{n}f(\\lfloor \\frac{n}{2} \\rfloor) + 1&\nn > 1\n\\end{array}\n\\right."}], "enumerated_list": {"@enumtype": "arabic", "@prefix": "", "@suffix": ".", "list_item": [{"paragraph": {"math": "X", "strong": "L", "#text": "is in ."}}, {"paragraph": {"math": "X", "#text": "is equally likely to be in any position."}}, {"paragraph": {"math": ["n = 2^k - 1", "k"], "#text": "for some non-negative integer ."}}]}, "bullet_list": [{"@bullet": "*", "list_item": [{"paragraph": "There is one chance to hit in one probe."}, {"paragraph": "There are two chances to hit in two probes."}, {"paragraph": {"math": ["2^{i-1}", "i"], "#text": "There are  chances to hit in  probes."}}, {"paragraph": {"math": "i \\leq k", "#text": "."}}]}, {"@bullet": "*", "list_item": [{"paragraph": {"math": "X < L[i]", "#text": "Left side:"}}, {"paragraph": {"math": ["L(i) == X", "1/n"], "#text": "has no additional cost, with chance"}}, {"paragraph": {"math": "X > L[i]", "#text": "Right side:"}}]}]}, {"@ids": "lower-bounds-proof", "@names": "lower\\ bounds\\ proof", "title": "Lower Bounds Proof", "paragraph": [{"math": "O(\\log n)", "#text": "So,  time for Binary Search seems pretty good.\nCan we do better than this?\nWe can prove that this is the best possible algorithm in the worst\ncase for searching in a sorted list by using a proof similar to that\nused to show the lower bound on sorting."}, {"strong": ["L", "L", "L", "L", "L", "L", "L", "L"], "math": ["K", "K", "K", "n+1", "n+1", "K", "\\log n", "\\Omega(\\log n)"], "#text": "We use the decision tree to model our algorithm.\nUnlike when searching an unsorted list, comparisons between elements\nof  tell us nothing new about their relative order (since \nis already sorted), so we consider only comparisons between \nand an element in .\nAt the root of the decision tree, our knowledge rules out no positions\nin , so all are potential candidates.\nAs we take branches in the decision tree based on the result of\ncomparing  to an element in , we gradually rule out\npotential candidates.\nEventually we reach a leaf node in the tree representing the single\nposition in  that can contain .\nThere must be at least  nodes in the tree because we have\n distinct positions that  can be in (any position\nin , plus not in  at all).\nSome path in the tree must be at least  levels deep, and\nthe deepest node in the tree represents the worst case for that\nalgorithm.\nThus, any algorithm on a sorted array requires at least\n comparisons in the worst case."}, {"title_reference": "total path length", "math": "n", "#text": "We can modify this proof to find the average cost lower bound.\nAgain, we model algorithms using decision trees.\nExcept now we are interested not in the depth of the deepest node (the\nworst case) and therefore the tree with the least-deepest node.\nInstead, we are interested in knowing what the minimum possible is for\nthe \"average depth\" of the leaf nodes.\nDefine the  as the sum of the levels for each\nnode.\nThe cost of an outcome is the level of the corresponding node plus 1.\nThe average cost of the algorithm is the average cost of the outcomes\n(total path length / ).\nWhat is the tree with the least average depth?\nThis is equivalent to the tree that corresponds to binary search.\nThus, binary search is optimal in the average case."}, {"strong": "L", "math": ["K", "\\Theta(\\log \\log n)", "O(\\log n)"], "title_reference": ["interpolation search <interpolation search> <SortedSearch>", "self-organizing list <self-organizing list> <SelfOrg>"], "#text": "While binary search is indeed an optimal algorithm for a sorted list\nin the worst and average cases when searching a sorted array, there\nare a number of circumstances that might lead us to select another\nalgorithm instead.\nOne possibility is that we know something about the distribution of\nthe data in the array.\nIf each position in  is equally likely to hold \n(equivalently, the data are\nwell distributed along the full key range), then an\n\nis  in the average case.\nIf the data are not sorted, then using binary search requires us to\npay the cost of sorting the list in advance, which is only worthwhile\nif many (at least  searches will be performed on the\nlist.\nBinary search also requires that the list (even if sorted) be\nimplemented using an array or some other structure that supports\nrandom access to all elements with equal cost.\nFinally, if we know all search requests in advance, we might prefer to\nsort the list by frequency and do linear search in extreme search\ndistributions, or use a\n."}]}, {"@ids": "interpolation-and-quadratic-binary-search", "@names": "interpolation\\ and\\ quadratic\\ binary\\ search", "title": "Interpolation and Quadratic Binary Search", "paragraph": [{"title_reference": ["dictionary search", "interpolation search"], "strong": "L", "math": ["p", "K"], "#text": "If we know nothing about the distribution of key values,\nthen we have just proved that binary search is the best\nalgorithm available for searching a sorted array.\nHowever, sometimes we do know something about the expected\nkey distribution.\nConsider the typical behavior of a person looking up a word in\na large dictionary.\nMost people certainly do not use sequential search!\nTypically, people use a modified form of binary search, at least until\nthey get close to the word that they are looking for.\nThe search generally does not start at the middle of the dictionary.\nA person looking for a word starting with 'S'\ngenerally assumes that entries beginning with 'S' start about three\nquarters  of the way through the dictionary.\nThus, he or she will first open the dictionary about three quarters of\nthe way through and then make a decision based on what is found as to\nwhere to look next.\nIn other words, people typically use some knowledge about the\nexpected distribution of key values to \"compute\" where to look next.\nThis form of \"computed\" binary search is called a\n or .\nIn a dictionary search, we search  at a position  that\nis appropriate to the value of  as follows."}, {"math": "K", "#text": "This equation is computing the position of  as a fraction of\nthe distance between the smallest and largest key values.\nThis will next be translated into that position which is the same\nfraction of the way through the array,\nand this position is checked first.\nAs with binary search, the value of the key found eliminates\nall records either above or below that position.\nThe actual value of the key found can then be used to\ncompute a new position within the remaining range of the array.\nThe next check is made based on the new computation.\nThis proceeds until either the desired record is found, or the array\nis narrowed until no records are left."}, {"math": ["Quadratic Binary Search", "\\mathbf{L}[\\lceil pn\\rceil]", "K < \\mathbf{L}[\\lceil pn\\rceil]", "\\sqrt{n}"], "#text": "A variation on dictionary search is known as\n (QBS),\nand we will analyze this in detail because its analysis is easier than\nthat of the general dictionary search.\nQBS will first compute (p) and then examine\n.\nIf  then QBS will sequentially\nprobe to the left by steps of size , that is, we step\nthrough"}, {"math": ["K", "K > \\mathbf{L}[\\lceil pn\\rceil]", "\\sqrt{n}", "K", "\\sqrt{n}", "K", "K", "\\sqrt{n}", "\\sqrt{\\sqrt{n}}"], "strong": "L", "#text": "until we reach a value less than or equal to .\nSimilarly for \nwe will step to the right by  until we reach a value\nin  that is greater than .\nWe are now within  positions of .\nAssume (for now) that it takes a constant number of comparisons to\nbracket  within a sublist of size .\nWe then take this sublist and repeat the process recursively.\nThat is, at the next level we compute an interpolation to start\nsomewhere in the subarray.\nWe then step to the left or right (as appropriate) by steps of size\n."}, {"math": ["\\sqrt{c^n} =c^{n/2}", "n = 2^{\\log n}", "\\log n", "\\log \\log n", "\\Theta(\\log \\log n)"], "emphasis": "if", "#text": "What is the cost for QBS?\nNote that , and we will be repeatedly\ntaking square roots of the current sublist size until we find the item\nthat we are looking for.\nBecause  and we can cut  in half\nonly  times, the cost is \n the number of probes on jump search is constant."}, {"math": ["i", "i", "i", "\\mathbf{P}_i", "i"], "#text": "Say that the number of comparisons needed is , in which case\nthe cost is  (since we have to do  comparisons).\nIf  is the probability of needing exactly \nprobes, then"}, "We now show that this is the same as", "We require at least two probes to set the bounds, so the cost is", {"math": ["\\mathbf{P}(\\mbox{need exactly}\\ i\\ \\mbox{probes})", "\\mathbf{P}_i"], "#text": "We now make take advantage of a useful fact known as Chebyshev's\nInequality.\nChebyshev's inequality states that\n,\nor , is"}, {"math": ["p(1-p) \\leq 1/4", "p"], "#text": "because  for any probability .\nThis assumes uniformly distributed data.\nThus, the expected number of probes is"}, {"math": ["O(\\log \\log n)", "O(\\log n)", "c_1 \\log n", "c_2 \\log \\log n", "\\log \\log n", "\\log n"], "#text": "Is QBS better than binary search?\nTheoretically yes, because  grows slower than\n.\nHowever, we have a situation here which illustrates the limits to the\nmodel of asymptotic complexity in some practical situations.\nYes,  does grow faster than .\nIn fact, it is exponentially faster!\nBut even so, for practical input sizes, the absolute cost difference\nis fairly small.\nThus, the constant factors might play a role.\nFirst we compare  to ."}, "It is not always practical to reduce an algorithm's growth rate.\nThere is a \"practicality window\" for every problem, in that we have\na practical limit to how big an input we wish to solve for.\nIf our problem size never grows too big, it might not matter if we can\nreduce the cost by an extra log factor, because the constant factors\nin the two algorithms might differ by more than the log of the log of\nthe input size.", {"math": ["\\log n-1", "2.4 \\log \\log n"], "#text": "For our two algorithms, let us look further and check the actual\nnumber of comparisons used.\nFor binary search, we need about  total comparisons.\nQuadratic binary search requires about \ncomparisons.\nIf we incorporate this observation into our table, we get a different\npicture about the relative differences."}, "But we still are not done.\nThis is only a count of raw comparisons.\nBinary search is inherently much simpler than QBS,\nbecause binary search only needs to calculate the midpoint position of\nthe array before each comparison, while quadratic binary search must\ncalculate an interpolation point which is more expensive.\nSo the constant factors for QBS are even higher.", "Not only are the constant factors worse on average, but QBS\nis far more dependent than binary search on good data\ndistribution to perform well.\nFor example, imagine that you are searching a telephone directory for\nthe name \"Young\".\nNormally you would look near the back of the book.\nIf you found a name beginning with 'Z', you might look just a little\nways toward the front.\nIf the next name you find also begins with 'Z' you would look a\nlittle further toward the front.\nIf this particular telephone directory were unusual in that half of\nthe entries begin with 'Z', then you would need to move toward\nthe front many times, each time eliminating relatively few records\nfrom the search.\nIn the extreme, the performance of interpolation search might not be\nmuch better than sequential search if the distribution of key values\nis badly calculated.", "While it turns out that QBS is not a practical algorithm,\nthis is not a typical situation.\nFortunately, algorithm growth rates are usually well behaved, so that\nasymptotic algorithm analysis nearly always gives us a practical\nindication for which of two algorithms is better."], "math_block": [{"@xml:space": "preserve", "#text": "p = \\frac{K - \\mathbf{L}[1]}{\\mathbf{L}[n] - \\mathbf{L}[1]}"}, {"@xml:space": "preserve", "#text": "\\mathbf{L}[\\lceil pn - i\\sqrt{n}\\rceil], i = 1, 2, 3, ..."}, {"@xml:space": "preserve", "#text": "\\sum_{i=1}^{\\sqrt{n}} i \\mathbf{P}(\\mbox{need exactly $i$ probes})\\\\\n= 1 \\mathbf{P}_1 + 2 \\mathbf{P}_2 + 3 \\mathbf{P}_3 + \\cdots +\n  \\sqrt{n} \\mathbf{P}_{\\sqrt{n}}"}, {"@xml:space": "preserve", "#text": "\\sum_{i=1}^{\\sqrt{n}} \\mathbf{P}(\\mbox{need at least $i$ probes})"}, {"@xml:space": "preserve", "#text": "&=& 1 + (1-\\mathbf{P}_1) + (1-\\mathbf{P}_1-\\mathbf{P}_2) +\n    \\cdots + \\mathbf{P}_{\\sqrt{n}}\\\\\n&=& (\\mathbf{P}_1 + ... + \\mathbf{P}_{\\sqrt{n}}) +\n (\\mathbf{P}_2 + ... + \\mathbf{P}_{\\sqrt{n}}) +\\\\\n&& \\qquad    (\\mathbf{P}_3 + ... + \\mathbf{P}_{\\sqrt{n}}) + \\cdots\\\\\n&=& 1 \\mathbf{P}_1 + 2 \\mathbf{P}_2 + 3 \\mathbf{P}_3 + \\cdots +\n    \\sqrt{n} \\mathbf{P}_{\\sqrt{n}}"}, {"@xml:space": "preserve", "#text": "2 + \\sum_{i=3}^{\\sqrt{n}} \\mathbf{P}(\\mbox{need at least \\(i\\) probes})."}, {"@xml:space": "preserve", "#text": "\\mathbf{P}_i \\leq \\frac{p(1 - p)n}{(i - 2)^2 n} \\leq\n\\frac{1}{4(i-2)^2}"}, {"@xml:space": "preserve", "#text": "2 + \\sum_{i=3}^{\\sqrt{n}} \\frac{1}{4(i-2)^2}\n< 2 + \\frac{1}{4}\\sum_{i=1}^\\infty \\frac{1}{i^2} =\n2 + \\frac{1}{4}\\frac{\\pi}{6} \\approx 2.4112"}, {"@xml:space": "preserve", "#text": "\\begin{array}{llll}\n&&&{\\rm Factor}\\\\\nn  &\\log n&\\log \\log n&{\\rm Difference}\\\\\n\\hline\n16 &4    &2        &2\\\\\n256&8    &3        &2.7\\\\\n2^{16}&16   &4        &4\\\\\n2^{32}&32  &5      &6.4\\\\\n\\end{array}"}, {"@xml:space": "preserve", "#text": "\\begin{array}{llll}\n&&&{\\rm Factor}\\\\\nn  &\\log n -1&2.4 \\log \\log n&{\\rm Difference}\\\\\n\\hline\n16&3&4.8&{\\rm worse}\\\\\n256&7&7.2&\\approx {\\rm same}\\\\\n64K&15&9.6&1.6\\\\\n2^{32}&31&12&2.6\n\\end{array}"}], "raw": {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/Searching/binarySearchCON.js"}}]}}