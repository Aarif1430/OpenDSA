{"document": {"@dupnames": "deletion", "@ids": "deletion", "@source": "<string>", "@title": "Deletion", "title": "Deletion", "comment": [{"@xml:space": "preserve", "#text": "This file is part of the OpenDSA eTextbook project. See"}, {"@xml:space": "preserve", "#text": "http://algoviz.org/OpenDSA for more details."}, {"@xml:space": "preserve", "#text": "Copyright (c) 2012-2013 by the OpenDSA Project Contributors, and"}, {"@xml:space": "preserve", "#text": "distributed under an MIT open source license."}], "raw": {"@format": "xml", "@xml:space": "preserve", "avmetadata": "null"}, "section": [{"@dupnames": "deletion", "@ids": "id1", "title": "Deletion", "paragraph": ["When deleting records from a hash table, there are two important\nconsiderations.", {"title_reference": "tombstone", "#text": "Both of these problems can be resolved by placing a special mark in\nplace of the deleted record, called a\n.\nThe tombstone indicates that a record once occupied the slot but\ndoes so no longer.\nIf a tombstone is encountered when searching along a\nprobe sequence, the search procedure continues with the search.\nWhen a tombstone is encountered during insertion, that slot\ncan be used to store the new record.\nHowever, to avoid inserting duplicate keys, it will still be necessary\nfor the search procedure to follow the probe sequence until a truly\nempty position has been found, simply to verify that a duplicate is\nnot in the table.\nHowever, the new record would actually be inserted into the slot of\nthe first tombstone encountered."}, "Here is a practice exercise.", "The use of tombstones allows searches to work correctly and allows\nreuse of deleted slots.\nHowever, after a series of intermixed insertion and deletion\noperations, some slots will contain tombstones.\nThis will tend to lengthen the average distance from a record's\nhome position to the record itself, beyond where it could be if the\ntombstones did not exist.\nA typical database application will first load a collection of records\ninto the hash table and then progress to a phase of intermixed\ninsertions and deletions.\nAfter the table is loaded with the initial collection of\nrecords, the first few deletions will lengthen the average\nprobe sequence distance for records (it\nwill add tombstones).\nOver time, the average distance will reach an equilibrium point because\ninsertions will tend to decrease the average distance by filling in\ntombstone slots.\nFor example, after initially loading records into the database, the\naverage path distance might be 1.2 (i.e., an average of 0.2 accesses\nper search beyond the home position will be required).\nAfter a series of insertions and deletions, this average distance\nmight increase to 1.6 due to tombstones.\nThis seems like a small increase, but it is three times longer on\naverage beyond the home position than before deletions.", "Two possible solutions to this problem are"], "enumerated_list": [{"@enumtype": "arabic", "@prefix": "", "@suffix": ".", "list_item": [{"paragraph": "Deleting a record must not hinder later searches.\nIn other words, the search process must still pass through\nthe newly emptied slot to reach records whose probe sequence\npassed through this slot.\nThus, the delete process cannot simply mark the slot as empty, because\nthis will isolate records further down the probe sequence."}, {"paragraph": "We do not want to make positions in the hash table unusable because\nof deletion.\nThe freed slot should be available to a future insertion."}]}, {"@enumtype": "arabic", "@prefix": "", "@suffix": ".", "list_item": [{"paragraph": "Do a local reorganization upon deletion to try to shorten the average\npath length.\nFor example, after deleting a key, continue to follow the\nprobe sequence of that key and swap\nrecords further down the probe sequence\ninto the slot of the recently deleted record (being careful not to\nremove any key from its probe sequence).\nThis will not work for all collision resolution policies."}, {"paragraph": "Periodically rehash the table by\nreinserting all records into a new hash table.\nNot only will this remove the tombstones, but it also provides an\nopportunity to place the most frequently accessed records into their\nhome positions."}]}], "raw": [{"@format": "xml", "@xml:space": "preserve", "inlineav": {"@type": "ss", "@exer_name": "hashdelCON", "@long_name": "hashdelCON", "@points": "0", "@required": "True", "@threshold": "1.0"}}, {"@format": "xml", "@xml:space": "preserve", "avembed": {"@type": "pe", "@exer_name": "HashingDelPRO", "@long_name": "HashingDelPRO", "@points": "1.0", "@required": "True", "@threshold": "0.9"}}]}, {"@ids": "hashing-deletion-summary-questions", "@names": "hashing\\ deletion\\ summary\\ questions", "title": "Hashing Deletion Summary Questions", "paragraph": ["Now here are some practice questions.", "Congratulations! You have reached the end of the hashing tutorial.\nIn summary, a properly tuned hashing system will return records with\nan average cost of less than two record accesses.\nThis makes it the most effective way known to store a database of records\nto support exact-match queries.\nUnfortunately, hashing is not effective when implementing range queries,\nor answering questions like\n\"Which record in the collection has the smallest key value?\""], "raw": [{"@format": "xml", "@xml:space": "preserve", "avembed": {"@type": "ka", "@exer_name": "HashDelSumm", "@long_name": "HashDelSumm", "@points": "1.0", "@required": "True", "@threshold": "5"}}, {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/Hashing/hashdelCON.js"}]}]}}