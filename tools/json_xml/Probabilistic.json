{"document": {"@ids": "introduction-to-probabilistic-algorithms", "@names": "introduction\\ to\\ probabilistic\\ algorithms", "@source": "<string>", "@title": "Introduction to Probabilistic Algorithms", "title": "Introduction to Probabilistic Algorithms", "subtitle": {"@ids": "probabilistic-algorithms", "@names": "probabilistic\\ algorithms", "#text": "Probabilistic Algorithms"}, "comment": [{"@xml:space": "preserve", "#text": "This file is part of the OpenDSA eTextbook project. See"}, {"@xml:space": "preserve", "#text": "http://algoviz.org/OpenDSA for more details."}, {"@xml:space": "preserve", "#text": "Copyright (c) 2012-2013 by the OpenDSA Project Contributors, and"}, {"@xml:space": "preserve", "#text": "distributed under an MIT open source license."}], "raw": {"@format": "xml", "@xml:space": "preserve", "avmetadata": "null"}, "paragraph": ["We now consider how introducing randomness into our\nalgorithms might speed things up, although perhaps at the expense of\naccuracy.\nBut often we can reduce the possibility for error to be as low as we\nlike, while still speeding up the algorithm.", {"title_reference": "lower bound for maximum finding <maximum lower bound> <LowerBound>", "math": "\\Omega(n)", "#text": "The \nin an unsorted list is .\nThis is the least time needed to be certain that we have found the\nmaximum value.\nBut what if we are willing to relax our requirement for certainty?\nThe first question is: What do we mean by this?\nThere are many aspects to \"certainty\" and we might relax the\nrequirement in various ways."}, {"math": ["X", "Y"], "#text": "There are several possible guarantees that we might require from an\nalgorithm that produces  as the maximum value, when the true\nmaximum is ."}, "There are also different ways that we might choose to sacrifice\nreliability for speed.\nThese types of algorithms also have names.", {"title_reference": "probabilistic algorithm", "math": ["m", "n", "m \\approx \\log n", "m-1", "m", "\\frac{mn}{m+1}", "n = 1,000,000", "m = \\log n = 20", "n"], "#text": "Here is an example of an algorithm for finding a large value that\ngives up its guarantee of getting the best value in exchange for an\nimproved running time.\nThis is an example of a , since it\nincludes steps that are affected by random events.\nChoose  elements at random, and pick the best one of those as\nthe answer.\nFor large , if , the answer is pretty\ngood.\nThe cost is  compares (since we must find the maximum of\n values).\nBut we don't know for sure what we will get.\nHowever, we can estimate that the rank will be about\n.\nFor example, if  and ,\nthen we expect that the largest of the 20 randomly selected values be\namong the top 5% of the  values."}, {"math": ["n", "\\frac{n+1}{2}", "n/2"], "#text": "Next, consider a slightly different problem where the goal is to\npick a number in the upper half of  values.\nWe would pick the maximum from among the first \nvalues for a cost of  comparisons.\nCan we do better than this?\nNot if we want to guarantee getting the correct answer.\nBut if we are willing to accept near certainty instead of absolute\ncertainty, we can gain a lot in terms of speed."}, {"math": ["k", "1 - \\frac{1}{2^k}", "n", "n", "k", "2^{10} = 1024", "10^{100}"], "#text": "As an alternative, consider this probabilistic algorithm.\nPick 2 numbers and choose the greater.\nThis will be in the upper half with probability 3/4 (since it is not\nin the upper half only when both numbers we choose happen to be in the\nlower half).\nIs a probability of 3/4 not good enough?\nThen we simply pick more numbers!\nFor  numbers, the greatest is in upper half with probability\n, regardless of the number  that we\npick from, so long as  is much larger than \n(otherwise the chances might become even better).\nIf we pick ten numbers, then the chance of failure is only one in\n.\nWhat if we really want to be sure, because lives depend on drawing a\nnumber from the upper half?\nIf we pick 30 numbers, we can fail only one time in a billion.\nIf we pick enough numbers, then the chance of picking a small\nnumber is less than the chance of the power failing during the\ncomputation.\nPicking 100 numbers means that we can fail only one time in\n which is less than the chance any plausible\ndisaster that you can imagine will disrupt the process."}], "bullet_list": {"@bullet": "*", "list_item": [{"paragraph": {"math": ["X", "Y"], "title_reference": "deterministic algorithm", "#text": "So far we have assumed that we require  to equal .\nThis is known as an exact or  to\nsolve the problem."}}, {"paragraph": {"math": ["X", "Y"], "title_reference": "approximation algorithm", "#text": "We could relax this and require only that  's rank is\n\"close to\"  's rank (perhaps within a fixed distance or\npercentage).\nThis is known as an ."}}, {"paragraph": {"math": ["X", "Y"], "title_reference": "probabilistic algorithm", "#text": "We could require that  is \"usually\" .\nThis is known as a ."}}, {"paragraph": {"math": ["X", "Y"], "title_reference": "heuristic algorithm", "#text": "Finally, we could require only that  's rank is \"usually\"\n\"close\" to  's rank.\nThis is known as a ."}}]}, "enumerated_list": {"@enumtype": "arabic", "@prefix": "", "@suffix": ".", "list_item": [{"paragraph": {"title_reference": "Las Vegas Algorithms", "#text": ":\nWe always find the maximum value, and \"usually\" we find it fast.\nSuch algorithms have a guaranteed result, but do not guarantee fast\nrunning time."}}, {"paragraph": {"title_reference": "Monte Carlo Algorithms", "#text": ":\nWe find the maximum value fast, or we don't get an answer at all\n(but fast).\nWhile such algorithms have good running time, their result is not\nguaranteed."}}]}}}