{"document": {"@ids": "coping-with-np-complete-problems", "@names": "coping\\ with\\ np-complete\\ problems", "@source": "<string>", "@title": "Coping with NP-Complete Problems", "title": "Coping with NP-Complete Problems", "comment": [{"@xml:space": "preserve", "#text": "This file is part of the OpenDSA eTextbook project. See"}, {"@xml:space": "preserve", "#text": "http://algoviz.org/OpenDSA for more details."}, {"@xml:space": "preserve", "#text": "Copyright (c) 2012-2013 by the OpenDSA Project Contributors, and"}, {"@xml:space": "preserve", "#text": "distributed under an MIT open source license."}, {"@xml:space": "preserve", "#text": "topic: BIN PACKING\n\n**Input:** Numbers \\(x_1, x_2, ..., x_n\\) between 0 and\n1, and an unlimited supply of bins of size 1\n(no bin can hold numbers whose sum exceeds 1).\n\n**Output</b>** An assignment of numbers to bins that\nrequires the fewest possible bins."}], "raw": {"@format": "xml", "@xml:space": "preserve", "avmetadata": "null"}, "paragraph": ["Finding that your problem is NP-complete might not mean\nthat you can just forget about it.\nTraveling salesmen need to find reasonable sales routes regardless of\nthe complexity of the problem.\nWhat do you do when faced with an NP-complete problem that you must\nsolve?", "There are several techniques to try.\nOne approach is to run only small instances of the problem.\nFor some problems, this is not acceptable.\nFor example, TRAVELING SALESMAN grows so quickly that it cannot be\nrun on modern computers for problem sizes much over 30 cities, which\nis not an unreasonable problem size for real-life situations.\nHowever, some other problems in NP, while requiring exponential\ntime, still grow slowly enough that they allow solutions for\nproblems of a useful size.", {"title_reference": ["Knapsack problem <knapsack problem> <DynamicProgramming>", "dynamic programming <dynamic programming> <DynamicProgramming>", "Theta(nK)", "n", "K", "n", "K", "K", "O(n lg K)", "K", "Theta(nK)"], "#text": "Consider the\n.\nWe have a\n\nalgorithm whose cost is  for  objects being\nfit into a knapsack of size .\nBut it turns out that Knapsack is NP-complete.\nIsn't this a contradiction?\nNot when we consider the relationship between  and\n.\nHow big is ?\nInput size is typically  because the item sizes are\nsmaller than .\nThus,  is exponential on input size."}, {"title_reference": ["nK", "pseudo-polynomial", "n = 100"], "#text": "This dynamic programming algorithm is tractable if the numbers are\n\"reasonable\".\nThat is, we can successfully find solutions to the problem when\n is in the thousands.\nSuch an algorithm is called a  time\nalgorithm.\nThis is different from TRAVELING SALESMAN which cannot possibly be\nsolved when  given current algorithms."}, {"title_reference": ["K", "n"], "#text": "A second approach to handling NP-complete problems is to solve a\nspecial instance of the problem that is not so hard.\nFor example, many problems on graphs are NP-complete, but the same\nproblem on certain restricted types of graphs is not as difficult.\nFor example, while the VERTEX COVER\nand K-CLIQUE problems are\nNP-complete in general, there are polynomial time\nsolutions for bipartite graphs (i.e., graphs whose vertices can be\nseparated into two subsets such that no pair of vertices within one\nof the subsets has an edge between them).\n2-SATISFIABILITY (where every clause in a Boolean expression has at\nmost two literals) has a polynomial time\nsolution.\nSeveral geometric problems require only polynomial time in two\ndimensions, but are NP-complete in three dimensions or more.\nKNAPSACK is considered to run in polynomial time if the numbers\n(and ) are \"small\".\nSmall here means that they are polynomial on ,\nthe number of items."}, {"title_reference": "dynamic programming <dynamic programming> <DynamicProgramming>", "#text": "In general, if we want to guarantee that we get the correct answer for\nan NP-complete problem, we potentially need to examine all of the\n(exponential number of) possible solutions.\nHowever, with some organization, we might be able to either examine\nthem quickly, or avoid examining a great many of the possible answers\nin some cases.\nFor example,\n\nattempts to organize the processing of all the subproblems to a\nproblem so that the work is done efficiently."}, {"title_reference": ["backtracking", "2^n", "n", "2^n"], "#text": "If we need to do a brute-force search of the entire solution space, we\ncan use  to visit all of the possible solutions\norganized in a solution tree.\nFor example, SATISFIABILITY has  possible ways to assign\ntruth values to the  variables contained in the Boolean\nexpression being satisfied.\nWe can view this as a tree of solutions by considering that we have a\nchoice of making the first variable TRUE or FALSE.\nThus, we can put all solutions where the first variable is TRUE on\none side of the tree, and the remaining solutions on the other.\nWe then examine the solutions by moving down one branch of the tree,\nuntil we reach a point where we know the solution cannot be correct\n(such as if the current partial collection of assignments yields an\nunsatisfiable expression).\nAt this point we backtrack and move back up a node in the tree, and\nthen follow down the alternate branch.\nIf this fails, we know to back up further in the tree as necessary and\nfollow alternate branches, until finally we either find a solution\nthat satisfies the expression or exhaust the\ntree.\nIn some cases we avoid processing many potential solutions, or find a\nsolution quickly.\nIn others, we end up visiting a large portion of the \npossible solutions."}, {"title_reference": ["Banch-and-Bounds", "optimization problems"], "#text": "is an extension of backtracking that applies\nto  such as TRAVELING SALESMAN where we\nare trying to find the shortest tour through the\ncities.\nWe traverse the solution tree as with backtracking.\nHowever, we remember the best value found so far.\nProceeding down a given branch is equivalent to deciding which order\nto visit cities.\nSo any node in the solution tree represents some collection of cities\nvisited so far.\nIf the sum of these distances exceeds the best tour found so far, then\nwe know to stop pursuing this branch of the tree.\nAt this point we can immediately back up and take another branch.\nIf we have a quick method for finding a good (but not necessarily\nbest) solution, we can use this as an initial bound value to\neffectively prune portions of the tree."}, {"title_reference": "heuristic", "#text": "Another coping strategy is to find an approximate solution to the\nproblem.\nThere are many approaches to finding approximate solutions.\nOne way is to use a  to solve the problem, that is,\nan algorithm based on a \"rule of thumb\" that does not always give the\nbest answer.\nFor example, the TRAVELING SALESMAN problem can be solved\napproximately by using the heuristic that we start at an arbitrary\ncity and then always proceed to the next unvisited city that is\nclosest.\nThis rarely gives the shortest path, but the solution might be good\nenough.\nThere are many other heuristics for TRAVELING SALESMAN that do a\nbetter job."}, {"title_reference": ["M", "matching", "G", "|M| leq 2 cdot mbox{OPT}"], "emphasis": "any", "#text": "Some approximation algorithms have guaranteed performance,\nsuch that the answer will be within a certain\npercentage of the best possible answer.\nFor example, consider this simple heuristic for the VERTEX COVER\nproblem:\nLet  be a maximal (not necessarily maximum) \nin .\nA matching pairs vertices (with connecting edges) so that no\nvertex is paired with more than one partner.\nMaximal means to pick as many pairs as possible, selecting them in\nsome order until there are no more available pairs to select.\nMaximum means the matching that gives the most pairs possible for a\ngiven graph.\nIf OPT is the size of a minimum vertex cover, then\n\nbecause at least one endpoint of every matched edge must be in\n vertex cover."}, "A better example of a guaranteed bound on a solution comes\nfrom simple heuristics to solve the BIN PACKING\nproblem.", {"title_reference": ["k", "1/7 + epsilon", "1/3 + epsilon", "1/2 + epsilon", "epsilon"], "#text": "BIN PACKING in its decision form (i.e., asking if the items can be\npacked in less than  bins) is known to be NP-complete.\nOne simple heuristic for solving this problem is to use a\n\"first fit\" approach.\nWe put the first number in the first bin.\nWe then put the second number in the first bin if it fits, otherwise\nwe put it in the second bin.\nFor each subsequent number, we simply go through the bins in the order\nwe generated them and place the number in the first bin that fits.\nThe number of bins used is no more than twice the sum of the\nnumbers, because every bin (except perhaps one) must be at least half\nfull.\nHowever, this \"first fit\" heuristic can give us a result that is\nmuch worse than optimal.\nConsider the following collection of numbers: 6 of\n, 6 of ,\nand 6 of , where\n is a small, positive number.\nProperly organized, this requires 6 bins.\nBut if done wrongly, we might end up putting the numbers into 10 bins."}, {"title_reference": ["best fit <best fit> <BestFit>", "memory management <dynamic memory allocation> <Dynamic>"], "#text": "A better heuristic is to use decreasing first fit.\nThis is the same as first fit, except that we keep the bins sorted\nfrom most full to least full.\nThen when deciding where to put the next item, we place it in the\nfullest bin that can hold it.\nThis is similar to the  heuristic\nfor .\nThe significant thing about this heuristic is not just that it tends\nto give better performance than simple first fit.\nThis decreasing first fit heuristic\ncan be proven to require no more than 11/9 the optimal number\nof bins.\nThus, we have a guarantee on how much inefficiency can result when\nusing the heuristic."}, "The theory of NP-completeness gives a technique for separating\ntractable from (probably) intractable problems.\nWhen faced with a new problem, we might alternate between\nchecking if it is tractable (that is, we try to find a polynomial-time\nsolution) and checking if it is intractable (we try to prove the\nproblem is NP-complete).\nWhile proving that some problem is NP-complete does not actually make\nour upper bound for our algorithm match the lower bound for the\nproblem with certainty, it is nearly as good.\nOnce we realize that a problem is NP-complete, then we know that our\nnext step must either be to redefine the problem to make it easier, or\nelse use one of the \"coping\" strategies discussed in this section."]}}