<?xml version="1.0" encoding="utf8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.1 -->
<document dupnames="the\ fast\ fourier\ transform" ids="the-fast-fourier-transform" source="&lt;string&gt;" title="The Fast Fourier Transform"><title>The Fast Fourier Transform</title><subtitle dupnames="the\ fast\ fourier\ transform" ids="id1">The Fast Fourier Transform</subtitle><comment xml:space="preserve">This file is part of the OpenDSA eTextbook project. See</comment><comment xml:space="preserve">http://algoviz.org/OpenDSA for more details.</comment><comment xml:space="preserve">Copyright (c) 2012-2013 by the OpenDSA Project Contributors, and</comment><comment xml:space="preserve">distributed under an MIT open source license.</comment><raw format="xml" xml:space="preserve"><avmetadata>null</avmetadata></raw><paragraph>See the <reference name="FFT Storyboard" refuri="../../../Storyboard/FFT.pptx">FFT Storyboard</reference><target ids="fft-storyboard" names="fft\ storyboard" refuri="../../../Storyboard/FFT.pptx"></target> for some more
visualizations of this material.</paragraph><paragraph>Multiplication is considerably more difficult than addition.
The cost to multiply two <math>n</math>-bit numbers directly is
<math>O(n^2)</math>, while addition of two <math>n</math>-bit numbers is
<math>O(n)</math>.</paragraph><paragraph>Recall that one property of logarithms is that
<math>\log nm = \log n + \log m</math>.
Thus, if taking logarithms and anti-logarithms were cheap, then we
could reduce multiplication to addition by taking the log of the two
operands, adding, and then taking the anti-log of the sum.</paragraph><paragraph>Under normal circumstances, taking logarithms and anti-logarithms is
expensive, and so this reduction would not be considered practical.
However, this reduction is precisely the basis for the
slide rule.
The slide rule uses a logarithmic scale to measure the lengths of two
numbers, in effect doing the conversion to logarithms automatically.
These two lengths are then added together, and the inverse logarithm
of the sum is read off another logarithmic scale.
The part normally considered expensive (taking logarithms and
anti-logarithms) is cheap because it is a physical part of the
slide rule.
Thus, the entire multiplication process can be done cheaply via a
reduction to addition.
In the days before electronic calculators, slide rules were routinely
used by scientists and engineers to do basic calculations of this
nature.</paragraph><paragraph>Now consider the problem of  multiplying polynomials.
A vector <math>\mathbf a</math> of <math>n</math> values can uniquely represent
a polynomial of degree <math>n-1</math>, expressed as</paragraph><math_block xml:space="preserve">P_{\mathbf a}(x) = \sum_{i=0}^{n-1} {\mathbf a}_i x^i.</math_block><paragraph>Alternatively, a polynomial can be uniquely represented by a
list of its values at <math>n</math> distinct points.
Finding the value for a polynomial at a given point is called
<title_reference>evaluation</title_reference>.
Finding the coefficients for the polynomial given the values at
<math>n</math> points is called <title_reference>interpolation</title_reference>.</paragraph><paragraph>To multiply two <math>n-1</math>-degree polynomials <math>A</math> and <math>B</math>
normally takes <math>\Theta(n^2)</math> coefficient multiplications.
However, if we evaluate both polynomials (at the same points), we can
simply multiply the corresponding pairs of values to get the
corresponding values for polynomial <math>AB</math>.</paragraph><topic><title>Example</title><paragraph>Polynomial A: <math>x^2 + 1</math>.</paragraph><paragraph>Polynomial B: <math>2x^2 - x + 1</math>.</paragraph><paragraph>Polynomial AB: <math>2x^4 - x^3 + 3x^2 - x + 1</math>.</paragraph><paragraph>When we multiply the evaluations of <math>A</math> and <math>B</math> at
points 0, 1, and -1, we get the following results.</paragraph><math_block xml:space="preserve">\begin{eqnarray*}
AB(-1) &amp;=&amp; (2)(4) = 8\\
AB(0) &amp;=&amp; (1)(1) = 1\\
AB(1) &amp;=&amp; (2)(2) = 4
\end{eqnarray*}</math_block><paragraph>These results are the same as when we evaluate polynomial
<math>AB</math> at these points.</paragraph></topic><paragraph>Note that evaluating any polynomial at 0 is easy.
If we evaluate at 1 and -1, we can share a lot of the work
between the two evaluations.
But we would need five points to nail down polynomial <math>AB</math>,
since it is a degree-4 polynomial.
Fortunately, we can speed processing for any pair of values <math>c</math>
and <math>-c</math>.
This seems to indicate some promising ways to speed up the process of
evaluating polynomials.
But, evaluating two points in roughly the same time as evaluating one
point only speeds the process by a constant factor.
Is there some way to generalized these observations to speed things up
further?
And even if we do find a way to evaluate many points quickly, we will
also need to interpolate the five values to get the coefficients of
<math>AB</math> back.</paragraph><paragraph>So we see that we could multiply two polynomials in less than
<math>\Theta(n^2)</math> operations <emphasis>if</emphasis> a fast way could be
found to do evaluation/interpolation of <math>2n - 1</math> points.
Before considering further how this might be done, first observe again
the relationship between evaluating a polynomial at values <math>c</math>
and <math>-c</math>.
In general, we can write <math>P_a(x) = E_a(x) + O_a(x)</math> where
<math>E_a</math> is the even powers and <math>O_a</math> is the odd powers.
So,</paragraph><math_block xml:space="preserve">P_a(x) = \sum_{i=0}^{n/2-1} a_{2i} x^{2i} +
        \sum_{i=0}^{n/2-1} a_{2i+1} x^{2i+1}</math_block><paragraph>The significance is that when evaluating the pair of values
<math>c</math> and <math>c</math>, we get</paragraph><math_block xml:space="preserve">\begin{eqnarray*}
E_a(c) + O_a(c) &amp;=&amp; E_a(c) - O_a(-c)\\
O_a(c) &amp;=&amp; - O_a(-c)
\end{eqnarray*}</math_block><paragraph>Thus, we only need to compute the <math>E</math> s and <math>O</math> s once instead
of twice to get both evaluations.</paragraph><paragraph>The key to fast polynomial multiplication is finding the right points
to use for evaluation/interpolation to make the process efficient.
In particular, we want to take advantage of symmetries, such as the
one we see for evaluating <math>x</math> and <math>-x</math>.
But we need to find even more symmetries between points if we want to
do more than cut the work in half.
We have to find symmetries not just between pairs of values,
but also further symmetries between pairs of pairs, and then pairs of
pairs of pairs, and so on.</paragraph><paragraph>Recall that a <title_reference>complex</title_reference> number <math>z</math>
has a real component and an imaginary component.
We can consider the position of <math>z</math> on a number line if we use
the <math>y</math> dimension for the imaginary component.
Now, we will define a <title_reference>primitive nth root of unity</title_reference> if</paragraph><enumerated_list enumtype="arabic" prefix="" suffix="."><list_item><paragraph><math>z^n = 1</math> and</paragraph></list_item><list_item><paragraph><math>z^k \neq 1</math> for <math>0 &lt; k &lt; n</math>.</paragraph></list_item></enumerated_list><paragraph><math>z^0, z^1, ..., z^{n-1}</math> are called the
<title_reference>nth roots of unity</title_reference>.
For example, when <math>n=4</math>, then <math>z = i</math> or <math>z = -i</math>.
In general, we have the identities <math>e^{i\pi} = -1</math>,
and <math>z^j = e^{2\pi ij/n} = -1^{2j/n}</math>.
The significance is that we can find as many points on a unit circle
as we would need
(see Figure <title_reference>Figure #Unity</title_reference>).
But these points are special in that they will allow us to do just the
right computation necessary to get the needed symmetries to speed up
the overall process of evaluating many points at once.</paragraph><target refid="unity"></target><raw format="xml" ids="unity" names="unity" xml:space="preserve"><odsafig>null</odsafig></raw><paragraph>The next step is to define how the computation is done.
Define an <math>n \times n</math> matrix <math>A_{z}</math> with row <math>i</math>
and column <math>j</math> as</paragraph><math_block xml:space="preserve">A_{z} = (z^{ij}).</math_block><paragraph>The idea is that there is a row for each root (row <math>i</math> for
<math>z^i</math>) while the columns correspond to the power of the exponent
of the :math`x` value in the polynomial.
For example, when <math>n = 4</math> we have <math>z = i</math>.
Thus, the <math>A_{z}</math> array appears as follows.</paragraph><math_block xml:space="preserve">A_{z} =
\begin{array}{rrrr}
1&amp;1&amp;1&amp;1\\
1&amp;i&amp;-1&amp;-i\\
1&amp;-1&amp;1&amp;-1\\
1&amp;-i&amp;-1&amp;i
\end{array}</math_block><paragraph>Let <math>a = [a_0, a_1, ..., a_{n-1}]^T</math> be a vector that stores the
coefficients for the polynomial being evaluated.
We can then do the calculations to evaluate the polynomial at the
<math>n</math> th roots of unity by multiplying the <math>A_{z}</math> matrix by
the coefficient vector.
The resulting vector <math>F_{z}</math> is called the
<title_reference>Discrete Fourier Transform</title_reference> (<title_reference>DFT</title_reference>) for the polynomial.</paragraph><math_block xml:space="preserve">F_{z} = A_{z}a = b.\]
\[b_i = \sum_{k=0}^{n-1} a_kz^{ik}.</math_block><paragraph>When <math>n = 8</math>, then <math>z = \sqrt{i}</math>,
since <math>\sqrt{i}^8 = 1</math>.
So, the corresponding matrix is as follows.</paragraph><math_block xml:space="preserve">A_{z} =
\begin{array}{rrrrrrrr}
1&amp;         1&amp; 1&amp;         1&amp; 1&amp;         1&amp; 1&amp;         1\\
1&amp;  \sqrt{i}&amp; i&amp; i\sqrt{i}&amp;-1&amp; -\sqrt{i}&amp;-i&amp;-i\sqrt{i}\\
1&amp;         i&amp;-1&amp;        -i&amp; 1&amp;         i&amp;-1&amp;        -i\\
1&amp; i\sqrt{i}&amp;-i&amp;  \sqrt{i}&amp;-1&amp;-i\sqrt{i}&amp; i&amp; -\sqrt{i}\\
1&amp;        -1&amp; 1&amp;        -1&amp; 1&amp;        -1&amp; 1&amp;        -1\\
1&amp; -\sqrt{i}&amp; i&amp;-i\sqrt{i}&amp;-1&amp;  \sqrt{i}&amp;-i&amp; i\sqrt{i}\\
1&amp;        -i&amp;-1&amp;         i&amp; 1&amp;        -i&amp;-1&amp;         i\\
1&amp;-i\sqrt{i}&amp;-i&amp; -\sqrt{i}&amp;-1&amp; i\sqrt{i}&amp; i&amp;  \sqrt{i}
\end{array}</math_block><paragraph>We still have two problems.
We need to be able to multiply this matrix and the vector faster
than just by performing a standard matrix-vector multiplication,
otherwise the cost is still <math>n^2</math> multiplies to do the
evaluation.
Even if we can multiply the matrix and vector cheaply, we still
need to be able to reverse the process.
That is, after transforming the two input polynomials by evaluating
them, and then pair-wise multiplying the evaluated points, we must
interpolate those points to get the resulting polynomial back that
corresponds to multiplying the original input polynomials.</paragraph><paragraph>The interpolation step is nearly identical to the evaluation step.</paragraph><math_block xml:space="preserve">F_{z}^{-1} = A_{z}^{-1}b' = a'.</math_block><paragraph>We need to find <math>A_{z}^{-1}</math>.
This turns out to be simple to compute, and is defined as follows.</paragraph><math_block xml:space="preserve">A_{z}^{-1} = \frac{1}{n}A_{1/z}.</math_block><paragraph>In other words, interpolation (the inverse transformation) requires
the same computation as evaluation, except that we substitute
<math>1/z</math> for <math>z</math> (and multiply by <math>1/n</math> at the end).
So, if we can do one fast, we can do the other fast.</paragraph><paragraph>If you examine the example <math>A_z</math> matrix for <math>n=8</math>,
you should see that there are symmetries within the matrix.
For example, the top half is identical to the bottom half with
suitable sign changes on some rows and columns.
Likewise for the left and right halves.
An efficient divide and conquer algorithm exists to perform both the
evaluation and the interpolation in <math>\Theta(n \log n)</math> time.
This is called DFT.
It is a recursive function that decomposes the matrix
multiplications, taking advantage of the symmetries made available by
doing evaluation at the <math>n</math> th roots of unity.
The algorithm is as follows:</paragraph><literal_block xml:space="preserve">Fourier_Transform(double *Polynomial, int n) {
  // Compute the Fourier transform of Polynomial
  // with degree n. Polynomial is a list of
  // coefficients indexed from 0 to n-1. n is
  // assumed to be a power of 2.
  double Even[n/2], Odd[n/2], List1[n/2], List2[n/2];

  if (n==1) return Polynomial[0];

  for (j=0; j&amp;lt;=n/2-1; j++) {
    Even[j] = Polynomial[2j];
    Odd[j] = Polynomial[2j+1];
  }
  List1 = Fourier_Transform(Even, n/2);
  List2 = Fourier_Transform(Odd, n/2);
  for (j=0; j&amp;lt;=n-1, J++) {
    Imaginary z = pow(E, 2*i*PI*j/n);
    k = j % (n/2);
    Polynomial[j] = List1[k] + z*List2[k];
  }
  return Polynomial;
}</literal_block><paragraph>Thus, the full process for multiplying polynomials
<math>A</math> and <math>B</math> using the Fourier transform is as follows.</paragraph><enumerated_list enumtype="arabic" prefix="" suffix="."><list_item><paragraph>Represent an <math>n-1</math> -degree polynomial as <math>2n-1</math>
coefficients:</paragraph><math_block xml:space="preserve">[a_0, a_1, ..., a_{n-1}, 0, ..., 0]</math_block></list_item><list_item><paragraph>Perform <literal>Fourier_Transform</literal> on the representations for <math>A</math>
and <math>B</math></paragraph></list_item><list_item><paragraph>Pairwise multiply the results to get <math>2n-1</math> values.</paragraph></list_item><list_item><paragraph>Perform the inverse <literal>Fourier_Transform</literal> to get the <math>2n-1</math>
degree polynomial <math>AB</math>.</paragraph></list_item></enumerated_list></document>