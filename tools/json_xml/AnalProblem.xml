<?xml version="1.0" encoding="utf8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.1 -->
<document dupnames="analyzing\ problems" ids="analyzing-problems" source="&lt;string&gt;" title="Analyzing Problems"><title>Analyzing Problems</title><subtitle dupnames="analyzing\ problems" ids="id1">Analyzing Problems</subtitle><comment xml:space="preserve">This file is part of the OpenDSA eTextbook project. See</comment><comment xml:space="preserve">http://algoviz.org/OpenDSA for more details.</comment><comment xml:space="preserve">Copyright (c) 2012-2016 by the OpenDSA Project Contributors, and</comment><comment xml:space="preserve">distributed under an MIT open source license.</comment><raw format="xml" xml:space="preserve"><avmetadata>null</avmetadata></raw><paragraph>You most often use the techniques of "algorithm" analysis to analyze
an <title_reference>algorithm</title_reference>, or the instantiation of an algorithm as a
<title_reference>program</title_reference>.
You can also use these same techniques to analyze the cost of a
<title_reference>problem</title_reference>.
The key question that we want to ask is: How hard is a problem?
Certainly we should expect that in some sense, the problem of sorting a
list of records is harder than the problem of searching a list of
records for a given key value.
Certainly the algorithms that we know for sorting some records seem to
be more expensive than the algorithms that we know for searching those
same records.</paragraph><paragraph>What we need are useful definitions for the <title_reference>upper bound</title_reference> and
<title_reference>lower bound</title_reference> of a problem.</paragraph><paragraph>One might start by thinking that the upper bound for a problem is how
hard any algorithm can be for the problem.
But we can make algorithms as bad as we want, so that is not useful.
Instead, what is useful is to say that a problem is only as hard as
what we CAN do.
In other words, we should define the upper bound for a problem to be
the <strong>best</strong> algorithm that we know for the problem.
Of course, whenever we talk about bounds, we have to say when they
apply.
We we really should say something like the best algorithm that we know
in the worst case, or the best algorithm that we know in the average
case.</paragraph><paragraph>But what does it mean to give a lower bound for a problem?
Lower bound refers to the minimum that any algorithm MUST cost.
For example, when searching an unsorted list, we MUST look at every
record.
When sorting a list, we MUST look at every record (to even know if it
is sorted).</paragraph><paragraph>It is much easier to show that an algorithm (or program) is in
<title_reference>Omega(f(n))</title_reference> than it is to show that a problem is in
<title_reference>Omega(f(n))</title_reference>.
For a problem to be in <title_reference>Omega(f(n))</title_reference> means that <emphasis>every</emphasis>
algorithm that solves the problem is in <title_reference>Omega(f(n))</title_reference>,
even algorithms that we have not thought of!
In other words, EVERY algorithm MUST have at least this cost.
So, to prove a lower bound, we need an argument that is true, even for
algorithms that we don't know.</paragraph><paragraph>So far all of our examples of algorithm analysis
give "obvious" results, with big-Oh always matching <title_reference>Omega</title_reference>.
To understand how big-Oh, <title_reference>Omega</title_reference>, and <title_reference>Theta</title_reference> notations
are properly used to describe our understanding of a problem or an
algorithm, it is best to consider an example where you do not already
know a lot about the problem.</paragraph><paragraph>Let us look ahead to analyzing the problem of sorting to see
how this process works.
What is the least possible cost for any sorting algorithm
in the worst case?
The algorithm must at least look at every element in the input, just
to determine that the input is truly sorted.
Thus, any sorting algorithm must take at least <title_reference>cn</title_reference> time.
For many problems, this observation that each of the <title_reference>n</title_reference> inputs
must be looked at leads to an easy <title_reference>Omega(n)</title_reference> lower bound.</paragraph><paragraph>In your previous study of computer science, you have probably
seen an example of a sorting algorithm whose running time is in
<title_reference>O(n^2)</title_reference> in the worst case.
The simple Bubble Sort and Insertion Sort algorithms
typically given as examples in a first year programming course have
worst case running times in <title_reference>O(n^2)</title_reference>.
Thus, the problem of sorting can be said to have an upper bound
in <title_reference>O(n^2)</title_reference>.
How do we close the gap between <title_reference>Omega(n)</title_reference> and <title_reference>O(n^2)</title_reference>?
Can there be a better sorting algorithm?
If you can think of no algorithm whose worst-case growth rate is
better than <title_reference>O(n^2)</title_reference>, and if you have discovered no
analysis technique to show that the least cost for the problem of
sorting in the worst case is greater than <title_reference>Omega(n)</title_reference>,
then you cannot know for sure whether or not there is a better
algorithm.</paragraph><paragraph>Many good sorting algorithms have running time that is
in <title_reference>O(n log n)</title_reference> in the worst case.
This greatly narrows the gap.
With this new knowledge, we now have a lower bound in
<title_reference>Omega(n)</title_reference> and an upper bound in <title_reference>O(n log n)</title_reference>.
Should we search for a faster algorithm?
Many have tried, without success.
Fortunately (or perhaps unfortunately?),
<title_reference>we can prove that &lt;sorting lower bound&gt; &lt;SortingLowerBound&gt;</title_reference>
any sorting algorithm must have running
time in <title_reference>Omega(n log n)</title_reference> in the worst case.
This proof is one of the most important results in
the field of algorithm analysis, and it means that no sorting
algorithm can possibly run faster than <title_reference>c n log n</title_reference> for the
worst-case input of size <title_reference>n</title_reference>.
Thus, we can conclude that the problem of sorting is
<title_reference>Theta(n log n)</title_reference> in the worst case, because the upper and
lower bounds have met.</paragraph><paragraph>Knowing the lower bound for a problem does not give you a good
algorithm.
But it does help you to know when to stop looking.
If the lower bound for the problem matches the upper bound for the
algorithm (within a constant factor), then we know that we can find an
algorithm that is better only by a constant factor.</paragraph><paragraph>So, to summarize:
The upper bound for a problem is the best that you CAN do,
while the lower bound for a problem is the least work that you MUST
do.
If those two are the same, then we say that we really understand our
problem.</paragraph><footnote auto="1" ids="id2" names="1"><label>1</label><paragraph>While it is fortunate to know the truth, it is unfortunate that
sorting is <title_reference>Theta(n log n)</title_reference> rather than <title_reference>Theta(n)</title_reference>.</paragraph></footnote><raw format="xml" xml:space="preserve"><avembed
    type="ka"
    exer_name="AnalProblemSumm"
    long_name="AnalProblemSumm"
    points="1.0"
    required="True"
    threshold="5">
</avembed>
</raw></document>