{"document": {"@dupnames": "external\\ sorting", "@ids": "external-sorting", "@source": "<string>", "@title": "External Sorting", "title": "External Sorting", "comment": [{"@xml:space": "preserve", "#text": "This file is part of the OpenDSA eTextbook project. See"}, {"@xml:space": "preserve", "#text": "http://algoviz.org/OpenDSA for more details."}, {"@xml:space": "preserve", "#text": "Copyright (c) 2012-2013 by the OpenDSA Project Contributors, and"}, {"@xml:space": "preserve", "#text": "distributed under an MIT open source license."}], "raw": [{"@format": "xml", "@xml:space": "preserve", "avmetadata": "null"}, {"@format": "xml", "@xml:space": "preserve", "odsalink": "AV/Files/extsortCON.css"}], "section": [{"@dupnames": "external\\ sorting", "@ids": "id1", "title": "External Sorting", "paragraph": [{"title_reference": ["external sorts <external sort>", "internal sorts <internal sort> <InSort>", "disk drives <disk drive>"], "#text": "We now consider the problem of sorting collections of\nrecords too large to fit in main memory.\nBecause the records must reside in peripheral or external memory,\nsuch sorting methods are called\n.\nThis is in contrast to ,\nwhich assume that the records to be sorted are stored in main memory.\nSorting large collections of records is central to many applications,\nsuch as processing payrolls and other large business databases.\nAs a consequence, many external sorting algorithms have been devised.\nYears ago, sorting algorithm designers sought to optimize\nthe use of specific hardware configurations, such as multiple\ntape or .\nMost computing today is done on personal computers and low-end\nworkstations with relatively powerful CPUs, but only one or at most\ntwo disk drives.\nThe techniques presented here are geared toward\noptimized processing on a single disk drive.\nThis approach allows us to cover the most important issues in\nexternal sorting while skipping many less important machine-dependent\ndetails."}, {"title_reference": ["main memory", "disk I/O"], "#text": "When a collection of records is too large to fit in\n,\nthe only practical way to sort it is to read some records from disk,\ndo some rearranging, then write them back to disk.\nThis process is repeated until the file is sorted, with each record\nread perhaps many times.\nGiven the high cost of , it should come as no surprise\nthat the primary goal of an external sorting algorithm is to minimize\nthe number of times information must be read from or written to disk.\nA certain amount of additional CPU processing can profitably be traded\nfor reduced disk access."}, {"title_reference": "blocks <block>", "#text": "Before discussing external sorting techniques, consider again the\nbasic model for accessing information from disk.\nThe file to be sorted is viewed by the programmer as a sequential\nseries of fixed-size .\nAssume (for simplicity) that each block contains the same\nnumber of fixed-size data records.\nDepending on the application, a record might be only a few bytes\ncomposed of little or nothing more than the key  or might be\nhundreds of bytes with a relatively small key field.\nRecords are assumed not to cross block boundaries.\nThese assumptions can be relaxed for special-purpose sorting\napplications, but ignoring such complications makes the principles\nclearer."}, {"title_reference": "sector <sector> <DiskDrive>", "#text": "Recall that a  is the basic unit\nof I/O.\nIn other words, all disk reads and writes are for one or more complete\nsectors.\nSector sizes are typically a power of two, in the range 512 to 16K\nbytes, depending on the operating system and the size and speed of\nthe disk drive.\nThe block size used for external sorting algorithms should be equal to\nor a multiple of the sector size."}, {"title_reference": ["Recall that <Secondary>", "Quicksort Quicksort <Quicksort>"], "#text": "Under this model, a sorting algorithm reads a block of data into a\nbuffer in main memory, performs some processing on it, and at some\nfuture time writes it back to disk.\n reading or writing a block from disk\ntakes on the order of one million times longer than a memory access.\nBased on this fact, we can reasonably expect that the records\ncontained in a single block can be sorted by an internal\nsorting algorithm such as \nin less time than is required to read or write the block."}, "Under good conditions, reading from a file in sequential\norder is more efficient than reading blocks in random order.\nGiven the significant impact of seek time on disk access, it\nmight seem obvious that sequential processing is faster.\nHowever, it is important to understand precisely under what\ncircumstances sequential file processing is actually faster than\nrandom access, because it affects our approach to designing an external\nsorting algorithm.", "Efficient sequential access relies on seek time being kept to a minimum.\nThe first requirement is that the blocks making up a file are in\nfact stored on disk in sequential order and close together,\npreferably filling a small number of contiguous tracks.\nAt the very least, the number of extents making up the file should be\nsmall.\nUsers typically do not have much control over the layout of their file\non disk, but writing a file all at once in sequential order to a disk\ndrive with a high percentage of free space increases the likelihood of\nsuch an arrangement.", "The second requirement is that the disk drive's I/O head remain\npositioned over the file throughout sequential processing.\nThis will not happen if there is competition of any kind for the I/O\nhead.\nFor example, on a multi-user time-shared computer the sorting process\nmight compete for the I/O head with the processes of other users.\nEven when the sorting process has sole control of the I/O head, it is\nstill likely that sequential processing will not be efficient.\nImagine the situation where all processing is done on a single disk\ndrive, with the typical arrangement of a single bank of read/write\nheads that move together over a stack of platters.\nIf the sorting process involves reading from an input file,\nalternated with writing to an output file, then the I/O head will\ncontinuously seek between the input file and the output file.\nSimilarly, if two input files are being processed simultaneously\n(such as during a merge process), then the I/O head will\ncontinuously seek between these two files.", "The moral is that, with a single disk drive, there often is\nno such thing as efficient sequential processing of a data file.\nThus, a sorting algorithm might be more efficient if it performs a\nsmaller number of non-sequential disk operations rather than a larger\nnumber of logically sequential disk operations that require a large\nnumber of seeks in practice.", {"title_reference": ["key sort", "index file"], "#text": "As mentioned previously, the record size might be quite large compared\nto the size of the key.\nFor example, payroll entries for a large business might each store\nhundreds of bytes of information including the name, ID, address, and\njob title for each employee.\nThe sort key might be the ID number, requiring only a few bytes.\nThe simplest sorting algorithm might be to process such records as a\nwhole, reading the entire record whenever it is processed.\nHowever, this will greatly increase the amount of I/O required,\nbecause only a relatively few records will fit into a single disk\nblock.\nAnother alternative is to do a .\nUnder this method, the keys are all read and stored together in an\n, where each key is stored along\nwith a pointer indicating the position of the corresponding record in\nthe original data file.\nThe key and pointer combination should be substantially smaller than\nthe size of the original record; thus, the index file will be much\nsmaller than the complete data file.\nThe index file will then be sorted, requiring much less I/O because\nthe index records are smaller than the complete records."}, {"title_reference": "Indexing", "#text": "Once the index file is sorted, it is possible to reorder the records\nin the original database file.\nThis is typically not done for two reasons.\nFirst, reading the records in sorted order from the record file\nrequires a random access for each record.\nThis can take a substantial amount of time and is only of value if\nthe complete collection of records needs to be viewed or processed in\nsorted order (as opposed to a search for selected records).\nSecond, database systems typically allow searches to be\ndone on multiple keys.\nFor example, today's processing might be done in order of ID numbers.\nTomorrow, the boss might want information sorted by salary.\nThus, there might be no single \"sorted\" order for the full record.\nInstead, multiple index files are often maintained, one for each sort\nkey.\nThese ideas are explored further in Chapter ."}], "section": [{"@ids": "simple-approaches-to-external-sorting", "@names": "simple\\ approaches\\ to\\ external\\ sorting", "title": "Simple Approaches to External Sorting", "paragraph": ["If your operating system\nsupports virtual memory, the simplest\n\"external\" sort is to read the entire file into\nvirtual memory and run an internal sorting\nmethod such as Quicksort.\nThis approach allows the virtual memory manager to use its normal\nbuffer pool mechanism to control disk accesses.\nUnfortunately, this might not always be a viable option.\nOne potential drawback is that the size of virtual memory is\nusually limited to something much smaller than the disk space\navailable.\nThus, your input file might not fit into virtual memory.\nLimited virtual memory can be overcome by adapting an internal sorting\nmethod to make use of your own buffer pool.", {"title_reference": "log n", "#text": "A more general problem with adapting an internal sorting algorithm\nto external sorting is that it is not likely to be as efficient as\ndesigning a new algorithm with the specific goal of minimizing\ndisk I/O.\nConsider the simple adaptation of Quicksort to use a buffer pool.\nQuicksort begins by processing the entire array of records, with the\nfirst partition step moving indices inward from the two ends.\nThis can be implemented efficiently using a buffer pool.\nHowever, the next step is to process each of the subarrays,\nfollowed by processing of sub-subarrays, and so on.\nAs the subarrays get smaller, processing quickly approaches\nrandom access to the disk drive.\nEven with maximum use of the buffer pool, Quicksort still must read\nand write each record  times on average.\nWe can do much better.\nFinally, even if the virtual memory manager can give good performance\nusing a standard Quicksort, this will come at the cost of using a lot\nof the system's working memory, which will mean that the system cannot\nuse this space for other work.\nBetter methods can save time while also using less memory."}, {"title_reference": "run", "#text": "Our approach to external sorting is derived from the\nMergesort algorithm.\nThe simplest form of external Mergesort performs a series\nof sequential passes over the records, merging larger and larger\nsublists on each pass.\nThe first pass merges sublists of size 1 into sublists of\nsize 2; the second pass merges the sublists of size 2 into\nsublists of size 4; and so on.\nA sorted sublist is called a .\nThus, each pass is merging pairs of runs to form longer runs.\nEach pass copies the contents of the file to\nanother file.\nHere is a sketch of the algorithm."}, {"title_reference": "double buffering <double buffering> BuffPool", "#text": "This algorithm can easily take advantage of\n.\nNote that the various passes read the input run files\nsequentially and write the output run files sequentially.\nFor sequential processing and double buffering to be effective,\nhowever, it is necessary that there be a separate I/O head available\nfor each file.\nThis typically means that each of the input and output files must be\non separate disk drives, requiring a total of four disk drives for\nmaximum efficiency."}], "target": {"@refid": "exmerge"}, "raw": {"@format": "xml", "@ids": "exmerge", "@names": "exmerge", "@xml:space": "preserve", "inlineav": {"@type": "ss", "@exer_name": "extMergeSortCON", "@long_name": "extMergeSortCON", "@points": "0", "@required": "True", "@threshold": "1.0"}}, "enumerated_list": {"@enumtype": "arabic", "@prefix": "", "@suffix": ".", "list_item": [{"paragraph": {"title_reference": "run files <run file>", "#text": "Split the original file into two equal-sized\n."}}, {"paragraph": "Read one block from each run file into input buffers."}, {"paragraph": "Take the first record from each input buffer, and write a run of\nlength two to an output buffer in sorted order."}, {"paragraph": "Take the next record from each input buffer, and write a run of\nlength two to a second output buffer in sorted order."}, {"paragraph": "Repeat until finished, alternating output between the two output\nrun buffers.\nWhenever the end of an input block is reached, read the next block\nfrom the appropriate input file.\nWhen an output buffer is full, write it to the appropriate output\nfile."}, {"paragraph": "Repeat steps 2 through 5, using the original output files as\ninput files.\nOn the second pass, the first two records of each input run file\nare already in sorted order.\nThus, these two runs may be merged and output as a single run of\nfour elements."}, {"paragraph": "Each pass through the run files provides larger and larger runs\nuntil only one run remains."}]}}, {"@ids": "improving-performance", "@names": "improving\\ performance", "title": "Improving Performance", "paragraph": [{"title_reference": ["log n", "n", "log n"], "#text": "The external Mergesort algorithm just described requires that\n passes be made to sort a file of  records.\nThus, each record must be read from disk and written to disk\n times.\nThe number of passes can be significantly reduced by observing that\nit is not necessary to use Mergesort on small runs.\nA simple modification is to read in a block of data, sort it in\nmemory (perhaps using Quicksort), and then output it as a single\nsorted run."}, "We can extend this concept to improve performance even\nfurther.\nAvailable main memory is usually much more than one block in size.\nIf we process larger initial runs, then the number of passes\nrequired by Mergesort is further reduced.\nFor example, most modern computers can provide tens or even hundreds\nof megabytes of RAM to the sorting program.\nIf all of this memory (excepting a small amount for buffers\nand local variables) is devoted to building initial runs as large as\npossible, then quite large files can be processed in few passes.\nThe next section presents a technique for producing large runs,\ntypically twice as large as could fit directly into main memory.", "Another way to reduce the number of passes required is to increase\nthe number of runs that are merged together during each pass.\nWhile the standard Mergesort algorithm merges two runs at a time,\nthere is no reason why merging needs to be limited in this way.\nBelow we will discuss the technique of multiway merging.", "Over the years, many variants on external sorting have been\npresented, but all are based on the following two steps:"], "raw": {"@format": "xml", "@xml:space": "preserve", "inlineav": {"@type": "ss", "@exer_name": "extMergeSortExampCON", "@long_name": "extMergeSortExampCON", "@points": "0", "@required": "True", "@threshold": "1.0"}}, "enumerated_list": {"@enumtype": "arabic", "@prefix": "", "@suffix": ".", "list_item": [{"paragraph": "Break the file into large initial runs."}, {"paragraph": "Merge the runs together to form a single sorted file."}]}}, {"@ids": "replacement-selection", "@names": "replacement\\ selection", "title": "Replacement Selection", "paragraph": [{"title_reference": ["M", "M", "replacement selection", "2M"], "#text": "This section treats the problem of creating initial runs as large as\npossible from a disk file, assuming a fixed amount of RAM is available\nfor processing.\nAs mentioned previously, a simple approach is to\nallocate as much RAM as possible to a large array, fill this array\nfrom disk, and sort the array using\nQuicksort.\nThus, if the size of memory available for the array is \nrecords,\nthen the input file can be broken into initial runs of length .\nA better approach is to use an algorithm called\n that, on average, creates runs of\n records in length.\nReplacement selection is actually a slight variation on the Heapsort\nalgorithm.\nThe fact that Heapsort is slower than Quicksort is\nirrelevant in this context because I/O time will dominate the total\nrunning time of any reasonable external sorting algorithm.\nBuilding longer initial runs will reduce the total I/O time required."}, {"title_reference": ["M", "Figure #RSOver"], "#text": "Replacement selection views RAM as consisting of an array of\nsize  in addition to an input buffer and an output buffer.\n(Additional I/O buffers might be desirable if the\noperating system supports double buffering,\nbecause replacement selection does sequential\nprocessing on both its input and its output.)\nImagine that the input and output files are streams of records.\nReplacement selection takes the next record in sequential order from\nthe input stream when needed, and outputs runs one record at a\ntime to the output stream.\nBuffering is used so that disk I/O is performed one block at a time.\nA block of records is initially read and held in the input buffer.\nReplacement selection removes records from the input buffer one at a\ntime until the buffer is empty.\nAt this point the next block of records is read in.\nOutput to a buffer is similar:\nOnce the buffer fills up it is written to disk as a unit.\nThis process is illustrated by Figure ."}, {"title_reference": "M", "#text": "Replacement selection works as follows.\nAssume that the main processing is done in an array of size \nrecords."}, "When the test at step 3(b) is successful, a new record is added\nto the heap, eventually to be output as part of the run.\nAs long as records coming from the input file have key values\ngreater than the last key value output to the run, they can be safely\nadded to the heap.\nRecords with smaller key values cannot be output as\npart of the current run because they would not be in sorted order.\nSuch values must be stored somewhere for future processing as part of\nanother run.\nHowever, because the heap will shrink by one element in this case,\nthere is now a free space where the last element of the heap used to\nbe!\nThus, replacement selection will slowly shrink the heap and at the\nsame time use the discarded heap space to store records for the next\nrun.\nOnce the first run is complete (i.e., the heap becomes empty), the\narray will be filled with records ready to be processed for the second\nrun.\nHere is a visualization to show a run being created\nby replacement selection.", {"title_reference": ["M", "M", "M"], "#text": "It should be clear that the minimum length of a run will be \nrecords if the size of the heap is , because at least those\nrecords originally in the heap will be part of the run.\nUnder good conditions (e.g., if the input is sorted), then an\narbitrarily long run is possible.\nIn fact, the entire file could be processed as one run.\nIf conditions are bad (e.g., if the input is reverse sorted),\nthen runs of only size  result."}, {"title_reference": ["snowplow argument", "S", "S", "S", "2S", "2S", "S"], "#text": "What is the expected length of a run generated by replacement\nselection?\nIt can be deduced from an analogy called the\n.\nImagine that a snowplow is going around a circular track during a\nheavy, but steady, snowstorm.\nAfter the plow has been around at least once, snow on\nthe track must be as follows.\nImmediately behind the plow, the track is empty because it was just\nplowed.\nThe greatest level of snow on the track is immediately in front of the\nplow, because this is the place least recently plowed.\nAt any instant, there is a certain amount of snow  on the\ntrack.\nSnow is constantly falling throughout the track at a steady rate,\nwith some snow falling \"in front\" of the plow and some \"behind\"\nthe plow.\n(On a circular track, everything is actually \"in front\" of\nthe plow, but Figure~ref{SnowPlow} illustrates the idea.)\nDuring the next revolution of the plow, all snow  on the\ntrack is removed, plus half of what falls.\nBecause everything is assumed to be in steady state, after one\nrevolution  snow is still on the track, so  snow\nmust fall during a revolution, and  snow is removed during a\nrevolution (leaving  snow behind)."}, "At the beginning of replacement selection, nearly all values coming\nfrom the input file are greater (i.e., \"in front of the plow\")\nthan the latest key value output for\nthis run, because the run's initial key values should be small.\nAs the run progresses, the latest key value output becomes greater and\nso new key values coming from the input file are more likely to be too\nsmall (i.e., \"after the plow\"); such records go to the bottom of\nthe array.\nThe total length of the run is expected to be twice the size of the\narray.\nOf course, this assumes that incoming key values are evenly distributed\nwithin the key range (in terms of the snowplow analogy, we assume that\nsnow falls evenly throughout the track).\nSorted and reverse sorted inputs do not meet this expectation and so\nchange the length of the run."], "target": [{"@refid": "rsover"}, {"@refid": "snowplow"}], "raw": [{"@format": "xml", "@ids": "rsover", "@names": "rsover", "@xml:space": "preserve", "inlineav": {"@type": "dgm", "@exer_name": "extSortOverCON", "@long_name": "extSortOverCON", "@points": "0", "@required": "True", "@threshold": "1.0"}}, {"@format": "xml", "@xml:space": "preserve", "inlineav": {"@type": "ss", "@exer_name": "extRSCON", "@long_name": "extRSCON", "@points": "0", "@required": "True", "@threshold": "1.0"}}, {"@format": "xml", "@ids": "snowplow", "@names": "snowplow", "@xml:space": "preserve", "inlineav": {"@type": "dgm", "@exer_name": "extSortSnowCON", "@long_name": "extSortSnowCON", "@points": "0", "@required": "True", "@threshold": "1.0"}}, {"@format": "xml", "@xml:space": "preserve", "avembed": {"@type": "pe", "@exer_name": "extRSPRO", "@long_name": "extRSPRO", "@points": "1.0", "@required": "True", "@threshold": "0.9"}}], "enumerated_list": {"@enumtype": "arabic", "@prefix": "", "@suffix": ".", "list_item": [{"paragraph": {"literal": "LAST = M-1", "#text": "Fill the array from disk.  Set ."}}, {"paragraph": {"emphasis": "less", "#text": "Build a min-heap.\n(Recall that a min-heap is defined such that the\nrecord at each node has a key value  than the key values of\nits children.)"}}, {"paragraph": "Repeat until the array is empty:", "enumerated_list": {"@enumtype": "loweralpha", "@prefix": "(", "@suffix": ")", "list_item": [{"paragraph": "Send the record with the minimum key value (the root) to the\noutput buffer."}, {"paragraph": {"title_reference": ["R", "R"], "#text": "Let  be the next record in the input buffer.\nIf  's key value is greater than the key value just output ..."}, "enumerated_list": {"@enumtype": "lowerroman", "@prefix": "", "@suffix": ".", "list_item": [{"paragraph": {"title_reference": "R", "#text": "Then place  at the root."}}, {"paragraph": {"literal": ["LAST", "LAST", "LAST = LAST - 1"], "title_reference": "R", "#text": "Else replace the root with the record in array position\n, and place  at position .\nSet ."}}]}}, {"paragraph": "Sift down the root to reorder the heap."}]}}]}}]}, {"@ids": "multiway-merging", "@names": "multiway\\ merging", "title": "Multiway Merging", "paragraph": [{"title_reference": ["R", "R", "log R", "R"], "#text": "The second stage of a typical external sorting algorithm merges the\nruns created by the first stage.\nAssume that we have  runs to merge.\nIf a simple two-way merge is used, then  runs\n(regardless of their sizes) will require  passes through\nthe file.\nWhile  should be much less than the total number of records\n(because the initial runs should each contain many records),\nwe would like to reduce still further the number of passes required\nto merge the runs together.\nNote that two-way merging does not make good use of available memory.\nBecause merging is a sequential process on the two runs, only one block\nof records per run need be in memory at a time.\nKeeping more than one block of a run in memory at any time will\nnot reduce the disk I/O required by the merge process\n(though if several blocks are read from a file at once time,\nat least they take advantage of sequential access).\nThus, most of the space just used by the heap for replacement\nselection (typically many blocks in length) is not being used by the\nmerge process."}, {"title_reference": ["B", "B", "B"], "#text": "We can make better use of this space and at the same time greatly\nreduce the number of passes needed to merge the runs if we merge\nseveral runs at a time.\nMultiway merging is similar to two-way merging.\nIf we have  runs to merge, with a block from each run\navailable in memory, then the -way merge algorithm simply\nlooks at  values (the front-most value for each input run)\nand selects the smallest one to output.\nThis value is removed from its run, and the process is repeated.\nWhen the current block for any run is exhausted, the next block from\nthat run is read from disk.\nThe following slideshow illustrates a multiway merge."}, {"literal": "seek", "#text": "Conceptually, multiway merge assumes that each run is stored in a\nseparate file.\nHowever, this is not necessary in practice.\nWe only need to know the position of each run within a single file,\nand use  to move to the appropriate\nblock whenever we need new data from a particular run.\nNaturally, this approach destroys the ability to do sequential\nprocessing on the input file.\nHowever, if all runs were stored on a single disk drive,\nthen processing would not be truly sequential anyway because the\nI/O head would be alternating between the runs.\nThus, multiway merging replaces several (potentially) sequential\npasses with a single random access pass.\nIf the processing would not be sequential anyway (such as when all\nprocessing is on a single disk drive), no time is lost by doing so."}, {"title_reference": ["B", "B", "R", "B", "B", "B", "B"], "#text": "Multiway merging can greatly reduce the number of passes required.\nIf there is room in memory to store one block for each run, then all\nruns can be merged in a single pass.\nThus, replacement selection can build\ninitial runs in one pass, and multiway merging can merge all runs in\none pass, yielding a total cost of two passes.\nHowever, for truly large files, there might be too many runs for each\nto get a block in memory.\nIf there is room to allocate  blocks for a -way\nmerge, and the number of runs  is greater than ,\nthen it will be necessary to do multiple merge passes.\nIn other words, the first  runs are merged, then the next\n, and so on.\nThese super-runs are then merged by subsequent passes,\n super-runs at a time."}, {"title_reference": ["B", "2B", "B", "2B^{k+1}", "k", "B", "="], "#text": "How big a file can be merged in one pass?\nAssuming  blocks were allocated to the heap for\nreplacement selection (resulting in runs of average length \nblocks), followed by a -way merge, we can process\non average a file of size (2B^2) blocks in a single multiway merge.\n blocks on average can be processed in \n-way merges.\nTo gain some appreciation for how quickly this grows, assume that we\nhave available 0.5MB of working memory, and that a block is\n4KB, yielding 128 blocks in working memory.\nThe average run size is 1MB (twice the working memory size).\nIn one pass, 128 runs can be merged.\nThus, a file of size 128MB can, on average, be processed in two\npasses (one to build the runs, one to do the merge) with only\n0.5MB of working memory.\nAs another example, assume blocks are 1KB long and working memory\nis 1MB  1024 blocks.\nThen 1024 runs of average length 2MB (which is about 2GB) can be\ncombined in a single merge pass.\nA larger block size would reduce the size of the file that can be\nprocessed in one merge pass for a fixed-size working memory; a smaller\nblock size or larger working memory would increase the file size that\ncan be processed in one merge pass.\nTwo merge passes allow much bigger files to be processed.\nWith 0.5MB of working memory and 4KB blocks,\na file of size 16~gigabytes could be processed in two merge passes,\nwhich is big enough for most applications.\nThus, this is a very effective algorithm for single disk drive\nexternal sorting."}], "raw": [{"@format": "xml", "@xml:space": "preserve", "inlineav": {"@type": "ss", "@exer_name": "extMultiMergeCON", "@long_name": "extMultiMergeCON", "@points": "0", "@required": "True", "@threshold": "1.0"}}, {"@format": "xml", "@xml:space": "preserve", "avembed": {"@type": "pe", "@exer_name": "extMultiMergePRO", "@long_name": "extMultiMergePRO", "@points": "1.0", "@required": "True", "@threshold": "0.9"}}], "section": [{"@ids": "empirical-results", "@names": "empirical\\ results", "title": "Empirical Results", "paragraph": [{"title_reference": ["#ExSortTimes", "R", "R", "R"], "#text": "Table  shows a comparison of the running time to\nsort various-sized files for the following implementations:\n(1) standard Mergesort with two input runs and two output runs,\n(2) two-way Mergesort with large initial runs (limited by the size of\navailable memory),\nand (3) -way Mergesort performed after generating large\ninitial runs.\nIn each case, the file was composed of a series of four-byte records\n(a two-byte key and a two-byte data value),\nor 256K records per megabyte of file size.\nWe can see from this table that using even a modest memory size (two\nblocks) to create initial runs results in a tremendous savings in\ntime.\nDoing 4-way merges of the runs provides another considerable speedup,\nhowever large-scale multi-way merges for  beyond about 4 or 8\nruns does not help much because a lot of time is spent determining\nwhich is the next smallest element among the  runs."}, "We see from this experiment that building large initial runs reduces\nthe running time to slightly more than one third that of standard\nMergesort, depending on file and memory sizes.\nUsing a multi-way merge further cuts the time nearly in half."], "target": {"@refid": "exsorttimes"}, "topic": {"@ids": "exsorttimes", "@names": "exsorttimes", "title": "Table", "paragraph": "A comparison of three external sorts on a collection of small\nrecords for files of various sizes.\nEach entry in the table shows time in seconds and total number of\nblocks read and written by the program.\nFile sizes are in Megabytes.\nFor the third sorting algorithm, on a file size of 4MB, the time\nand blocks shown in the last column are for a 32-way merge\n(marked with an asterisk).\n32 is used instead of 16 because 32 is a root of the number of\nblocks in the file (while 16 is not), thus allowing the same number\nof runs to be merged at every pass.", "math_block": {"@xml:space": "preserve", "#text": "\\begin{array}{|r|c|cccc|ccc|}\n\\hline\n\\textbf{File}&\n\\textbf{Sort 1}&\n\\textbf{Sort 2}&&&&\n\\textbf{Sort 3}\\\\\n\\textbf{Size}&&\n\\textbf{Memory size (in blocks)}&&&&\n\\textbf{Memory size (in blocks)}\\\\\n(Mb)&&\\textbf{2} &\n\\textbf{4} &\n\\textbf{16} &\n\\textbf{256} &\n\\textbf{2} &\n\\textbf{4} &\n\\textbf{16}\\\\\n\\hline\n  1&   0.61 &   0.27 &   0.24 &   0.19 &   0.10 &   0.21 &   0.15 &   0.13\\\\\n   &  4,864 &  2,048 &  1,792 &  1,280 &    256 &  2,048 &  1,024 &    512\\\\\n\\hline\n  4&   2.56 &   1.30 &   1.19 &   0.96 &   0.61 &   1.15 &   0.68 &   0.66*\\\\\n   & 21,504 & 10,240 &  9,216 &  7,168 &  3,072 & 10,240 &  5,120 &  2,048\\\\\n\\hline\n 16&  11.28 &   6.12 &   5.63 &   4.78 &   3.36 &   5.42 &   3.19 &   3.10\\\\\n   & 94,208 & 49,152 & 45,056 & 36,864 & 20,480 & 49,152 & 24,516 & 12,288\\\\\n\\hline\n256& 220.39 & 132.47 & 123.68 & 110.01 &  86.66 & 115.73 &  69.31 &  68.71\\\\\n   &  1,769K&  1,048K&    983K&    852K&    589K&  1,049K&    524K&   262K\\\\\n\\hline\n\\end{array}"}}}, {"@ids": "summary", "@names": "summary", "title": "Summary", "paragraph": "In summary, a good external sorting algorithm will seek to do the\nfollowing:", "bullet_list": {"@bullet": "*", "list_item": [{"paragraph": "Make the initial runs as long as possible."}, {"paragraph": "At all stages, overlap input, processing, and output as much as\npossible."}, {"paragraph": "Use as much working memory as possible.\nApplying more memory usually speeds processing.\nIn fact, more memory will have a greater effect than a faster disk.\nA faster CPU is unlikely to yield much improvement in running time\nfor external sorting, because disk I/O speed is the limiting factor."}, {"paragraph": "If possible, use additional disk drives for more overlapping of\nprocessing with I/O, and to allow for sequential file\nprocessing."}]}, "raw": [{"@format": "xml", "@xml:space": "preserve", "odsascript": "DataStructures/binaryheap.js"}, {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/Files/extMergeSortCON.js"}, {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/Files/extMergeSortExampCON.js"}, {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/Files/extSortOverCON.js"}, {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/Files/extRSCON.js"}, {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/Files/extSortSnowCON.js"}, {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/Files/extMultiMergeCON.js"}]}]}]}}