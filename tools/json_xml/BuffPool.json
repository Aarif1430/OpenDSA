{"document": {"@dupnames": "buffer\\ pools", "@ids": "buffer-pools", "@source": "<string>", "@title": "Buffer Pools", "title": "Buffer Pools", "subtitle": {"@dupnames": "buffer\\ pools", "@ids": "id1", "#text": "Buffer Pools"}, "comment": [{"@xml:space": "preserve", "#text": "This file is part of the OpenDSA eTextbook project. See"}, {"@xml:space": "preserve", "#text": "http://algoviz.org/OpenDSA for more details."}, {"@xml:space": "preserve", "#text": "Copyright (c) 2012-2016 by the OpenDSA Project Contributors, and"}, {"@xml:space": "preserve", "#text": "distributed under an MIT open source license."}], "raw": [{"@format": "xml", "@xml:space": "preserve", "avmetadata": "null"}, {"@format": "xml", "@xml:space": "preserve", "odsalink": "AV/Files/buffpoolCON.css"}, {"@format": "xml", "@xml:space": "preserve", "inlineav": {"@type": "ss", "@exer_name": "buffintroCON", "@long_name": "buffintroCON", "@points": "0", "@required": "True", "@threshold": "1.0"}}], "paragraph": [{"title_reference": ["disk drive <DiskExamp>", "average seek time", "track-to-track seek time", "9.5 + 11.1 times 1.5 = 26.2", "track", "9.5 + 11.1/2 + (1/256)times11.1 = 15.1", "sector"], "#text": "Given a \nrotating at 5400 rpm,  of 9.5ms,\nand  of 2.2ms,\nwe can calculate that it takes about\n ms\nto read one  of data on average.\nIt takes about\n ms on average\nto read a single  of data.\nThis is a good savings (slightly over half the time), but\nless than 1% of the data on the track are read.\nIf we want to read only a single byte, it would save us effectively no\ntime over that required to read an entire sector.\nFor this reason, nearly all disk drives automatically read or write\nan entire sector's worth of information whenever the disk is\naccessed, even when only one byte of information is requested."}, {"title_reference": ["buffering", "caching", "locality of reference"], "#text": "Once a sector is read, its information is stored in main memory.\nThis is known as  or  the information.\nIf the next disk request is to that same sector, then\nit is not necessary to read from disk again because the information is\nalready stored in main memory.\nBuffering is an example of a standard method for minimizing disk\naccesses:\nBring off additional information from disk to satisfy future\nrequests.\nIf information from files were accessed at random, then the\nchance that two consecutive disk requests are to the same sector\nwould be low.\nHowever, in practice most disk requests are close to the location\n(in the logical file at least) of the previous request,\na concept referred to as .\nThis means that the probability of the next request\n\"hitting the cache\" is much higher than chance would indicate."}, {"title_reference": "disk drives <disk drive>", "#text": "This principle explains one reason why average access times for new\n are lower than in the past.\nNot only is the hardware faster, but information is also now stored\nusing better algorithms and larger caches that minimize the number\nof times information needs to be fetched from disk.\nThis same concept is also used to store parts of programs in faster\nmemory within the CPU, using the CPU cache that is\nprevalent in modern microprocessors."}, "Sector-level buffering is normally provided by the operating system\nand is often built directly into the disk drive controller hardware.\nMost operating systems maintain at least two buffers,\none for input and one for output.\nConsider what would happen if there were only one buffer during a\nbyte-by-byte copy operation.\nThe sector containing the first byte would be read into the I/O\nbuffer.\nThe output operation would need to destroy the contents of the single\nI/O buffer to write this byte.\nThen the buffer would need to be filled again from disk for the\nsecond byte, only to be destroyed during output.\nThe simple solution to this problem is to keep one buffer for input,\nand a second for output.", {"title_reference": "double buffering", "#text": "Most disk drive controllers operate independently\nfrom the CPU once an I/O request is received.\nThis is useful because the CPU can typically execute millions of\ninstructions during the time required for a single I/O operation.\nA technique that takes maximum advantage of this micro-parallelism is\n.\nImagine that a file is being processed sequentially.\nWhile the first sector is being read, the CPU cannot process that\ninformation and so must wait or find something else to do in the\nmeantime.\nOnce the first sector is read, the CPU can start processing\nwhile the disk drive (in parallel) begins reading\nthe second sector.\nIf the time required for the CPU to process a sector is approximately\nthe same as the time required by the disk controller to read a sector,\nit might be possible to keep the CPU continuously fed with data from\nthe file.\nThe same concept can also be applied to output, writing one sector to\ndisk while the CPU is writing to a second output buffer in memory.\nThus, in an operationg system that support double buffering, it pays\nto have at least two input buffers and two output buffers available."}, {"title_reference": ["backing storage", "buffering", "page", "buffer pool"], "#text": "Caching information in memory is such a good idea that\nit is usually extended to multiple buffers.\nThe operating system\nor an application program might store many buffers of information\ntaken from some  such as a disk file.\nThis process of using buffers as an intermediary between a user and a\ndisk file is called  the file.\nThe information stored in a buffer is often called a , and\nthe collection of buffers is called a .\nThe goal of the buffer pool is to increase the amount of information\nstored in memory in hopes of increasing the likelihood that new\ninformation requests can be satisfied from the buffer pool rather\nthan requiring new information to be read from disk."}], "section": [{"@ids": "replacement-strategies", "@names": "replacement\\ strategies", "title": "Replacement Strategies", "paragraph": ["As long as there is an unused buffer available in the buffer pool,\nnew information can be read in from disk on demand.\nWhen an application continues to read new information from\ndisk, eventually all of the buffers in the buffer pool will become\nfull.\nOnce this happens, some decision must be made about what information\nin the buffer pool will be sacrificed to make room for newly\nrequested information.", {"title_reference": "heuristic", "#text": "When replacing information contained in the buffer pool,\nthe goal is to select a buffer that has \"unnecessary\"\ninformation, that is, the information least likely to be requested\nagain.\nBecause the buffer pool cannot know for certain what the pattern of\nfuture requests will look like, a decision based on some\n, or best guess, must be used.\nThere are several approaches to making this decision."}, {"title_reference": "first-in, first-out <FIFO>", "#text": "One heuristic is .\nThis scheme simply orders the buffers in a queue.\nThe buffer at the front of the queue is used next to store new\ninformation and then placed at the end of the queue.\nIn this way, the buffer to be replaced is the one that has held its\ninformation the longest, in hopes that this information is no longer\nneeded.\nThis is a reasonable assumption when processing moves along the file\nat some steady pace in roughly sequential order.\nHowever, many programs work with certain key pieces of\ninformation over and over again, and the importance of information has\nlittle to do with how long ago the information was first accessed.\nTypically it is more important to know how many times the information\nhas been accessed, or how recently the information was last accessed."}, {"title_reference": ["least frequently used", "LFU"], "#text": "Another approach is called  ().\nLFU tracks the number of accesses to each buffer in the\nbuffer pool.\nWhen a buffer must be reused, the buffer that\nhas been accessed the fewest number of times is considered to contain\nthe \"least important\" information, and so it is used next.\nLFU, while it seems intuitively reasonable, has many drawbacks.\nFirst, it is necessary to store and update access counts for each buffer.\nSecond, what was referenced many times in the past might now be\nirrelevant.\nThus, some time mechanism where counts \"expire\" is often desirable.\nThis also avoids the problem of buffers that slowly build up big\ncounts because they get used just often enough to avoid being\nreplaced.\nAn alternative is to maintain counts for all sectors ever read, not\njust the sectors currently in the buffer pool.\nThis avoids immediately replacing the buffer just read, which has not\nyet had time to build a high access count."}, {"title_reference": ["least recently used", "LRU"], "#text": "The third approach is called \n().\nLRU simply keeps the buffers in a list.\nWhenever information in a buffer is accessed, this buffer is brought\nto the front of the list.\nWhen new information must be read, the buffer at the back of the\nlist (the one least recently used) is taken and its \"old\"\ninformation is either discarded or written to disk, as appropriate.\nThis is an easily implemented approximation to LFU and is often the\nmethod of choice for managing buffer pools unless\nspecial knowledge about information access patterns for an application\nsuggests a special-purpose buffer management scheme."}], "raw": {"@format": "xml", "@xml:space": "preserve", "inlineav": {"@type": "ss", "@exer_name": "LRUCON", "@long_name": "LRUCON", "@points": "0", "@required": "True", "@threshold": "1.0"}}}, {"@ids": "the-dirty-bit", "@names": "the\\ dirty\\ bit", "title": "The Dirty Bit", "paragraph": ["The main purpose of a buffer pool is to minimize disk I/O.\nWhen the contents of a block are modified, we could write the updated\ninformation to disk immediately.\nBut what if the block is changed again?\nIf we write the block's contents after every change, that might be a\nlot of disk write operations that can be avoided.\nIt is more efficient to wait until either the file is to be closed,\nor the contents of the buffer containing that block is to be flushed\nfrom the buffer pool.", {"title_reference": "dirty bit", "#text": "When a buffer's contents are to be replaced in the buffer pool,\nwe only want to write the contents to disk if it is necessary.\nThat would be necessary only if the contents have changed since the\nblock was read in originally from the file.\nThe way to insure that the block is written when necessary, but only\nwhen necessary, is to maintain a Boolean variable with the buffer\n(often referred to as the ) that is turned on when\nthe buffer's contents are modified by the client.\nAt the time when the block is flushed from the buffer pool, it is\nwritten to disk if and only if the dirty bit has been turned on."}, {"title_reference": "virtual memory", "#text": "Modern operating systems support .\nVirtual memory is a technique that allows the programmer to write\nprograms as though there is more of the faster main memory (such as\nRAM) than actually exists.\nVirtual memory makes use of a buffer pool to store data read from\nblocks on slower, secondary memory (such as on the disk drive).\nThe disk stores the complete contents of the virtual memory.\nBlocks are read into main memory as demanded by memory accesses.\nNaturally, programs using virtual memory techniques are slower than\nprograms whose data are stored completely in main memory.\nThe advantage is reduced programmer effort because a good virtual memory\nsystem provides the appearance of larger main memory without\nmodifying the program."}, "Here is a visualization to let you experiment with the various buffer\npool replacement strategies."], "raw": [{"@format": "xml", "@xml:space": "preserve", "inlineav": {"@type": "ss", "@exer_name": "LRUwriteCON", "@long_name": "LRUwriteCON", "@points": "0", "@required": "True", "@threshold": "1.0"}}, {"@format": "xml", "@xml:space": "preserve", "avembed": {"@type": "ss", "@exer_name": "BufferPoolAV", "@long_name": "BufferPoolAV", "@points": "0.0", "@required": "True", "@threshold": "1.0"}}], "comment": [{"@xml:space": "preserve", "#text": "The following exercise needs to be completed, so that this can be"}, {"@xml:space": "preserve", "#text": "added back in."}, {"@xml:space": "preserve", "#text": "Here is an exercise to help you practice."}, {"@xml:space": "preserve", "#text": ".. avembed:: AV/Files/bufferpoolPRO.html pe"}]}, {"@ids": "implementing-buffer-pools", "@names": "implementing\\ buffer\\ pools", "title": "Implementing Buffer Pools", "paragraph": ["When implementing buffer pools, there are two basic approaches that can\nbe taken regarding the transfer of information between the user of the\nbuffer pool and the buffer pool class itself.\nThe first approach is to pass \"messages\" between the two.\nThis approach is illustrated by the following abstract class:", {"literal": ["insert", "getbytes", "space", "sz", "insert", "getbytes", "pos", "read", "write"], "title_reference": "RandomAccessFile <FileProg>", "#text": "This simple class provides an interface with two member functions,\n and .\nThe information is passed between the buffer pool user and the\nbuffer pool through the  parameter.\nThis is storage space, provided by the bufferpool client and at least\n bytes long, which the\nbuffer pool can take information from (the  function) or\nput information into (the  function).\nParameter  indicates where the information will be placed\nin the buffer pool's logical storage space.\nPhysically, it will actually be copied to the appropriate byte\nposition in some buffer in the buffer pool.\nThis ADT is similar to the  and  methods of the\n class of Java."}, "An alternative interface is to have the buffer pool provide to the\nuser a direct pointer to a buffer that contains the requested\ninformation.\nSuch an interface might look as follows:", "In this approach, the buffer pool user is made aware that the\nstorage space is divided into blocks of a given size, where each block\nis the size of a buffer.\nThe user requests specific blocks from the buffer pool, with a pointer\nto the buffer holding the requested block being returned to the user.\nThe user might then read from or write to this space.\nIf the user writes to the space, the buffer pool must be informed of\nthis fact.\nThe reason is that, when a given block is to be removed from the\nbuffer pool, the contents of that block must be written to the backing\nstorage if it has been modified.\nIf the block has not been modified, then it is unnecessary to write it\nout.", {"literal": ["getblock", "dirtyblock"], "#text": "A variation on this approach is to have the  function\ntake another parameter to indicate the \"mode\" of use for the\ninformation.\nIf the mode is READ then the buffer pool assumes that no changes will\nbe made to the buffer's contents (and so no write operation need be\ndone when the buffer is reused to store another block).\nIf the mode is WRITE then the buffer pool assumes that the client will\nnot look at the contents of the buffer and so no read from the file is\nnecessary.\nIf the mode is READ AND WRITE then the buffer pool would read the\nexisting contents of the block in from disk, and write the contents of\nthe buffer to disk when the buffer is to be reused.\nUsing the \"mode\" approach, the  method is avoided."}, {"title_reference": "stale pointers <stale pointer>", "strong": ["T1", "T2", "T1"], "#text": "One problem with the buffer-passing ADT is the risk of\n.\nWhen the buffer pool user is given a pointer to some buffer\nspace at time , that pointer does indeed refer to the desired\ndata at that time.\nAs further requests are made to the buffer pool, it is possible that\nthe data in any given buffer will be removed and replaced with new\ndata.\nIf the buffer pool user at a later time  then refers to the\ndata referred to by the pointer given at time , it is possible\nthat the data are no longer valid because the buffer contents have\nbeen replaced in the meantime.\nThus the pointer into the buffer pool's memory has become \"stale\".\nTo guarantee that a pointer is not stale, it should not be used if\nintervening requests to the buffer pool have taken place."}, {"literal": ["acquireBuffer", "releaseBuffer", "acquireBuffer", "releaseBuffer"], "#text": "We can solve this problem by introducing the concept of a user (or\npossibly multiple users) gaining access to a buffer, and then\nreleasing the buffer when done.\nWe will add method  and  for\nthis purpose.\nMethod  takes a block ID as input and returns a\npointer to the buffer that will be used to store this block.\nThe buffer pool will keep a count of the number of requests currently\nactive for this block.\nMethod  will reduce the count of active users for\nthe associated block.\nBuffers associated with active blocks will not be eligible for\nflushing from the buffer pool.\nThis will lead to a problem if the client neglects to release active\nblocks when they are no longer needed.\nThere would also be a problem if there were more total active blocks\nthan buffers in the buffer pool.\nSo, the buffer pool should be initialized to include more\nbuffers than will ever need to be active at one time."}, "An additional problem with both ADTs presented so far comes when the\nuser intends to completely overwrite the contents of a block, and does\nnot need to read in the old contents already on disk.\nHowever, the buffer pool cannot in general know whether the user\nwishes to use the old contents or not.\nThis is especially true with the message-passing approach where a\ngiven message might overwrite only part of the block.\nIn this case, the block will be read into memory even when not needed,\nand then its contents will be overwritten.", {"literal": ["acquireBuffer", "readBlock"], "#text": "This inefficiency can be avoided (at least in the buffer-passing\nversion) by separating the assignment of\nblocks to buffers from actually reading in data for the block.\nIn particular, the following revised buffer-passing ADT does not\nactually read data in the  method.\nUsers who wish to see the old contents must then issue a\n request to read the data from disk into the buffer."}, {"literal": ["acquireBuffer", "readBlock", "markDirty"], "#text": "Again, a mode parameter could be added to the \nmethod, eliminating the need for the  and\n methods."}, {"title_reference": "B-trees <B-tree> <BTree>", "#text": "Clearly, the buffer-passing approach places more obligations on the\nuser of the buffer pool.\nThese obligations include knowing the size of a block, not corrupting\nthe buffer pool's storage space, and informing the buffer pool both\nwhen a block has been modified and when it is no longer needed.\nSo many obligations make this approach prone to error.\nAn advantage is that there is no need to do an extra copy step when\ngetting information from the user to the buffer.\nIf the size of the records stored is small, this is not an important\nconsideration.\nIf the size of the records is large (especially if the record size and\nthe buffer size are the same, as typically is the case when\nimplementing , then this efficiency\nissue might become important.\nNote however that the in-memory copy time will always be far less than\nthe time required to write the contents of a buffer to disk.\nFor applications where disk I/O is the bottleneck for the program,\neven the time to copy lots of information between the buffer pool user\nand the buffer might be inconsequential.\nAnother advantage to buffer passing is the reduction in unnecessary\nread operations for data that will be overwritten anyway."}, {"literal": ["space", "byte[]", "byte[]"], "#text": "Note that using Java generics would not be appropriate for use in the\nbuffer pool implementation.\nIn our ADTs, the  parameter and the buffer pointer are declared\nto be \nWhen a class uses a Java generic, that means that the record type is\narbitrary, but that the class knows what the record type is.\nIn contrast, using  for the space means that not\nonly is the record type arbitrary, but also the buffer pool does not\neven know what the user's record type is.\nIn fact, a given buffer pool might have many users who store many types\nof records."}, {"title_reference": "memory manager <MemmanIntro>", "#text": "In a buffer pool, the user decides where a given record will be stored\nbut has no control over the precise mechanism by which data are\ntransferred to the backing storage.\nThis is in contrast to the , in\nwhich the user passes a record to the manager and has no control at\nall over where the record is stored."}], "raw": [{"@format": "xml", "@xml:space": "preserve", "codeinclude": "null"}, {"@format": "xml", "@xml:space": "preserve", "codeinclude": "null"}, {"@format": "xml", "@xml:space": "preserve", "codeinclude": "null"}, {"@format": "xml", "@xml:space": "preserve", "codeinclude": "null"}, {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/Files/buffintroCON.js"}, {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/Files/LRUCON.js"}, {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/Files/LRUwriteCON.js"}], "target": {"@refid": "examplebuffer"}, "topic": [{"@ids": "examplebuffer", "@names": "examplebuffer", "title": "Example", "paragraph": "Assume each sector of the disk file (and thus each block in the\nbuffer pool) stores 1024 bytes.\nIf the next request is to copy 40 bytes beginning at position 6000 of\nthe file, these bytes should be placed into Sector 5 (whose bytes go\nfrom position 5120 to position 6143)."}, {"title": "Example", "paragraph": {"literal": ["getblock", "dirtyblock"], "#text": "We wish to write 40 bytes beginning at logical position 6000 in\nthe file.\nUsing the second ADT, the client would need to know that blocks\n(buffers) are of size 1024, and therefore would request access to\nSector 5.\nA pointer to the buffer containing Sector 5 would be returned by\nthe call to .\nThe client would then copy 40 bytes to positions 880-919 of the\nbuffer, and call  to warn the buffer pool that the\ncontents of this block have been modified."}}]}]}}