<?xml version="1.0" encoding="utf8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.1 -->
<document dupnames="growth\ rates\ review" ids="growth-rates-review" source="&lt;string&gt;" title="Growth Rates Review"><title>Growth Rates Review</title><comment xml:space="preserve">This file is part of the OpenDSA eTextbook project. See</comment><comment xml:space="preserve">http://algoviz.org/OpenDSA for more details.</comment><comment xml:space="preserve">Copyright (c) 2012-2016 by the OpenDSA Project Contributors, and</comment><comment xml:space="preserve">distributed under an MIT open source license.</comment><raw format="xml" xml:space="preserve"><avmetadata>null</avmetadata></raw><section dupnames="growth\ rates\ review" ids="id1"><title>Growth Rates Review</title><paragraph>Two functions of <math>n</math> have different
<title_reference>growth rates &lt;growth rate&gt;</title_reference> if as <math>n</math> goes to infinity
their ratio either goes to infinity or goes to zero.</paragraph><target refid="runtimegraph"></target><raw format="xml" ids="runtimegraph" names="runtimegraph" xml:space="preserve"><odsafig>null</odsafig></raw><paragraph>Where does <math>(1.618)^n</math> go on here?</paragraph><paragraph>Exact equations relating program operations to running time require
machine-dependent constants.
Sometimes, the equation for exact running time is complicated to
compute.
Usually, we are satisfied with knowing an approximate growth rate.</paragraph><paragraph>Example: Given two algorithms with growth rate <math>c_1n</math> and
<math>c_2 2^{n!}</math>, do we need to know the values of <math>c_1</math>
and <math>c_2</math>?</paragraph><paragraph>Consider <math>n^2</math> and <math>3n</math>.
PROVE that <math>n^2</math> must eventually become (and remain) bigger.</paragraph><paragraph>Proof by Contradiction:
Assume there are some values for constants <math>r</math> and <math>s</math>
such that, for all values of <math>n</math>,
<math>n^2 &lt; rn + s</math>.
Then, <math>n &lt; r + s/n</math>.
But, as <math>n</math> grows, what happens to <math>s/n</math>?
It goes to zero.</paragraph><paragraph>Since <math>n</math> grows toward infinity, the assumption must be false.
Conclusion: In the limit, as <math>n \rightarrow \infty</math>, constants
don't matter.
Limits are the typical way to prove that one function grows faster
than another.</paragraph><paragraph>Here are some useful observatios.</paragraph><paragraph>Since <math>n^2</math> grows faster than <math>n</math>,</paragraph><bullet_list bullet="*"><list_item><paragraph><math>2^{n^2}</math> grows faster than <math>2^n</math>.
(Take antilog of both sides.)</paragraph></list_item><list_item><paragraph><math>n^4</math> grows faster than <math>n^2</math>.
(Square boths sides.)</paragraph></list_item><list_item><paragraph><math>n</math> grows faster than <math>\sqrt{n}</math>.
(<math>n = (\sqrt{n})^2</math>.
Replace <math>n</math> with <math>\sqrt{n}</math>.)</paragraph></list_item><list_item><paragraph><math>2 \log n</math> grows <emphasis>no slower</emphasis> than <math>\log n</math>.
(Take <math>\log</math> of both sides. Log "flattens" growth rates.)</paragraph></list_item></bullet_list><paragraph>Since <math>n!</math> grows faster than <math>2^n</math>,</paragraph><bullet_list bullet="*"><list_item><paragraph><math>n!!</math> grows faster than <math>2^n!</math>.
(Apply factorial to both sides.)</paragraph></list_item><list_item><paragraph><math>2^{n!}</math> grows faster than <math>2^{2^n}</math>.
(Take antilog of both sides.)</paragraph></list_item><list_item><paragraph><math>n!^2</math> grows faster than <math>2^{2n}</math>.
(Square both sides.)</paragraph></list_item><list_item><paragraph><math>\sqrt{n!}</math> grows faster than <math>\sqrt{2^n}</math>.
(Take square root of both sides.)</paragraph></list_item><list_item><paragraph><math>\log n!</math> grows <emphasis>no slower</emphasis> than <math>n</math>.
(Take log of both sides.
Actually, it grows faster since <math>\log n! = \Theta(n \log n)</math>.)</paragraph></list_item></bullet_list><paragraph>If <math>f</math> grows faster than <math>g</math>, then
must <math>\sqrt{f}</math> grow faster than <math>\sqrt{g}</math>?
Yes.</paragraph><paragraph>Must <math>\log f</math> grow faster than <math>\log g</math>?
No.
<math>\log n \approx \log n^2</math> within a constant factor, that is, the
growth <strong>rate</strong> is the same!</paragraph><paragraph><math>\log n</math> is related to <math>n</math> in exactly the same way that
<math>n</math> is related to <math>2^n</math>.</paragraph><paragraph><math>2^{\log n} = n</math>.</paragraph></section><section ids="asymptotic-notation" names="asymptotic\ notation"><title>Asymptotic Notation</title><math_block xml:space="preserve">\begin{array}{llcl}
\mathrm{little\ oh}&amp;f(n) \in o(g(n))&amp;&lt;&amp;\lim f(n)/g(n) = 0\\
\mathrm{big\ oh}&amp;f(n) \in O(g(n))&amp;\leq\\
\mathrm{Theta}&amp;f(n) = \Theta(g(n))&amp;=&amp;f=O(g) and\\
&amp;&amp;&amp; g=O(f)\\
\mathrm{Big\ Omega}&amp;f(n) \in \Omega(g(n))&amp;\geq\\
\mathrm{Little Omega}&amp;f(n) \in \omega(g(n))&amp;&gt;&amp;\lim g(n)/f(n) = 0
\end{array}</math_block><paragraph>I prefer "<math>f \in O(n^2)</math>" to "<math>f = O(n^2)</math>"
While <math>n \in O(n^2)</math> and <math>n^2 \in O(n^2)</math>,
<math>O(n) \neq O(n^2)</math>.</paragraph><paragraph>Note: Big oh does not say how good an algorithm is
only how bad it <strong>can</strong> be.</paragraph><paragraph>If <math>\mathcal{A}\in O(n)</math> and <math>\mathcal{B} \in O(n^2)</math>,
is <math>\mathcal{A}</math> better than <math>\mathcal{B}</math>?
Perhaps... but perhaps better analysis will show that
<math>\mathcal{A} = \Theta(n)</math> while
<math>\mathcal{B} = \Theta(\log n)</math>.</paragraph><paragraph>Order Notation has practical limits.
Notation: <math>\log n^2 (= 2 \log n)</math> vs.
<math>\log^2 n (= (\log n)^2)</math>
vs. <math>\log \log n</math>.</paragraph><paragraph><math>\log 16^2 = 2 \log 16 = 8</math>.</paragraph><paragraph><math>\log^2 16 = 4^2 = 16</math>.</paragraph><paragraph><math>\log \log 16 = \log 4 = 2</math>.</paragraph><paragraph>Statement: Resource requirements for Algorithm <math>\mathcal{A}</math>
grow slower than resource requirements for Algorithm <math>\mathcal{B}</math>.</paragraph><paragraph>Is <math>\mathcal{A}</math> better than <math>\mathcal{B}</math>?</paragraph><paragraph>Potential problems:</paragraph><bullet_list bullet="*"><list_item><paragraph>How big must the input be?</paragraph></list_item><list_item><paragraph>Some growth rate differences are trivial
Example: <math>\Theta(\log^2 n)</math> vs. <math>\Theta(n^{1/10})</math>.
If <math>n</math> is <math>10^{12} (\approx 2^{40})</math> then
<math>\log^2 n \approx 1600</math>, <math>n^{1/10} = 16</math> even though
<math>n^{1/10}</math> grows faster than <math>\log^2 n</math>.
<math>n</math> must be enormous (like <math>2^{150}</math>) for
<math>n^{1/10}</math> to be bigger than <math>\log^2 n</math>.</paragraph></list_item></bullet_list><paragraph>It is not always practical to reduce an algorithm's growth rate
"Practical" here means that the constants might become too
much higher when we shave off the minor asymptotic growth.</paragraph><paragraph>Shaving a factor of <math>n</math> reduces cost by a factor of a million
for input size of a million.
Shaving a factor of <math>\log \log n</math> saves only a factor of 4-5.</paragraph><paragraph>There is the concept of a "Practicality Window".
In general, (1) we have limited time to solve a problem,
and (2) input can only get so big before the computer chokes.
Fortunately, algorithm growth rates are USUALLY well behaved, so that
Order Notation gives practical indications.
"Practical" is the keyword.
We use asymptotics because they provide a simple <strong>model</strong> that
<strong>usually</strong> mirrors reality.
This is <strong>useful</strong> to simplify our thinking.</paragraph></section></document>