{"document": {"@ids": "space-bounds", "@names": "space\\ bounds", "@source": "<string>", "@title": "Space Bounds", "title": "Space Bounds", "comment": [{"@xml:space": "preserve", "#text": "This file is part of the OpenDSA eTextbook project. See"}, {"@xml:space": "preserve", "#text": "http://algoviz.org/OpenDSA for more details."}, {"@xml:space": "preserve", "#text": "Copyright (c) 2012-2016 by the OpenDSA Project Contributors, and"}, {"@xml:space": "preserve", "#text": "distributed under an MIT open source license."}], "raw": {"@format": "xml", "@xml:space": "preserve", "avmetadata": "null"}, "paragraph": ["Besides time, space is the other computing resource that is commonly\nof concern to programmers.\nJust as computers have become much faster over the years, they have\nalso received greater allotments of memory.\nEven so, the amount of available disk space or main memory can\nbe significant constraints for algorithm designers.", "The analysis techniques used to measure space requirements are\nsimilar to those used to measure time requirements.\nHowever, while time requirements are normally measured for an\nalgorithm that manipulates a particular data structure,\nspace requirements are normally determined for the data structure\nitself.\nThe concepts of asymptotic analysis for growth rates\non input size apply completely to measuring space requirements.", {"title_reference": "overhead", "#text": "A data structure's primary purpose is to store data in a way that\nallows efficient access to those data.\nTo provide efficient access, it may be necessary to store\nadditional information about where the data are within the data\nstructure.\nFor example, each node of a linked list must store a pointer to the\nnext value on the list.\nAll such information stored in addition to the actual data values is\nreferred to as .\nIdeally, overhead should be kept to a minimum while allowing maximum\naccess.\nThe need to maintain a balance between these opposing goals is what\nmakes the study of data structures so interesting."}, {"title_reference": "space/time tradeoff", "#text": "One important aspect of algorithm design is referred to as\nthe  principle.\nThe space/time tradeoff principle says that one can often achieve a\nreduction in time if one is willing to sacrifice space or\nvice versa.\nMany programs can be modified to reduce storage requirements by\n\"packing\" or encoding information.\n\"Unpacking\" or decoding the information requires additional\ntime.\nThus, the resulting program uses less space but runs slower.\nConversely, many programs can be modified to pre-store results or\nreorganize information to allow faster running time at the expense of\ngreater storage requirements.\nTypically, such changes in time and space are both by a constant\nfactor."}, {"title_reference": ["lookup table", "n!", "n > 12"], "literal": ["int", "int"], "#text": "A classic example of a space/time tradeoff is the\n.\nA lookup table pre-stores the value of a function that would\notherwise be computed each time it is needed.\nFor example, 12! is the greatest value for the factorial function that\ncan be stored in a 32-bit  variable.\nIf you are writing a program that often computes factorials,\nit is likely to be much more time efficient to simply pre-compute\nand store the 12 values in a table.\nWhenever the program needs the value of  it can\nsimply check the lookup table.\n(If , the value is too large to store as an \nvariable anyway.)\nCompared to the time required to compute factorials, it may be well\nworth the small amount of additional space needed to store the\nlookup table."}, "Lookup tables can also store approximations\nfor an expensive function such as sine or cosine.\nIf you compute this function only for exact degrees or are\nwilling to approximate the answer with the value for the nearest\ndegree, then a lookup table storing the computation for exact degrees\ncan be used instead of repeatedly computing the sine function.\nNote that initially building the lookup table requires a certain\namount of time.\nYour application must use the lookup table often\nenough to make this initialization worthwhile.", {"title_reference": ["n", "n-1", "binsort <binsort> <BinSort>"], "#text": "Another example of the space/time tradeoff is typical of what a\nprogrammer might encounter when trying to optimize space.\nHere is a simple code fragment for sorting an array of integers.\nWe assume that this is a special case where there are \nintegers whose values are a permutation\nof the integers from 0 to .\nThis is an example of a .\nBinsort assigns each value to an array position corresponding to its\nvalue."}, {"title_reference": ["Theta(n)", "n"], "#text": "This is efficient and requires  time.\nHowever, it also requires two arrays of size .\nNext is a code fragment that places the permutation in order but does\nso within the same array (thus it is an example of an \"in place\"\nsort)."}, {"literal": ["swap(A, i, j)", "i", "j", "A", "for", "A[i]", "swap"], "title_reference": ["i", "i", "n", "Theta(n)"], "#text": "Function  exchanges elements \nand  in array .\nIt may not be obvious that the second code fragment\nactually sorts the array.\nTo see that this does work, notice that each pass through the\n loop will at least move the integer with value \nto its correct position in the array, and that during this iteration,\nthe value of  must be greater than or equal to .\nA total of at most   operations take place, because\nan integer cannot be moved out of its correct position once it has\nbeen placed there, and each swap operation places at least one integer\nin its correct position.\nThus, this code fragment has cost .\nHowever, it requires more time to run than the first code fragment.\nOn my computer the second version takes nearly twice as long to run\nas the first, but it only requires half the space."}, {"title_reference": "stored on disk <file processing> <FileProc>", "#text": "A second principle for the relationship between a program's space and\ntime requirements applies to programs that process\ninformation .\nStrangely enough, the disk-based space/time tradeoff principle is\nalmost the reverse of the space/time tradeoff principle for programs\nusing main memory."}, {"title_reference": "disk-based space/time tradeoff", "#text": "The  principle states that the\nsmaller you can make your disk storage requirements, the faster your\nprogram will run.\nThis is because the time to read information from disk is enormous\ncompared to computation time, so almost any amount of additional\ncomputation needed to unpack the data is going to be less than the\ndisk-reading time saved by reducing the storage requirements.\nNaturally this principle does not hold true in all cases,\nbut it is good to keep in mind when designing programs that process\ninformation stored on disk."}], "topic": [{"title": "Example", "paragraph": {"title_reference": ["n", "c", "cn", "Theta(n)"], "#text": "What are the space requirements for an array of  integers?\nIf each integer requires  bytes, then the array requires\n bytes, which is ."}}, {"title": "Example", "paragraph": {"title_reference": ["n", "n times n", "j", "i", "j", "i", "i", "j", "n", "Theta(n^2)"], "#text": "Imagine that we want to keep track of friendships between \npeople.\nWe can do this with an array of size .\nEach row of the array represents the friends of an individual, with\nthe columns indicating who has that individual as a friend.\nFor example, if person  is a friend of person ,\nthen we place a mark in column  of row  in the\narray.\nLikewise, we should also place a mark in column  of row\n if we assume that friendship works both ways.\nFor  people, the total size of the array is\n."}}], "block_quote": [{"raw": {"@format": "xml", "@xml:space": "preserve", "codeinclude": "null"}}, {"raw": {"@format": "xml", "@xml:space": "preserve", "codeinclude": "null"}}]}}