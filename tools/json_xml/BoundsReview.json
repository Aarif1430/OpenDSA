{"document": {"@ids": "bounds-review", "@names": "bounds\\ review", "@source": "<string>", "@title": "Bounds Review", "title": "Bounds Review", "comment": [{"@xml:space": "preserve", "#text": "This file is part of the OpenDSA eTextbook project. See"}, {"@xml:space": "preserve", "#text": "http://algoviz.org/OpenDSA for more details."}, {"@xml:space": "preserve", "#text": "Copyright (c) 2012-2016 by the OpenDSA Project Contributors, and"}, {"@xml:space": "preserve", "#text": "distributed under an MIT open source license."}], "raw": [{"@format": "xml", "@xml:space": "preserve", "avmetadata": "null"}, {"@format": "xml", "@xml:space": "preserve", "odsalink": "AV/SeniorAlgAnal/SimpleCostsCON.css"}], "section": [{"@ids": "introduction", "@names": "introduction", "title": "Introduction", "paragraph": [{"title_reference": ["upper bound <problem upper bound>", "lower bound <problem lower bound>"], "#text": "We define the  for a problem\nto be the upper bound of the best algorithm we know for that\nproblem, and the\n to be the tightest lower\nbound that we can prove over all algorithms for that problem.\nWhile we usually can recognize the upper bound for a given algorithm,\nfinding the tightest lower bound for all possible algorithms is often\ndifficult, especially if that lower bound is more than the\n\"trivial\" lower bound determined by measuring the amount\nof input that must be processed."}, "The benefits of being able to discover a strong lower bound are\nsignificant.\nIn particular, when we can make the upper and lower bounds for a\nproblem meet, this means that we truly understand our problem in a\ntheoretical sense.\nIt also saves us the effort of attempting to discover more\n(asymptotically) efficient algorithms when no such algorithm can\nexist.", {"title_reference": ["reduction <reduction> <Reduction>", "sorting lower bound proof <sorting lower bound> <SortingLowerBound>", "O(n log n)", "worst case"], "#text": "Often the most effective way to determine the lower bound for a\nproblem is to find a  to\nanother problem whose lower bound is already known.\nHowever, this approach does not help us when we cannot find a suitable\n\"similar problem\".\nOne thing that we will focus on is discovering and proving lower bounds\nfrom first principles.\nOur most  significant example of a lower bounds argument so far is the\n,\nwhich shows that the problem of sorting has a lower bound of\n in the ."}, {"strong": "for all possible algorithms", "title_reference": "Omega(n)", "#text": "The lower bound for the problem is the tightest (highest) lower bound\nthat we can prove  that solve the\nproblem.\nThis can be a difficult bar, given that we cannot possibly know all\nalgorithms for any problem, because there are theoretically an\ninfinite number.\nHowever, we can often recognize a simple lower bound based on the\namount of input that must be examined.\nFor example, we can argue that the lower bound for any algorithm to\nfind the maximum-valued element in an unsorted list must be\n because any algorithm must examine all of the inputs\nto be sure that it actually finds the maximum value."}, {"title_reference": ["O(n)", "Omega(n)"], "#text": "In the case of maximum finding, the fact that we know of a simple\nalgorithm that runs in  time, combined with the fact that\nany algorithm needs  time, is significant.\nBecause our upper and lower bounds meet (within a constant factor),\nwe know that we indeed have a \"good\" algorithm for solving the\nproblem.\nIt is possible that someone can develop an implementation that is a\n\"little\" faster than an existing one, by a constant factor.\nBut we know that its not possible to develop one that is\nasymptotically better."}, "We must be careful about how we interpret this last statement,\nhowever.\nThe world is certainly better off for the invention of Quicksort,\neven though Mergesort was available at the time.\nQuicksort is not asymptotically faster than Mergesort, yet is not\nmerely a \"tuning\" of Mergesort either.\nQuicksort is a substantially different approach to sorting.\nSo even when our upper and lower bounds for a problem meet,\nthere are still benefits to be gained from a new, clever algorithm."]}, {"@ids": "analyzing-problems", "@names": "analyzing\\ problems", "title": "Analyzing Problems", "paragraph": [{"title_reference": ["n", "n", "n", "n", "n", "n", "n"], "#text": "Our first example for analyzing a problem was Towers of Hanoi.\nThis had the advantage that for a given input size , there\nwas only one input (the value ).\nOf course, this is not always true.\nFor example, there are a number of ways to think about the input to\nthe problem of finding the maximum value in an array of \nrecords.\nWe could consider an array of  arbitrary values,\nin which there are an infinite number of inputs of size .\nIt might be hard for us to understand the analysis of some problem if\nwe cannot even enumerate all of the possible inputs.\nSo we might prefer a simpler model that we believe will not change the\nunderlying behavor.\nFor example, we could assume that the input is some permutation of the\nvalues from 1 to .\nWhile we probably do not want to restrict our find algorithm to such\ninput, if all we care about for analysis purposes is the position of\nthe biggest value within the array, then a permutation of the values\nfrom 1 to  might make it easier for us to think through all\nof the possibilities."}, {"title_reference": ["X", "n"], "#text": "Another complicating factor that might or might not arise is that\ndifferent inputs of a given size might have different costs.\nFor example, we probably realize that to find the maximum value in the\narray, we need to look at all values of the array.\nSo, the order of the values does not really affect the time required.\nHowever, consider if our problem is to find where in the array the\nrecord with value  appears, if any such record exists.\nNow, not only are there many possible inputs of size , but\nthose inputs have different costs when using, for example, a simple\nsequential search from the start of the array.\nThis is why we need the concepts of best, average, and worst case\ninputs."}, {"title_reference": "n", "#text": "To make things even worse, the cost to solve the problem for a\ngiven input depends on the algorithm that we use!\nFor example, which input of size  is the worst is different\nfor the algorithm that moves sequentially through the array from the\nstart to the end, as opposed to the algorithm that moves sequentially\nthrough the array from the end to the start."}, {"title_reference": ["worst-case cost <worst case>", "n", "best-case cost <best case>", "n", "n", "n", "n"], "#text": "The  (for all input of size $n$)\nis the maximum cost for the algorithm over all problem instances of\nsize .\nThe  (for all input of size $n$)\nis the minimum cost for the algorithm over all problem instances of\nsize .\nIt is possible that the {best, worst} case cost changes\nradically with .\nThat is, even  might have a very different cost from odd\n."}, {"title_reference": ["mathcal{A}", "I_n", "mathcal{A}", "n", "I", "I_n", "f_mathcal{A}", "mathcal{A}"], "#text": "We will use the following notation at varous times this semester.\n is an algorithm.\n is the set of all possible inputs to \nof size .\n is an input in .\n is a function that expresses the resource cost\nof algorithm .\nUsing this notation, we can define the worst and best case costs as:"}, {"title_reference": ["n", "n", "n=1"], "#text": "This point that we are considering all of the inputs of size \nis crucial.\nIn other words, we don't pick the  for which the best\n(or worst) case occurs.\nSo it would be wrong to say something like\n\"The best case is when .\""}, {"title_reference": ["average-case cost <average case>", "n", "(n+1)/2"], "strong": "only", "#text": "If we want the ,\nit is even more complicated.\nWe might model this as half way between the best and worst case costs,\nbut this is not often correct.\n(Think about what circumstances would make it correct, and some\nsituations where it would not be correct.)\nTo account for the true average cost for input of size ,\nwe have to consider the entire collection of such inputs.\nFor each one of these, we need its relative frequency, and its cost.\nFrequencies for inputs can be hard to determine!\nFor example, the average cost of sequential search is ,\nbut  if every position of the array is equally likely to hold\nthe value that we are looking for.\nAnd what do we do about the situation where the value is not even in\nthe array?"}, "However, ideally we have all the information that we need to calcluate\nthe average case cost.\nThen we can calculate the weighted average:", "Think about this: Can the average cost be worse than the worst cost?\nOr better than the best cost?", {"title_reference": ["mathcal{M}", "mathcal{A_M}", "mathcal{M}"], "strong": "worst case", "#text": "So now we are ready to give a more precise definition for the lower\nbound of a problem.\nAs always, we have to define it for some class of inputs.\nWe also have to consider that there are many (infinitely many in\ntheory) algorithms that solve the problem.\nRecall that to analyze any problem, we have to define a model that\nincludes the definition for problem size and the definition for\nsolution cost.\nCall such a model .\nThen,  is the set of all algorithms that solve\nthe problem under model .\nThen, the lower bound of a problem in the  is:"}], "math_block": [{"@xml:space": "preserve", "#text": "\\mbox{worst cost}(\\mathcal{A}) = \\max_{I \\in I_n}\nf_{\\mathcal{A}}(I)."}, {"@xml:space": "preserve", "#text": "\\mbox{best cost}(\\mathcal{A}) = \\min_{I \\in I_n}\nf_{\\mathcal{A}}(I)."}, {"@xml:space": "preserve", "#text": "\\frac{\\sum_{I\\in I_n} \\mathrm{freq}(I) *\n\\mathrm{cost}(I)}{\\mathrm{total\\ count\\ of\\ frequencies}}"}, {"@xml:space": "preserve", "#text": "\\min_{{\\mathcal A} \\in {\\mathcal A}_M} \\left\\{\n\\max_{I \\in I_n} f_{\\mathcal A}(I)\\right\\}"}], "raw": {"@format": "xml", "@xml:space": "preserve", "inlineav": {"@type": "dgm", "@exer_name": "SimpleCostsCON", "@long_name": "SimpleCostsCON", "@points": "0", "@required": "True", "@threshold": "1.0"}}, "section": {"@ids": "modeling-the-inputs", "@names": "modeling\\ the\\ inputs", "title": "Modeling the Inputs", "paragraph": ["Especially when trying to come to grips with what the average case\ncost of the algorithm will be, it might be easier to think about what\nis going on if we simplify the model that we use for the class of\ninputs that we are considering.", {"title_reference": ["X", "n", "n", "n"], "#text": "Think about the seemingly simple problem of finding the value\n in an (unsorted) array of  records.\nWhat are the inputs to this problem?\nOf course, an array of  records!\nBut what does that mean if we try to enumerate all of the inputs of\nsize ?\nHow many such inputs are there?"}, "Well, if each position in the array can take any value, then there are\nan infinite number of values for each position.\nEven if we restrict these values to something like a 64-bit integer,\nit is still a lot of possibilities to consider!", {"title_reference": ["n", "X"], "#text": "Given the cognative load involved in thinking about all of those\ninputs, we might want to consider instead analyzing a simpler set of\ninputs.\nFor example, we might decide to only consider (for analysis purposes)\nthat the input is a permuation of the numbers 1 to .\nThe argument here might be that we don't care about the actual values\nin the array. We only care about whether a given value is  or\nnot, and so we can simplify the inputs that we consider."}, {"title_reference": ["n", "n", "n", "n", "n", "n"], "#text": "There are two dangers that we have to be aware of when doing such a\nsimplification.\nFirst, our simplification has to still reflect reality.\nIf we simplify an array of  numbers as a permutation of the\nnumbers 1 to , then we eliminate the inputs that have\nduplicates.\nThat might make for a wrong analysis.\nSecond, we have to separate the issue of inputs for the purpose of\nanalysis from inputs for the purpose of solving a problem.\nIn the case of sorting, we might want to analyze behavior on a\ncollection of  records each with a unique key value.\nSince we don't care about the actual key values, we might simplify\nthis to some permuation of a set of records with key values 1 to\n.\nBut, sorting a collection of records whose keys are known to be a\npermuation of the values 1 to  is much simpler than sorting a\ncollection of  arbitrary records!\nFor example, we can sort the permutation in linear time with a simple\nBinsort."}, {"title_reference": ["X", "n", "X", "X", "X", "X"], "#text": "Going back to the example of finding value  in an array of\n records,\nwe might want to consider a model that only considers the position of\nthe first occcurance of  in the array.\nIn other words, we lump all inputs whose first occurance of \nis in the first position into one input.\nAll inputs whose first occurance of  is in the second\nposition is another input.\nAnd so on.\nThen we can analyze the cost only for those \"groups\" of inputs that we\ncare about.\nOf course, we might have difficulty deciding what the proper\nfrequencies are for each of these synthetic input groups.\nPerhaps it is reasonable to say that each position in the array has\nequal probability of holding the first occurrance of .\nOr perhaps it is not."}], "footnote": {"@auto": "1", "@ids": "id1", "@names": "1", "label": "1", "paragraph": "Throughout this discussion, it should be\nunderstood that any mention of bounds must specify what class\nof inputs are being considered.\nDo we mean the bound for the worst case input?\nThe average cost over all inputs?\nRegardless of which class of inputs we\nconsider, all of the issues raised apply equally."}, "raw": {"@format": "xml", "@xml:space": "preserve", "odsascript": "AV/SeniorAlgAnal/SimpleCostsCON.js"}}}]}}