<?xml version="1.0" encoding="utf8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.2a -->
<document dupnames="an\ empirical\ comparison\ of\ sorting\ algorithms" ids="an-empirical-comparison-of-sorting-algorithms" source="&lt;string&gt;" title="An Empirical Comparison of Sorting Algorithms"><title>An Empirical Comparison of Sorting Algorithms</title><subtitle dupnames="an\ empirical\ comparison\ of\ sorting\ algorithms" ids="id1">An Empirical Comparison of Sorting Algorithms</subtitle><comment xml:space="preserve">This file is part of the OpenDSA eTextbook project. See</comment><comment xml:space="preserve">http://algoviz.org/OpenDSA for more details.</comment><comment xml:space="preserve">Copyright (c) 2012-2016 by the OpenDSA Project Contributors, and</comment><comment xml:space="preserve">distributed under an MIT open source license.</comment><raw format="xml" xml:space="preserve"><avmetadata>null</avmetadata></raw><raw format="xml" xml:space="preserve"><index>null</index></raw><paragraph>Which sorting algorithm is fastest?  Asymptotic complexity analysis
lets us distinguish between <math>\Theta(n^2)</math> and
<math>\Theta(n \log n)</math> algorithms, but it does not help distinguish
between algorithms with the same asymptotic complexity.
Nor does asymptotic analysis say anything about which algorithm is
best for sorting small lists.
For answers to these questions, we can turn to empirical testing.</paragraph><target refid="sortcomptable"></target><topic ids="sortcomptable" names="sortcomptable"><title>Table</title><paragraph>Empirical comparison of sorting algorithms run on a 3.4 GHz Intel
Pentium 4 CPU running Linux.
All times shown are milliseconds.</paragraph><math_block xml:space="preserve">\begin{array}{l|rrrrrrrr}
\hline
\textbf{Sort} &amp; \textbf{10}&amp; \textbf{100} &amp; \textbf{1K}&amp;
\textbf{10K} &amp; \textbf{100K}&amp; \textbf{1M}&amp; \textbf{Up} &amp; \textbf{Down}\\
\hline
\textrm{Insertion} &amp; .00023 &amp; .007 &amp; 0.66 &amp;  64.98 &amp;  7381.0 &amp;  674420 &amp; 0.04 &amp; 129.05\\
\textrm{Bubble}    &amp; .00035 &amp; .020 &amp; 2.25 &amp; 277.94 &amp; 27691.0 &amp; 2820680 &amp;  70.64 &amp; 108.69\\
\textrm{Selection} &amp; .00039 &amp; .012 &amp; 0.69 &amp;  72.47 &amp;  7356.0 &amp;  780000 &amp;  69.76 &amp;  69.58\\
\textrm{Shell}     &amp; .00034 &amp; .008 &amp; 0.14 &amp;   1.99 &amp;    30.2 &amp;     554 &amp;   0.44 &amp;   0.79\\
\textrm{Shell/O}   &amp; .00034 &amp; .008 &amp; 0.12 &amp;   1.91 &amp;    29.0 &amp;     530 &amp;   0.36 &amp;   0.64\\
\textrm{Merge}     &amp; .00050 &amp; .010 &amp; 0.12 &amp;   1.61 &amp;    19.3 &amp;     219 &amp;   0.83 &amp;   0.79\\
\textrm{Merge/O}   &amp; .00024 &amp; .007 &amp; 0.10 &amp;   1.31 &amp;    17.2 &amp;     197 &amp;   0.47 &amp;   0.66\\
\textrm{Quick}     &amp; .00048 &amp; .008 &amp; 0.11 &amp;   1.37 &amp;    15.7 &amp;     162 &amp;   0.37 &amp;   0.40\\
\textrm{Quick/O}   &amp; .00031 &amp; .006 &amp; 0.09 &amp;   1.14 &amp;    13.6 &amp;     143 &amp;   0.32 &amp;   0.36\\
\textrm{Heap}      &amp; .00050 &amp; .011 &amp; 0.16 &amp;   2.08 &amp;    26.7 &amp;     391 &amp;   1.57 &amp;   1.56\\
\textrm{Heap/O}    &amp; .00033 &amp; .007 &amp; 0.11 &amp;   1.61 &amp;    20.8 &amp;     334 &amp;   1.01 &amp;   1.04\\
\textrm{Radix/4}   &amp; .00838 &amp; .081 &amp; 0.79 &amp;   7.99 &amp;    79.9 &amp;     808 &amp;   7.97 &amp;   7.97\\
\textrm{Radix/8}   &amp; .00799 &amp; .044 &amp; 0.40 &amp;   3.99 &amp;    40.0 &amp;     404 &amp;   4.00 &amp;   3.99\\
\hline
\end{array}</math_block></topic><paragraph>Table <title_reference>#SortCompTable</title_reference> shows timing results for
actual implementations of the sorting algorithms presented in this
chapter.
The algorithms compared include
<title_reference>Insertion Sort &lt;insertion sort&gt; &lt;InsertionSort&gt;</title_reference>,
<title_reference>Bubble Sort &lt;bubble sort&gt; &lt;BubbleSort&gt;</title_reference>,
<title_reference>Selection Sort &lt;selection sort&gt; &lt;SelectionSort&gt;</title_reference>,
<title_reference>Shellsort &lt;Shellsort&gt; &lt;Shellsort&gt;</title_reference>,
<title_reference>Quicksort &lt;Quicksort&gt; &lt;Quicksort&gt;</title_reference>,
<title_reference>Mergesort &lt;Mergesort&gt; &lt;Mergesort&gt;</title_reference>,
<title_reference>Heapsort &lt;Heapsort&gt; &lt;Heapsort&gt;</title_reference>,
<title_reference>Radix Sort &lt;radix sort&gt; &lt;Radixsort&gt;</title_reference>.</paragraph><paragraph>Shellsort compares times for both the basic version and a version with
increments based on division by three.
Mergesort compares both the basic array-based implementation and an
optimized version (which includes calls to Insertion Sort for lists of
length below nine).
For Quicksort, two versions are compared: the basic implementation
and an optimized version that does not partition sublists below length
nine (with Insertion Sort performed at the end).
The first Heapsort version uses a standard class definition with
methods to implement access functions like "parent".
The second version removes all the method definitions and operates
directly on the array using inlined code for all access functions.</paragraph><paragraph>Except for the rightmost columns,
the input to each algorithm is a random array of integers.
This affects the timing for some of the sorting algorithms.
For example, Selection Sort is not being used to best advantage
because the record size is small, so it does not get the best possible
showing.
The Radix Sort implementation certainly takes advantage of this
key range in that it does not look at more digits than necessary.
On the other hand, it was not optimized to use bit shifting instead of
division, even though the bases used would permit this.</paragraph><paragraph>The various sorting algorithms are shown for lists of sizes
10, 100, 1000, 10,000, 100,000, and 1,000,000.
The final two columns of each table show the performance for the
algorithms on inputs of size 10,000 where the numbers are in
ascending (sorted) and descending (reverse sorted) order,
respectively.
These columns demonstrate best-case performance for some
algorithms and worst-case performance for others.
They also show that for some algorithms, the order of input
has little effect.</paragraph><paragraph>These figures show a number of interesting results.
As expected, the <math>O(n^2)</math> sorts are quite poor performers for
large arrays.
Insertion Sort is by far the best of this group, unless the array is
already reverse sorted.
Shellsort is clearly superior to any of these <math>O(n^2)</math> sorts for
lists of even 100 records.
Optimized Quicksort is clearly the best overall algorithm for all but
lists of 10 records.
Even for small arrays, optimized Quicksort performs well because
it does one partition step before calling Insertion Sort.
Compared to the other <math>O(n \log n)</math> sorts, unoptimized Heapsort
is quite slow due to the overhead of the class structure.
When all of this is stripped away and the algorithm is implemented to
manipulate an array directly, it is still somewhat slower than
mergesort.
In general, optimizing the various algorithms makes a
noticeable improvement for larger array sizes.</paragraph><paragraph>Overall, Radix Sort is a surprisingly poor performer.
If the code had been tuned to use bit shifting of the key value, it
would likely improve substantially;
but this would seriously limit the range of record types that the
sort could support.</paragraph><paragraph>Here are a few multiple choice questions that ask you to
compare the sorting algorithms that we learned about in this chapter.</paragraph><raw format="xml" xml:space="preserve"><avembed
    type="ka"
    exer_name="SortAlgCompSumm"
    long_name="SortAlgCompSumm"
    points="1.0"
    required="True"
    threshold="5">
</avembed>
</raw></document>