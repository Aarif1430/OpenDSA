<div id="content">
<ODSAtitle>Quicksort</ODSAtitle>
<ODSAprereq "Sorting" />
<ODSAprereq "BST" />
<ODSAprereq "InsertionSort" />
<ODSAprereq "MergeSort" />

<p>
While Mergesort uses the most obvious form of divide and conquer
(split the list in half then sort the halves), it is not the only way
that we can break down the sorting problem.
And we saw that doing the merge step for Mergesort when using an array
implementation is not so easy.
So perhaps a different divide and conquer strategy might turn out to
be more efficient?
</p>

<p>
<dfn>Quicksort</dfn> is aptly named because, when properly
implemented, it is the fastest known general-purpose in-memory sorting
algorithm in the average case.
It does not require the extra array needed by Mergesort, so it is
space efficient as well.
Quicksort is widely used, and is typically the algorithm implemented
in a library sort routine such as the UNIX <code>qsort</code>
function.
Interestingly, Quicksort is hampered by exceedingly poor worst-case
performance, thus making it inappropriate for certain applications.
</p>

<p>
Before we get to Quicksort, consider for a moment the practicality
of using a Binary Search Tree for sorting.
You could insert all of the values to be sorted into the BST
one by one, then traverse the completed tree using an inorder traversal.
The output would form a sorted list.
This approach has a number of drawbacks, including the extra space
required by BST pointers and the amount of time required to insert
nodes into the tree.
However, this method introduces some interesting ideas.
First, the root of the BST (i.e., the first node inserted) splits the
list into two sublists:
The left subtree contains those values in the
list less than the root value while the right subtree contains those
values in the list greater than or equal to the root value.
Thus, the BST implicitly implements a "divide and conquer" approach
to sorting the left and right subtrees.
Quicksort implements this concept in a much more efficient
way.
</p>

<p>
Quicksort first selects a value called the <dfn>pivot</dfn>.
(This is conceptually like the root node's value in the BST.)
Assume that the input array contains \(k\) values less than the pivot.
The records are then rearranged in such a way that the \(k\) values less
than the pivot are placed in the first, or leftmost, \(k\) positions
in the array, and the values greater than or equal to
the pivot are placed in the last, or rightmost, \(n-k\) positions.
This is called a <dfn>partition</dfn> of the array.
The values placed in a given partition need not (and typically will
not) be sorted with respect to each other.
All that is required is that all values end up in the correct
partition.
The pivot value itself is placed in position \(k\).
Quicksort then proceeds to sort the resulting subarrays now on either
side of the pivot, one of size \(k\) and the other of size \(n-k-1\).
How are these values sorted?
Because Quicksort is such a good algorithm, using Quicksort on
the subarrays would be appropriate.
</p>

<p>
Unlike some of the sorts that we have seen earlier in this chapter,
Quicksort might not seem very "natural" in that it is not an
approach that a person is likely to use to sort real objects.
But it should not be too surprising that a really efficient sort for
huge numbers of abstract objects on a computer would be rather
different from our experiences with sorting a relatively few physical
objects.
</p>

<p>
Here is the Java code for Quicksort.
Parameters <code>i</code> and <code>j</code> define the left and right
indices, respectively, for the subarray being sorted.
The initial call to Quicksort would be
<code>qsort(array, 0, n-1)</code>.
<p>

<pre>
static &lt;E extends Comparable&lt;? super E>>
void qsort(E[] A, int i, int j) {      // Quicksort
  int pivotindex = findpivot(A, i, j); // Pick a pivot
  DSutil.swap(A, pivotindex, j);       // Stick pivot at end
  // k will be the first position in the right subarray
  int k = partition(A, i-1, j, A[j]);
  DSutil.swap(A, k, j);              // Put pivot in place
  if ((k-i) > 1) qsort(A, i, k-1);   // Sort left partition
  if ((j-k) > 1) qsort(A, k+1, j);   // Sort right partition
}
</pre>

<p>
Function <code>partition</code> will move records to the
appropriate partition and then return <code>k</code>, the first
position in the right partition.
Note that the pivot value is initially placed at the end of the array
(position <code>j</code>).
Thus, <code>partition</code> must not affect the value of array
position <code>j</code>.
After partitioning, the pivot value is placed in position
<code>k</code>,
which is its correct position in the final, sorted array.
By doing so, we guarantee that at least one value (the pivot) will not
be processed in the recursive calls to <code>qsort</code>.
Even if a bad pivot is selected, yielding a completely empty
partition to one side of the pivot, the larger partition will contain
at most \(n-1\) elements.
</p>

<p>
Selecting a pivot can be done in many ways.
The simplest is to use the first key.
However, if the input is sorted or reverse sorted, this will produce a
poor partitioning with all values to one side of the pivot.
It is better to pick a value at random, thereby reducing the chance of
a bad input order affecting the sort.
Unfortunately, using a random number generator is relatively
expensive, and we can do nearly as well by selecting the middle
position in the array.
Here is a simple <code>findpivot</code> function.
</p>

<pre>
static &lt;E extends Comparable&lt;? super E>>
int findpivot(E[] A, int i, int j)
  { return (i+j)/2; }
</pre>

<p>
We now turn to function <code>partition</code>.
If we knew in advance how many keys are less than the pivot,
<code>partition</code> could simply copy elements with key values less
than the pivot to the low end of the array, and elements with larger
keys to the high end.
Because we do not know in advance how many keys are less than
the pivot,
we use a clever algorithm that moves indices inwards from the
ends of the subarray, swapping values as necessary until the two
indices meet.
Here is a Java implementation for the partition step.
</p>

<pre>
static &lt;E extends Comparable&lt;? super E>>
int partition(E[] A, int l, int r, E pivot) {
  do {                 // Move bounds inward until they meet
    while (A[++l].compareTo(pivot) < 0);
    while ((r!=0) && (A[--r].compareTo(pivot)>0));
    DSutil.swap(A, l, r);       // Swap out-of-place values
  } while (l < r);              // Stop when they cross
  DSutil.swap(A, l, r);         // Reverse last, wasted swap
  return l;      // Return first position in right partition
}
</pre>

<p>
Figure <ODSAref "Partition" /> illustrates <code>partition</code>.
Initially, variables <code>l</code> and <code>r</code> are immediately
outside the actual bounds of the subarray being partitioned.
Each pass through the outer <code>do</code> loop moves the counters
<code>l</code> and <code>r</code> inwards, until eventually they meet.
Note that at each iteration of the inner <code>while</code> loops, the
bounds are moved prior to checking against the pivot value.
This ensures that progress is made by each <code>while</code> loop,
even when the two values swapped on the last iteration of the
<code>do</code> loop were equal to the pivot.
Also note the check that <code>r > l</code> in the second
<code>while</code> loop.
This ensures that <code>r</code> does not run off the low end of the
partition in the case where the pivot is the least value in that
partition.
Function <code>partition</code> returns the first index of the right
partition so that the subarray bound for the recursive calls to
<code>qsort</code> can be determined.
Figure <ODSAref "QuickSortPic" /> illustrates the complete Quicksort
algorithm.

<figure>
<center>
<img src="Images/Partit.png" height="300" alt="The Quicksort partition step" />
</center>

<figcaption>
<ODSAfig "Partition" />
The Quicksort partition step.
The first row shows the initial positions for a collection of ten key
values.
The pivot value is 60, which has been swapped to the end of the array.
The <code>do</code> loop makes three iterations, each time moving
counters <code>l</code> and <code>r</code> inwards until they meet in
the third pass.
In the end, the left partition contains four values and the right
partition contains six values.
Function <code>qsort</code> will place the pivot value into
position 4.
</figcaption>
</figure>

<figure>
<center>
<img src="Images/Qsort.png" height="400" alt="An illustration of Quicksort" />
</center>

<figcaption>
<ODSAfig "QuickSortPic" />
An illustration of Quicksort.
</figcaption>
</figure>

<p>
To analyze Quicksort, we first analyze the <code>findpivot</code> and
<code>partition</code> functions operating on a subarray of length
\(k\).
Clearly, <code>findpivot</code> takes constant time.
Function <code>partition</code> contains a <code>do</code> loop with
two nested <code>while</code> loops.
The total cost of the partition operation is constrained by
how far <code>l</code> and <code>r</code> can move inwards.
In particular, these two bounds variables together can move a total of
\(s\) steps for a subarray of length \(s\).
However, this does not directly tell us how much work is done by the
nested <code>while</code> loops.
The <code>do</code> loop as a whole is guaranteed to move both
<code>l</code> and <code>r</code> inward at least one position on each
first pass.
Each <code>while</code> loop moves its variable at least once (except
in the special case where <code>r</code> is at the left edge of the
array, but this can happen only once).
Thus, we see that the <code>do</code> loop can be executed at most
\(s\) times, the total amount of work done moving <code>l</code> and
<code>r</code> is \(s\), and
each <code>while</code> loop can fail its test at most \(s\) times.
The total work for the entire <code>partition</code> function is
therefore \(\Theta(s)\).
<p>

<p>
Knowing the cost of <code>findpivot</code> and <code>partition</code>,
we can determine the cost of Quicksort.
We begin with a worst-case analysis.
The worst case will occur when the pivot does a poor job of breaking
the array, that is, when there are no elements in one partition, and
\(n-1\) elements in the other.
In this case, the divide and conquer
strategy has done a poor job of
dividing, so the conquer phase will work on a subproblem only one
less than the size of the original problem.
If this happens at each partition step, then the total cost of the
algorithm will be
\[\sum_{k=1}^n k = \Theta(n^2).\]
</p>

<p>
In the worst case, Quicksort is \(\Theta(n^2)\).
This is terrible,
no better than Bubble Sort. <sup><a href="#fn1" id="r1">[1]</a></sup>
When will this worst case occur?
Only when each pivot yields a bad partitioning of the array.
If the pivot values are selected at random, then this is extremely
unlikely to happen.
When selecting the middle position of the current subarray, it is
still unlikely to happen.
It does not take many good partitionings for Quicksort to
work fairly well.
</p>

<p>
Quicksort's best case occurs when <code>findpivot</code> always breaks
the array into two equal halves.
Quicksort repeatedly splits the array into
smaller partitions, as shown in Figure <ODSAref "QuickSortPic" />.
In the best case, the result will be \(\log n\) levels of partitions,
with the top level having one array of size \(n\), the second level
two arrays of size \(n/2\), the next with four arrays of size \(n/4\),
and so on.
Thus, at each level, all partition steps for that level do a total of
\(n\) work, for an overall cost of \(n \log n\) work when Quicksort
finds perfect pivots.
</p>

<p>
Quicksort's average-case behavior falls somewhere
between the extremes of worst and best case.
Average-case analysis considers the cost for all possible arrangements
of input, summing the costs and dividing by the number of cases.
We make one reasonable simplifying assumption:
At each partition step, the pivot is
equally likely to end in any position in the (sorted) array.
In other words, the pivot is equally likely to break an array into
partitions of sizes 0 and \(n-1\), or 1 and \(n-2\), and so on.
</p>

<p>
Given this assumption, the average-case cost is computed from the
following equation:
\[{\bf T}(n) = cn + \frac{1}{n}\sum_{k=0}^{n-1}[{\bf T}(k) +
{\bf T}(n - 1 - k)],
\quad {\bf T}(0) = {\bf T}(1) = c.\]
This equation is in the form of a recurrence relation.
Recurrence relations are discussed in Modules <ODSAref "MathPre" />
and <ODSAref "AnalTech" />,
and this one is solved in Module <ODSAref "QuickAnal" />.
This equation says that there is one chance in \(n\) that the pivot
breaks the array into subarrays of size 0 and \(n-1\),
one chance in \(n\) that the pivot breaks the array into
subarrays of size 1 and \(n-2\), and so on.
The expression "\( {\bf T}(k) + {\bf T}(n - 1 - k)\)" is the cost
for the two recursive calls to Quicksort on two arrays of size \(k\)
and \(n-1-k\).
The initial \(cn\) term is the cost of doing the
<code>findpivot</code> and <code>partition</code> steps, for some
constant \(c\).
The closed-form solution to this recurrence relation is
\(\Theta(n \log n)\).
Thus, Quicksort has average-case cost \(\Theta(n \log n)\).
<p>

<p>
This is an unusual situation that the average case cost and the worst
case cost have asymptotically different growth rates.
Consider what "average case" actually means.
We compute an average cost for inputs of size \(n\) by summing up for
every possible input of size \(n\) the product of the running time
cost of that input times the probability that that input will occur.
To simplify things, we assumed that every permutation is equally
likely to occur.
Thus, finding the average means summing up the cost for every
permutation and dividing by the number of inputs (\(n!\)).
We know that some of these \(n!\) inputs cost \(O(n^2)\).
But the sum of all the permutation costs has to be \((n!)(O(n \log n))\).
Given the extremely high cost of the worst inputs, there must be
very few of them.
In fact, there cannot be a constant fraction of the inputs with cost
\(O(n^2)\).
Even, say, 1% of the inputs with cost \(O(n^2)\) would lead to an
average cost of \(O(n^2)\).
Thus, as \(n\) grows, the fraction of inputs with high cost must be
going toward a limit of zero.
We can conclude that Quicksort will have good behavior if
we can avoid those very few bad input permutations.
</p>

<p>
The running time for Quicksort can be improved (by a constant factor),
and much study has gone into optimizing this algorithm.
Since Quicksort's worst case behavior arises when the pivot does a
poor job of splitting the array into equal size subarrays,
improving <code>findpivot</code> seems like a good place to start.
If we are willing to do more work searching for a better pivot, the
effects of a bad pivot can be decreased or even eliminated.
Hopefully this will save more time than was added by the additional
work needed to find the pivot.
One widely-used choice is to use the "median of three" algorithm,
which uses as a pivot the middle of three randomly selected values.
Using a random number generator to choose the positions is relatively
expensive, so a common compromise is to look at the first, middle, and
last positions of the current subarray.
However, our simple <code>findpivot</code> function that takes the
middle value as its pivot has the virtue of making it highly unlikely
to get a bad input by chance, and it is quite cheap to implement.
This is in sharp contrast to selecting the first or last element as
the pivot, which would yield bad performance for many permutations
that are nearly sorted or nearly reverse sorted.
</p>

<p>
A significant improvement can be gained by recognizing that
Quicksort is relatively slow when \(n\) is
small.
This might not seem to be relevant if most of the time we sort
large arrays, nor should it matter how long Quicksort takes in the
rare instance when a small array is sorted because it will be fast
anyway.
But you should notice that Quicksort itself sorts many, many small
arrays!
This happens as a natural by-product of the divide and conquer
approach.
</p>

<p>
A simple improvement might then be to replace Quicksort with a faster
sort for small numbers, say Insertion Sort or Selection Sort.
However, there is an even better &mdash; and still simpler &mdash;
optimization.
When Quicksort partitions are below a certain size, do nothing!
The values within that partition will be out of order.
However, we do know that all values in the array to the left of the
partition are smaller than all values in the partition.
All values in the array to the right of the partition are greater than
all values in the partition.
Thus, even if Quicksort only gets the values to
"nearly" the right locations, the array will be close to sorted.
This is an ideal situation in which to take advantage of the best-case
performance of Insertion Sort.
The final step is a single call to Insertion Sort to process the
entire array, putting the elements into final sorted order.
Empirical testing shows that the subarrays should be left unordered
whenever they get down to nine or fewer elements.
</p>

<p>
The last speedup to be considered reduces the cost of making
recursive calls.
Quicksort is inherently recursive, because each Quicksort operation
must sort two sublists.
Thus, there is no simple way to turn Quicksort into an iterative
algorithm.
However, Quicksort can be implemented using a stack
to imitate recursion, as the amount of information that must
be stored is small.
We need not store copies of a subarray, only the subarray bounds.
Furthermore, the stack depth can be kept small if care is taken on
the order in which Quicksort's recursive calls are executed.
We can also place the code for <code>findpivot</code> and
<code>partition</code> inline to eliminate the remaining function
calls.
Note however that by not processing sublists of size nine or
less as suggested above, about three quarters of the function calls
will already have been eliminated.
Thus, eliminating the remaining function calls will yield only a
modest speedup.
</p>

<section>
<p id="fn1"><a href="#r1">[1]</a>
The worst insult that I can think of for a sorting algorithm.
</p>
</section>

</div>
